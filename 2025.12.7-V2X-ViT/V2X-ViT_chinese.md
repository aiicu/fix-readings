V2X-ViT：基于视觉 Transformer 的车联网协同感知

**作者**：Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, Jiaqi Ma
**机构**：加州大学洛杉矶分校 (UCLA), 德克萨斯大学奥斯汀分校 (UT Austin), Google Research, 加州大学默塞德分校 (UC Merced)

---

### 摘要 (Abstract)

在本文中，我们研究了应用车联网（V2X）通信来提高自动驾驶车辆感知性能的方法。我们提出了一种利用新型视觉 Transformer 进行 V2X 通信的鲁棒协同感知框架。具体而言，我们构建了一个整体注意力模型，即 **V2X-ViT**，以有效地融合道路上各个智能体（即车辆和基础设施）之间的信息。V2X-ViT 由异构多智能体自注意力层和多尺度窗口自注意力层交替组成，分别用于捕捉智能体间的交互和智能体自身的空间关系。这些关键模块被设计在一个统一的 Transformer 架构中，以处理常见的 V2X 挑战，包括异步信息共享、姿态误差以及 V2X 组件的异构性。为了验证我们的方法，我们使用 CARLA 和 OpenCDA 创建了一个大规模的 V2X 感知数据集。大量的实验结果表明，V2X-ViT 在 3D 目标检测方面树立了新的技术水平（SOTA），并且即使在恶劣、嘈杂的环境下也能实现鲁棒的性能。代码已在 https://github.com/DerrickXuNu/v2x-vit 开源。

**关键词**：V2X，车联网，协同感知，自动驾驶，Transformer

---

### 1. 引言 (Introduction)

精确感知复杂的驾驶环境对于自动驾驶汽车（AVs）的安全至关重要。随着深度学习的最新进展，单车感知系统在语义分割、深度估计以及目标检测和跟踪等多个任务中的鲁棒性已显示出显著提升。尽管有了这些进步，挑战依然存在。单智能体感知系统往往受限于遮挡和远距离传感器的稀疏观测，这可能导致灾难性的后果。造成这一问题的根本原因在于，单个车辆只能从单一视角、有限的视野来感知环境。为了解决这些问题，最近的研究利用同一场景的多个视角优势，研究车对车（V2V）协作，其中来自附近多个 AV 的视觉信息（例如检测输出、原始传感器信息、中间深度学习特征，详见第 2 节）被共享，以获得对环境的完整和准确的理解。

虽然 V2V 技术有望彻底改变移动出行行业，但它忽略了一个关键的协作者——路侧基础设施。AV 的出现通常是不可预测的，而基础设施一旦安装在路口和人行横道等关键场景中，就能始终提供支持。此外，安装在高处的传感器配备的基础设施具有更广阔的视野，且潜在的遮挡更少。尽管有这些优势，但包含基础设施以部署鲁棒的 V2X 感知系统并非易事。与所有智能体都是同构的 V2V 协作不同，V2X 系统通常涉及由基础设施和 AV 组成的异构图。基础设施和车辆传感器之间的配置差异，例如类型、噪声水平、安装高度，甚至传感器属性和模态，使得 V2X 感知系统的设计充满挑战。此外，GPS 定位噪声以及 AV 和基础设施的传感器测量不同步可能会引入不准确的坐标变换和滞后的传感信息。如果不能妥善处理这些挑战，系统将变得脆弱。

在本文中，我们介绍了一种用于 V2X 感知的统一融合框架，即 **V2X Vision Transformer** 或 **V2X-ViT**，它可以联合处理这些挑战。图 2 展示了整个系统。AV 和基础设施相互捕获、编码、压缩并发送中间视觉特征，而自车（即接收者）使用 V2X-Transformer 执行信息融合以进行目标检测。我们提出了两个新颖的注意力模块来适应 V2X 的挑战：
1.  一个定制的**异构多智能体自注意力模块 (Heterogeneous Multi-agent Self-attention)**，在执行注意力融合时明确考虑智能体类型（车辆和基础设施）及其连接；
2.  一个**多尺度窗口注意力模块 (Multi-scale Window Attention)**，可以通过并行使用多分辨率窗口来处理定位误差。

这两个模块将自适应地迭代融合视觉特征，以捕捉智能体间的交互和智能体自身的空间关系，修正由定位误差和时间延迟引起的特征错位。此外，我们还集成了一个**延迟感知位置编码 (Delay-aware Positional Encoding)** 来进一步处理时间延迟的不确定性。值得注意的是，所有这些模块都集成在一个单一的 transformer 中，学习端到端地解决这些挑战。

为了评估我们的方法，我们收集了一个新的大规模开放数据集，名为 **V2XSet**，该数据集使用高保真模拟器 CARLA 和协同驾驶自动化仿真工具 OpenCDA，明确考虑了 V2X 通信期间的现实世界噪声。图 1 显示了收集的数据集中的一个数据样本。实验表明，我们提出的 V2X-ViT 显著提升了基于 LiDAR 的 V2X 3D 目标检测性能，与单智能体基线相比，AP 提高了 21.2%，并且比起领先的中间融合方法表现至少高出 7.3%。

**我们的贡献如下：**
* 我们提出了第一个用于 V2X 感知的统一 transformer 架构（V2X-ViT），它能够捕捉 V2X 系统的异构性，并对各种噪声具有很强的鲁棒性。此外，所提出的模型在具有挑战性的协同检测任务上达到了最先进的性能。
* 我们提出了一种新颖的异构多智能体注意力模块（HMSA），专为异构智能体之间的自适应信息融合而定制。
* 我们提出了一种新的多尺度窗口注意力模块（MSwin），可以并行地同时捕捉局部和全局空间特征交互。
* 我们构建了 V2XSet，这是一个用于 V2X 感知的新型大规模开放仿真数据集，它明确考虑了不完美的现实世界条件。

---



### 2. 相关工作 (Related Work)

**V2X 感知。** 协同感知研究如何有效地融合来自相邻智能体的视觉线索。根据其消息共享策略，可以分为 3 类：
1.  **早期融合 (Early Fusion)**：共享原始数据并收集以形成整体视图；
2.  **中间融合 (Intermediate Fusion)**：基于每个智能体的观测提取中间神经特征并传输；
3.  **晚期融合 (Late Fusion)**：循环检测输出（例如 3D 边界框位置、置信度分数）。
由于早期融合通常需要较大的传输带宽，而晚期融合无法提供有价值的场景上下文，中间融合因其在准确性和传输带宽之间的良好平衡而受到越来越多的关注。最近已经提出了几种用于 V2V 感知的中间融合方法。OPV2V 实现了一个单头自注意力模块来融合特征，而 F-Cooper 采用了 maxout 融合操作。V2VNet 提出了一种空间感知的消息传递机制来联合推理检测和预测。为了减弱异常消息的影响，相关研究通过一致的姿态约束来回归车辆的定位误差。DiscoNet 利用知识蒸馏来增强训练，通过将相应特征约束到来自早期融合网络的特征。然而，V2X 的中间融合仍处于起步阶段。大多数 V2X 方法探索了晚期融合策略来聚合来自基础设施和车辆的信息。例如，有研究提出了针对路侧基础设施故障情况的晚期融合两级卡尔曼滤波器。Xiangmo 等人提出融合基础设施和车辆传感器的车道标记检测，利用 Dempster-Shafer 理论对不确定性进行建模。

**基于 LiDAR 的 3D 目标检测。** 已经探索了许多方法从原始点、体素、鸟瞰图（BEV）图像及其混合中提取特征。PointRCNN 提出了一种基于原始点云的两阶段策略，第一阶段学习粗略估计，然后利用语义属性对其进行细化。有研究提出将空间划分为体素并产生每个体素的特征。尽管精度很高，但由于依赖 3D 卷积，其推理速度和内存消耗难以优化。为了避免计算昂贵的 3D 卷积，有研究提出了高效的 BEV 表示。为了同时满足计算和灵活的感受野要求，一些方法结合了基于体素和基于点的方法来检测 3D 对象。

**视觉中的 Transformers。** Transformer 最初是为机器翻译提出的，其中堆叠了多头自注意力和前馈层以捕捉单词之间的长距离交互。Dosovitskiy 等人提出了一种用于图像识别的 Vision Transformer (ViT)，将图像块视为视觉单词并直接应用自注意力。ViT 中的全自注意力尽管具有全局交互，但计算复杂度高，且无法扩展到长序列或高分辨率图像。为了改善这个问题，许多方法在自注意力中引入了局部性，例如 Swin、CSwin、Twins、窗口和稀疏注意力。通常采用分层架构来逐步增加感受野以捕获更长的依赖关系。虽然这些视觉 transformer 已证明在建模同构结构化数据方面是有效的，但它们表示异构图的功效研究较少。与我们工作相关的一个发展是异构图 Transformer (HGT)。HGT 最初是为网络规模的开放学术图设计的，其中节点是文本和属性。受 HGT 启发，我们构建了一个定制的异构多头自注意力模块，专为图属性感知多智能体 3D 视觉特征融合而设计，能够捕捉 V2X 系统的异构性。

---



### 3. 方法论 (Methodology)

在本文中，我们将 V2X 感知视为一个异构多智能体感知系统，其中不同类型的智能体（即智能基础设施和 AV）感知周围环境并相互通信。为了模拟真实场景，我们假设所有智能体都具有不完美的定位，并且在特征传输过程中存在时间延迟。鉴于此，我们的目标是开发一个鲁棒的融合系统，以增强车辆的感知能力，并以统一的端到端方式处理上述挑战。我们框架的整体架构如图 2 所示，主要包括五个组件：1) 元数据共享，2) 特征提取，3) 压缩与共享，4) V2X 视觉 Transformer，5) 检测头。



#### 3.1 主要架构设计

**V2X 元数据共享。** 在协作的早期阶段，通信网络中的每个智能体 $i \in \{1...N\}$ 彼此共享元数据，如姿态、外参和智能体类型 $c_i \in \{I, V\}$（表示基础设施或车辆）。我们选择其中一辆连接的 AV 作为自车（ego，用 $e$ 表示），以构建围绕它的 V2X 图，其中节点是 AV 或基础设施，边表示有向 V2X 通信通道。具体来说，我们假设元数据的传输是良好同步的，这意味着每个智能体 $i$ 可以在时间 $t_i$ 接收到自车姿态 $x_e^{t_i}$。在接收到自车的姿态后，所有其他连接的附近智能体将在特征提取之前将其自己的 LiDAR 点云投影到自车的坐标系中。

**特征提取。** 我们利用基于锚点的 PointPillar 方法从点云中提取视觉特征，因为它具有低推理延迟和优化的内存使用。原始点云将被转换为堆叠的柱状张量（pillar tensor），然后散射到 2D 伪图像并馈送到 PointPillar 主干网络。主干网络提取信息特征图 $F_i^{t_i} \in \mathbb{R}^{H \times W \times C}$，表示智能体 $i$ 在时间 $t_i$ 的特征，高度为 $H$，宽度为 $W$，通道数为 $C$。

**压缩与共享。** 为了减少所需的传输带宽，我们利用一系列 $1 \times 1$ 卷积沿通道维度逐步压缩特征图。大小为 $(H, W, C')$（其中 $C' \ll C$）的压缩特征随后传输到自车（$e$），在自车上使用 $1 \times 1$ 卷积将特征投影回 $(H, W, C)$。

在连接的智能体捕获 LiDAR 数据的时间与自车接收到提取特征的时间之间存在不可避免的时间差（详情见附录）。因此，从周围智能体收集的特征通常在时间上与自车上捕获的特征不同步。为了纠正这种由延迟引起的全局空间错位，我们需要将接收到的特征变换（即旋转和平移）到当前的自车姿态。因此，我们利用**时空校正模块 (STCM)**，该模块采用微分变换和采样算子 $\Gamma_{\xi}$ 来对特征图进行空间扭曲。还会计算一个 ROI 掩码，以防止网络关注由空间扭曲引起的填充零值。

**V2X-ViT。** 从连接的智能体聚合的中间特征 $H_i = \Gamma_{\xi}(F_i^{t_i}) \in \mathbb{R}^{H \times W \times C}$ 被馈送到我们框架的主要组件，即 V2X-ViT，以使用自注意力机制进行迭代的智能体间和智能体内特征融合。我们在整个 Transformer 中保持特征图处于相同的高分辨率水平，因为我们观察到缺乏高清特征会极大地损害目标检测性能。我们提出的 V2X-ViT 的详细信息将在 3.2 节中展开。

**检测头。** 在接收到最终融合的特征图后，我们应用两个 $1 \times 1$ 卷积层进行框回归和分类。回归输出为 $(x, y, z, w, l, h, \theta)$，分别表示预定义锚框的位置、大小和偏航角。分类输出是每个锚框作为对象或背景的置信度分数。我们使用平滑 $l_1$ 损失进行回归，使用焦点损失 (Focal Loss) 进行分类。



#### 3.2 V2X-Vision Transformer

我们的目标是设计一个定制的视觉 Transformer，可以联合处理常见的 V2X 挑战。首先，为了有效地捕捉基础设施和 AV 之间的异构图表示，我们构建了一个异构多智能体自注意力模块，根据节点和边类型学习不同的关系。此外，我们提出了一种新颖的空间注意力模块，即多尺度窗口注意力（MSwin），它可以在各种尺度上捕捉长距离交互。MSwin 使用多种窗口大小来聚合空间信息，这极大地提高了针对定位误差的检测鲁棒性。最后，这两个注意力模块以因子化的方式集成到单个 V2X-ViT 块中（如图 3a 所示），使我们能够在整个过程中保持高分辨率特征。我们堆叠一系列 V2X-ViT 块来迭代学习智能体间的交互和每个智能体的空间注意力，从而产生用于检测的鲁棒聚合特征表示。

**异构多智能体自注意力 (Heterogeneous Multi-agent Self-attention)。** 基础设施和 AV 捕获的传感器测量值可能具有明显的特征。基础设施的 LiDAR 通常安装在较高的位置，遮挡较少，视角不同。此外，由于维护频率、硬件质量等原因，传感器可能具有不同程度的传感器噪声。为了编码这种异构性，我们构建了一个新颖的异构多智能体自注意力（HMSA），其中我们在有向图中为节点和边附加类型。为了简化图结构，我们假设同一类别的智能体之间的传感器设置是相同的。如图 3b 所示，我们有两种类型的节点和四种类型的边，即节点类型 $c_i \in \{I, V\}$ 和边类型 $\phi(e_{ij}) \in \{V-V, V-I, I-V, I-I\}$。请注意，与将节点特征视为向量的传统注意力不同，我们仅推理来自不同智能体的相同空间位置的特征交互，以保留空间线索。形式上，HMSA 表示为：

$$H_i = \text{Dense}_{c_i} \left( \sum_{j \in N(i)} \text{ATT}(i,j) \cdot \text{MSG}(i,j) \right) \quad (1)$$

其中包含 3 个算子：线性聚合器 $\text{Dense}_{c_i}$、注意力权重估计器 ATT 和消息聚合器 MSG。Dense 是一组由节点类型 $c_i$ 索引的线性投影器，用于聚合多头信息。ATT 计算以关联节点和边类型为条件的节点对之间的重要性权重：

$$\text{ATT}(i,j) = \text{softmax}_{\forall j \in N(i)} \left( \mathop{\Big|\Big|}\limits_{m \in [1,h]} \text{head}_{\text{ATT}}^m(i,j) \right) \quad (2)$$
$$\text{head}_{\text{ATT}}^m(i,j) = (K^m(j) W_{\phi(e_{ij})}^{m,\text{ATT}} Q^m(i)^T) \frac{1}{\sqrt{C}} \quad (3)$$
$$K^m(j) = \text{Dense}_{c_j}^m(H_j) \quad (4)$$
$$Q^m(i) = \text{Dense}_{c_i}^m(H_i) \quad (5)$$

其中 $||$ 表示拼接，$m$ 是当前头编号，$h$ 是总头数。注意这里的 Dense 是由节点类型 $c_{i/j}$ 和头编号 $m$ 索引的。K 和 Q 中的线性层具有不同的参数。为了结合边的语义含义，我们计算由矩阵 $W_{\phi(e_{ij})}^{m,\text{ATT}} \in \mathbb{R}^{C \times C}$ 加权的查询 (Query) 和键 (Key) 向量之间的点积。同样，在解析来自相邻智能体的消息时，我们通过 $\text{Dense}^m$ 分别嵌入基础设施和车辆的特征。使用矩阵 $W_{\phi(e_{ij})}^{m,\text{MSG}}$ 根据源节点和目标节点之间的边类型来投影特征：

$$\text{MSG}(i,j) = \mathop{\Big|\Big|}\limits_{m \in [1,h]} \text{head}_{\text{MSG}}^m(i,j) \quad (6)$$
$$\text{head}_{\text{MSG}}^m(i,j) = \text{Dense}_{c_j}^m(H_j) W_{\phi(e_{ij})}^{m,\text{MSG}} \quad (7)$$

**多尺度窗口注意力 (Multi-scale Window Attention)。** 我们提出了一种新型的注意力机制，专为高分辨率检测上的高效长距离空间交互而定制，称为多尺度窗口注意力（MSwin）。它使用金字塔式的窗口，每个窗口覆盖不同的注意力范围，如图 3c 所示。使用可变窗口大小可以极大地提高 V2X-ViT 针对定位误差的检测鲁棒性（见图 5b 中的消融研究）。在较大窗口内执行的注意力可以捕捉长距离视觉线索以补偿较大的定位误差，而较小的窗口分支在更精细的尺度上执行注意力以保留局部上下文。之后，使用分割注意力 (Split-Attention) 模块来自适应地融合来自多个分支的信息，使 MSwin 能够处理一定范围的姿态误差。请注意，MSwin 独立应用于每个智能体，不考虑任何智能体间的融合；因此为简单起见，我们在本小节中省略智能体下标。

形式上，设 $H \in \mathbb{R}^{H \times W \times C}$ 为单个智能体的输入特征图。在 $k$ 个并行分支中的第 $j$ 个分支中，$H$ 使用窗口大小 $P_j \times P_j$ 被划分为形状为 $(\frac{H}{P_j} \times \frac{W}{P_j}, P_j \times P_j, C)$ 的张量，这表示 $\frac{H}{P_j} \times \frac{W}{P_j}$ 个非重叠补丁的网格，每个补丁大小为 $P_j \times P_j$。我们在第 $j$ 个分支使用 $h_j$ 个头来提高注意力能力。更详细的公式见附录。遵循相关工作，我们还考虑了一个额外的相对位置编码 $B$，作为添加到注意力图的偏置项。由于沿每个轴的相对位置位于范围 $[-P_j+1, P_j-1]$ 内，我们从参数化矩阵 $\hat{B} \in \mathbb{R}^{(2P_j-1) \times (2P_j-1)}$ 中获取 $B$。

为了获得每个智能体的多范围空间关系，每个分支用不同的窗口大小划分输入张量 $H$，即 $\{P_j\}_{j=1}^k = \{P, 2P, ..., kP\}$。我们在使用较大窗口大小时逐步减少头数以节省内存使用。最后，我们通过 Split-Attention 模块融合来自所有分支的特征，产生输出特征 $Y$。所提出的 MSwin 的复杂度与图像大小 $HW$ 成线性关系，同时享有长距离多尺度感受野，并并行地自适应融合局部和（次）全局视觉提示。值得注意的是，与 Swin Transformer 不同，我们的多尺度窗口方法不需要掩码、填充或循环移位，使其在实现上更高效，同时具有更大尺度的空间交互。

**延迟感知位置编码 (Delay-aware Positional Encoding)。** 尽管全局错位被空间扭曲矩阵 $\Gamma_{\xi}$ 捕捉，但另一种由延迟引起的时间滞后期间的对象运动产生的局部错位也需要被考虑。为了编码这种时间信息，我们利用自适应延迟感知位置编码（DPE），由线性投影和可学习嵌入组成。我们使用以时间延迟 $\Delta t_i$ 和通道 $c \in [1, C]$ 为条件的正弦函数对其进行初始化：

$$p_c(\Delta t_i) = \begin{cases} \sin(\Delta t_i / 10000^{\frac{2k}{C}}), & c=2k \\ \cos(\Delta t_i / 10000^{\frac{2k}{C}}), & c=2k+1 \end{cases} \quad (8)$$

线性投影 $f: \mathbb{R}^C \rightarrow \mathbb{R}^C$ 将进一步扭曲可学习嵌入，使其可以更好地泛化到未见的时间延迟。我们在将特征馈送到 Transformer 之前，将此投影嵌入添加到每个智能体的特征 $H_i$ 中，以便特征在时间上预先对齐。

$$\text{DPE}(\Delta t_i) = f(p(\Delta t_i)) \quad (9)$$
$$H_i = H_i + \text{DPE}(\Delta t_i) \quad (10)$$

---



### 4. 实验 (Experiments)

#### 4.1 V2XSet：一个用于 V2X 协同感知的开放数据集

据我们所知，目前不存在适合研究常见 V2X 挑战（如定位误差和传输时间延迟）的完全公开的 V2X 感知数据集。DAIR-V2X 是一个大规模真实世界 V2I 数据集，但没有 V2V 协作。V2X-Sim 是一个开放的 V2X 模拟数据集，但不模拟噪声设置，且仅包含单一道路类型。OPV2V 包含更多道路类型，但仅限于 V2V 协作。为此，我们收集了一个新的大规模 V2X 感知数据集，该数据集利用 CARLA 和 OpenCDA 共同在 V2X 协作期间明确考虑了这些现实世界的噪声。我们的数据集总共有 11,447 帧（如果计算同一场景中每个智能体的帧数，则为 33,081 个样本），训练/验证/测试划分分别为 6,694/1,920/2,833。与现有数据集相比，V2XSet 结合了 V2X 协作和逼真的噪声模拟。更多细节请参阅补充材料。



#### 4.2 实验设置

评估范围在 x 和 y 方向上分别为 [-140, 140] 米和 [-40, 40] 米。我们在两种设置下评估模型：
1) **完美设置 (Perfect Setting)**，其中姿态是准确的，并且所有智能体之间是同步的；
2) **噪声设置 (Noisy Setting)**，其中同时考虑姿态误差和时间延迟。
在噪声设置中，发射器的位置和航向噪声来自高斯分布，默认标准差分别为 0.2 米和 0.2°，遵循现实世界的噪声水平。所有评估模型的时间延迟均设置为 100 毫秒，以便公平比较它们对异步消息传播的鲁棒性。

**评估指标。** 检测性能通过 Intersection-over-Union (IoU) 阈值为 0.5 和 0.7 时的平均精度 (AP) 来衡量。在这项工作中，我们专注于基于 LiDAR 的车辆检测。任何被任何连接智能体的至少一个 LiDAR 点击中的车辆都将被包括为评估目标。

**实现细节。** 在训练期间，随机选择一个 AV 作为自车，而在测试期间，我们在所有比较模型中对固定的自车进行评估。每个智能体的通信范围根据相关文献设置为 70 米，而超出此广播半径的智能体将被忽略。对于 PointPillar 主干，我们将高度和宽度的体素分辨率设置为 0.4 米。所有中间融合方法的默认压缩率为 32。我们的 V2X-ViT 有 3 个编码器层，MSwin 中的 3 个窗口大小分别为：4、8 和 16。我们首先在完美设置下训练模型，然后在噪声设置下对其进行微调。我们采用 Adam 优化器，初始学习率为 $10^{-3}$，并每 10 个周期稳步衰减 0.1 倍。所有模型均在 Tesla V100 上训练。

**比较方法。** 我们将“无融合 (No Fusion)”视为基线，它仅使用自车的 LiDAR 点云。我们还与“晚期融合 (Late Fusion)”进行比较，后者收集所有智能体的检测输出并应用非极大值抑制以产生最终结果；以及“早期融合 (Early Fusion)”，它直接聚合来自附近智能体的原始 LiDAR 点云。对于中间融合策略，我们评估了四种最先进的方法：OPV2V、F-Cooper、V2VNet 和 DiscoNet。为了公平比较，所有模型都使用 PointPillar 作为主干，并且每个比较的 V2V 方法也接收基础设施数据，但它们不区分基础设施和车辆。



#### 4.3 定量评估

**主要性能比较。** 表 1 显示了在完美设置和噪声设置下的性能比较。在完美设置下，所有协同方法都显著优于无融合基线。我们提出的 V2X-ViT 在 AP@0.5/0.7 上分别优于 SOTA 中间融合方法 3.8%/1.7%。它甚至比理想的早期融合（接收完整的原始信息）在 AP@0.7 上高出 0.2%。在噪声设置下，当考虑定位误差和时间延迟时，早期融合和晚期融合的性能急剧下降至 AP@0.7 的 38.4% 和 30.7%，甚至比单智能体基线“无融合”更差。尽管 OPV2V、F-Cooper、V2VNet 和 DiscoNet 仍然高于无融合，但它们的性能在 AP@0.7 上分别下降了 17.7%、21.1%、18.4% 和 15.4%。相比之下，V2X-ViT 表现优异，大幅领先无融合方法，即在 AP@0.5 和 AP@0.7 上分别高出 23% 和 21.2%。此外，与完美设置相比，V2X-ViT 在噪声设置下的 AP@0.5 和 AP@0.7 下降均小于 5% 和 10%，证明了其对正常 V2X 噪声的鲁棒性。V2X-ViT 的实时性能也如表 4 所示。V2X-ViT 的推理时间为 57 毫秒，通过仅使用 1 个编码器层，$V2X-ViT_S$ 仍然可以击败 DiscoNet，同时推理时间仅为 28 毫秒，实现了实时性能。

**对定位误差的敏感性。** 为了评估模型对姿态误差的敏感性，我们从标准差 $\sigma_{xyz} \in [0, 0.5]$ 米，$\sigma_{heading} \in [0^{\circ}, 1.0^{\circ}]$ 的高斯分布中采样噪声。如图 4 所示，当位置和航向误差保持在正常范围内（即 $\sigma_{xyz} \le 0.2m, \sigma_{heading} \le 0.4^{\circ}$）时，V2X-ViT 的性能下降小于 3%，而其他中间融合方法至少下降 6%。此外，早期融合和晚期融合的精度在 AP@0.7 上下降了近 20%。当噪声巨大（例如 0.5 米和 1° 标准差）时，V2X-ViT 仍然可以保持 60% 左右的检测精度，而其他方法的性能显著下降，显示了 V2X-ViT 对姿态误差的鲁棒性。

**时间延迟分析。** 我们进一步研究了范围为 [0, 400] 毫秒的时间延迟的影响。如图 4c 所示，仅 100 毫秒的延迟，晚期融合的 AP 就急剧下降到无融合以下。早期融合和其他中间融合方法相对不太敏感，但当延迟持续增加时它们仍然迅速下降，并且在 400 毫秒后都低于基线。相比之下，即使在 400 毫秒的延迟下，我们的 V2X-ViT 在 AP@0.7 上仍超过无融合 6.8%，这远大于现实世界系统中的通常传输延迟。这清楚地证明了其对时间延迟的极大鲁棒性。

**基础设施与车辆。** 为了分析基础设施在 V2X 系统中的作用，我们评估了 V2V（仅车辆可以共享信息）和 V2X（基础设施也可以传输消息）之间的性能。我们用智能体数量表示可以共享信息的基础设施和车辆的总数。如图 5a 所示，当智能体数量增加时，V2V 和 V2X 都有更好的性能。在我们收集的场景中，V2X 系统与 V2V 相比具有更好的 AP。我们认为这是由于基础设施传感器更好的视野和更少的遮挡，从而产生了更丰富的信息特征来推理环境上下文。

**传输大小的影响。** 传输消息的大小会显著影响传输延迟，从而影响检测性能。在这里，我们研究模型检测性能与传输数据大小的关系。数据传输时间通过 $t_c = f_s/v$ 计算，其中 $f_s$ 表示特征大小，传输速率 $v$ 设置为 27 Mbps。遵循相关研究，我们还包括另一个系统级的异步延迟，遵循 0 到 200 毫秒之间的均匀分布。详情见补充材料。从图 5c 中，我们可以观察到：1）大的带宽需求会迅速消除协同感知的优势，例如，早期融合下降到 28%，表明压缩的必要性；2）在默认压缩率（32x）下，我们的 V2X-ViT 大幅优于其他中间融合方法；3）V2X-ViT 对大压缩率不敏感。即使在 128x 压缩率下，我们的模型仍然可以保持高性能。



#### 4.4 定性评估

**检测可视化。** 图 6 显示了 OPV2V、V2VNet、DiscoNet 和 V2X-ViT 在噪声设置下两个具有挑战性的场景中的检测可视化。我们的模型预测了高度准确的边界框，这些边界框与真实值对齐良好，而其他方法表现出较大的位移。更重要的是，V2X-ViT 可以识别更多的动态对象（更多的真实边界框有匹配），这证明了其有效融合来自附近智能体的所有传感信息的能力。请参阅附录以获取更多结果。

**注意力图可视化。** 为了理解基础设施的重要性，我们还在图 7 中可视化了学习到的注意力图，其中颜色越亮意味着自车支付的注意力越多。如图 7a 所示，从自车和 AV2 的视角来看，有几个对象被大面积遮挡（蓝色圈出），而基础设施仍然可以捕获丰富的点云。因此，V2X-ViT 在遮挡区域对基础设施的关注（图 7d）远多于其他智能体（图 7b 和 7c），证明了基础设施在遮挡情况下的关键作用。此外，基础设施的注意力图通常比车辆的更亮，表明训练好的 V2X-ViT 模型认为基础设施更重要。



#### 4.5 消融研究

**V2X-ViT 中主要组件的贡献。** 现在我们调查 V2X-ViT 中各个组件的有效性。我们的基础模型是带有朴素多智能体自注意力融合的 PointPillars，它平等对待车辆和基础设施。我们在噪声设置下通过逐步添加 i) MSwin, ii) 分割注意力, iii) HMSA, 和 iv) DPE 来评估每个组件的影响。如表 2 所示，所有模块都有利于性能提升，而我们提出的 MSwin 和 HMSA 贡献最显著，分别使 AP@0.7 增加了 4.1% 和 6.6%。

**MSwin 对定位误差的作用。** 为了验证 MSwin 中的多尺度设计对定位误差的有效性，我们比较了三种不同的窗口配置：i) 使用单个小窗口分支 (SW)，ii) 使用小窗口和中窗口 (SMW)，iii) 使用所有三个窗口分支 (SMLW)。我们通过组合不同水平的位置和航向噪声来模拟定位误差。从图 5b 可以清楚地观察到，并行使用大窗口和小窗口显著提高了其对定位误差的鲁棒性，这验证了 MSwin 的设计优势。

**DPE 在延迟下的性能。** 表 3 显示 DPE 可以提高各种时间延迟下的性能。AP 增益随着延迟的增加而增加。

---



### 5. 结论 (Conclusion)

在本文中，我们提出了一种用于 V2X 感知的新型视觉 transformer (V2X-ViT)。其关键组件是两个新颖的注意力模块，即 HMSA 和 MSwin，它们可以捕捉异构的智能体间交互和多尺度的智能体内空间关系。为了评估我们的方法，我们构建了 V2XSet，这是一个新的大规模 V2X 感知数据集。大量实验表明，V2X-ViT 可以在完美和噪声设置下显著提升协同 3D 目标检测。这项工作专注于基于 LiDAR 的协同 3D 车辆检测，仅限于单一传感器模态和车辆检测任务。我们未来的工作涉及用于联合 V2X 感知和预测的多传感器融合。

**广泛影响和局限性。** 所提出的模型可以通过结合使用新型视觉 Transformer 的 V2X 通信来提高自动驾驶系统的性能和鲁棒性。然而，对于在模拟数据集上训练的模型，在数据偏差和对现实世界场景的泛化能力方面存在已知问题。此外，尽管我们的通信方法的设计选择（即在一开始将 LiDAR 投影到其他智能体）具有准确性的优势（详见补充材料），但其可扩展性有限。此外，在数据捕获和共享过程中可能会出现围绕隐私和对抗性鲁棒性的新担忧，这尚未受到太多关注。这项工作促进了未来在自动驾驶视觉学习系统的公平性、隐私和鲁棒性方面的研究。

**致谢。** 本材料部分由联邦公路管理局探索性高级研究 (EAR) 计划以及美国国家科学基金会通过资助 CMMI # 1901998 支持。感谢 Xiaoyu Dong 提供的深刻讨论。

---



### 附录 (Appendix)

在本补充材料中，我们首先讨论我们方法的设计选择和可扩展性问题（附录 A）。然后，我们就所提出的架构提供更多模型细节和分析（附录 B），包括时空校正模块的数学细节（附录 B.1）、所提出的 MSwin 的细节（附录 B.2）以及整体架构规范（附录 B.3）。之后，附录 C 中显示了所提出的 V2XSet 数据集的更多信息和可视化。最后，我们在附录 D 中展示了更多定量实验、定性检测结果、注意力图可视化以及关于传输大小实验影响的细节。



#### A. 设计选择的讨论

**自车的可扩展性。** 我们的方法可以通过两种方式进行扩展：

1) **去中心化**：本文进行的消融研究（图 5a）和 OPV2V 表明，当协作者数量大于 4 时，性能增益变得微乎其微，而计算量仍呈线性增加。在实践中，每个智能体只需要与有限数量的智能体共享特征。例如，Who2Com 研究了请求/传输哪些智能体的数据，大大减少了计算量。此外，所选的 PointPillar 主干计算效率高，例如，在 RTX3090 上，4 个智能体完全并行计算约为 4 毫秒，顺序计算为 16 毫秒。
2) **中心化**：在一定的通信范围内，只选择一个自车智能体来聚合邻居的所有特征以预测边界框，并将结果与其他智能体共享。该解决方案对于一组智能体只需要一个计算节点，因此具有可扩展性。

**通信的设计选择。** 与广播方法（即在每个 CAV 自己的空间中计算特征并直接在自车端转换特征图）相比，我们的方法在检测精度方面具有更多优势。大多数 LiDAR 检测方法通常会根据评估范围大幅裁剪 LiDAR 范围以减少计算量。如下图所示，在广播方法中，CAV 根据自己的评估范围裁剪 LiDAR 数据，这导致了冗余数据。相反，我们的方法总是根据自车的评估范围进行裁剪，从而保证更有效的特征传输。我们通过将我们的框架与广播方法进行比较来进一步验证这一点。下表 TO 显示，对于 DiscoNet 和 V2X-ViT，我们的设计比广播方法分别高出 8.5% 和 8.9%。

*表 T0：我们的设计选择与广播方法之间的比较。*

|                  | DiscoNet (广播 / 我们的) | V2X-ViT (广播 / 我们的) |
| :--------------- | :----------------------- | :---------------------- |
| AP@0.7 (perfect) | 0.610 / 0.695            | 0.623 / 0.712           |



#### B. 模型细节和分析

##### B.1 时空校正模块 (Spatial-Temporal Correction Module)

在协作的早期阶段，当每个连接的智能体 $i$ 在时间 $t_i$ 接收到自车姿态时，智能体 $i$ 观测到的点云将在特征提取之前被投影到时间 $t_i$ 的自车姿态 $x_e^{t_i}$。然而，由于时间延迟，自车在不同的时间 $t_e$ 观测数据。因此，从连接智能体接收到的特征是围绕延迟的自车姿态（即 $x_e^{t_i}$）为中心的，而自车的特征是围绕当前姿态（即 $x_e^{t_e}$）为中心的，导致由延迟引起的空间错位。为了纠正接收特征与自车特征之间的这种错位，需要从自车过去姿态 $x_e^{t_i}$ 到其当前姿态 $x_e^{t_e}$ 的全局变换 $\xi_{x_e^{t_i}, x_e^{t_e}} \in \mathfrak{se}(3)$。为此，我们采用微分 2D 变换 $T_{\xi}(\cdot)$ 在空间上扭曲中间特征。更具体地说，我们将使用仿射变换来变换特征的位置：

$$\begin{bmatrix} x_s \\ y_s \end{bmatrix} = T_{\xi} \left( \begin{bmatrix} x_t \\ y_t \\ 1 \end{bmatrix} \right) = \begin{bmatrix} R_{11} & R_{12} & \delta x \\ R_{21} & R_{22} & \delta y \end{bmatrix} \begin{bmatrix} x_t \\ y_t \\ 1 \end{bmatrix} \quad (11)$$

其中 $(x_s, y_s)$ 和 $(x_t, y_t)$ 是源坐标和目标坐标。由于计算出的坐标可能不是整数，我们使用双线性插值来采样输入特征向量。还会计算一个 ROI 掩码，以防止网络关注由空间扭曲引起的填充零值。该掩码将用于异构多智能体自注意力中，将填充值的注意力权重掩盖为零。

##### B.2 多尺度窗口注意力 (MSwin)

**详细公式。** 设 $H \in \mathbb{R}^{H \times W \times C}$ 为单个智能体的输入特征。设 $h_j$ 为在分支 $j$ 中使用的注意力头数（即头维度 $d_{h_j} = C/h_j$），在 $k$ 个分支中的分支 $j$ 上，在特征 $H$ 的每个非重叠窗口 $P_j \times P_j$ 内应用自注意力可以表示为：

$$H = [H^1, H^2, ..., H^{HW/(P_j)^2}] \quad (12)$$
$$\hat{H}_m^i = \text{Attention}(H^i W_m^Q, H^i W_m^K, H^i W_m^V), \quad i=1, ..., HW/(P_j)^2 \quad (13)$$
$$Y_m = [\hat{H}_m^1, \hat{H}_m^2, ..., \hat{H}_m^{HW/(P_j)^2}], \quad m=1, ..., h_j \quad (14)$$
$$Y^j = [Y_1, Y_2, ..., Y_{h_j}] \quad \text{for branch } j \quad (15)$$

其中 $\hat{H}_m^i \in \mathbb{R}^{P_j^2 \times d_{h_j}}$，$W_m^Q, W_m^K, W_m^V$ 分别表示查询、键和值的投影矩阵。$Y_m$ 是分支 $j$ 的第 $m$ 个头的输出。之后，所有头 $1, 2, ..., h_j$ 的输出被连接以获得最终输出 $Y^j$。这里的 Attention 操作表示相对自注意力，类似于 Swin 中的用法：

$$\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d}} + B \right) V \quad (16)$$

其中 $Q, K, V \in \mathbb{R}^{P_j^2 \times d}$ 表示查询、键和值矩阵。$d$ 是查询/键的维度，而 $P_j^2$ 表示分支 $j$ 的窗口大小。遵循相关工作，我们还考虑了一个额外的相对位置编码 $B$，作为添加到注意力图的偏置项。由于沿每个轴的相对位置位于范围 $[-P_j+1, P_j-1]$ 内，我们从参数化矩阵 $\hat{B} \in \mathbb{R}^{(2P_j-1) \times (2P_j-1)}$ 中获取 $B$。为了自适应地融合来自所有 $k$ 个分支的特征，我们采用分割注意力 (Split-Attention) 模块进行并行特征聚合：

$$Y = \text{SplitAttention}(Y^1, Y^2, ..., Y^k) \quad (17)$$

**时间复杂度。** 如论文中所述，我们有 $k$ 个并行分支。每个分支具有 $P_j \times P_j$ 的窗口大小和 $h_j$ 个头，其中 $P_j = jP$ 且 $h_j = h/j$。划分后，输入张量 $H \in \mathbb{R}^{H \times W \times C}$ 被分割成形状为 $(\frac{H}{P_j} \times \frac{W}{P_j}, P_j \times P_j, C/h_j)$ 的 $h_j$ 个特征。遵循 Swin，我们关注向量-矩阵乘法和注意力权重计算的计算量。因此，MSwin 的复杂度可以写成：

$$\mathcal{O} \left( \sum_{j=1}^k \frac{HW}{P_j^2} \times \frac{C}{h_j} \times (P_j \times P_j)^2 \times h_j + 4 \frac{HW}{P_j^2} \times P_j^2 \times \left( \frac{C}{h_j} \right)^2 \times h_j \right)$$
$$= \mathcal{O} \left( \sum_{j=1}^k P_j^2 HWC + \frac{4HWC^2}{h_j} \right) = \mathcal{O} \left( \sum_{j=1}^k j^2 P^2 HWC + \frac{4HWC^2 j}{h} \right)$$
$$= \mathcal{O} \left( \frac{1}{3} k^3 P^2 HWC + \frac{2HWC^2 k^2}{h} \right) \quad (18)$$

其中第一项对应于注意力权重计算，第二项与向量-矩阵乘法相关，最后一个等式是由于 $\sum_{j=1}^k j^2 = \mathcal{O}(\frac{k^3}{3})$ 和 $\sum_{j=1}^k j = \mathcal{O}(\frac{k^2}{2})$。因此，总复杂度为

$$\text{FLOPs}(\text{MSwin}) = \mathcal{O} \left( \left( \frac{k^3 P^2 C}{3} + \frac{2k^2 C^2}{h} \right) HW \right) \sim \mathcal{O}(HW) \quad (19)$$

这与图像大小呈线性关系。表 T1 显示了不同类型 Transformer 的时间复杂度比较，其中 $N$ 表示输入像素的数量（这里 $N=HW$）。我们的 MSwin 获得了与 $N$ 呈线性复杂度的多尺度空间交互，而其他长距离注意力机制如 ViT、Axial 和 CSwin 需要超过线性的复杂度，这无法扩展到高分辨率密集预测任务，如目标检测和分割。

*表 T1：我们提出的 MSwin 注意力与 (a) ViT 中的全注意力, (b) Axial, (c) Swin, (d) CSwin 的计算复杂度比较。*

| Attention Models | Complexity                                                   |
| :--------------- | :----------------------------------------------------------- |
| ViT              | $\mathcal{O}(4HWC^2 + 2(HW)^2C) \sim \mathcal{O}(N^2)$       |
| Axial            | $\mathcal{O}(HWC(4C+H+W)) \sim \mathcal{O}(N\sqrt{N})$       |
| Swin             | $\mathcal{O}(4HWC^2 + 2P^2HWC) \sim \mathcal{O}(N)$          |
| CSwin            | $\mathcal{O}(HWC(4C+sH+sW)) \sim \mathcal{O}(N\sqrt{N})$     |
| MSwin (ours)     | $\mathcal{O}(\frac{1}{3}k^3P^2HWC + \frac{2HWC^2k^2}{h}) \sim \mathcal{O}(N)$ |

**有效感受野。** 不同 Transformer 之间的感受野比较如图 8 所示。Swin 通过使用移动窗口扩大了感受野，但它需要顺序块来累积。Axial Transformer 在行和列方向上都进行注意力。同样，CSwin 提出在不同方向上具有不对称感受野范围的水平和垂直条带上执行注意力，但需要多项式时间复杂度 $\mathcal{O}(N^{1.5})$。相比之下，我们提出的 MSwin 可以并行地聚合来自多尺度分支的特征以增加视野，具有更对称的感受野和与 $N$ 相关的线性复杂度。

##### B.3 架构配置

鉴于所有这些定义，整个 V2X-ViT 模型可以表述为：

$$z_i = \text{PointPillar}(x_i), \quad x_i \in \mathbb{R}^{P \times 4}, z_i \in \mathbb{R}^{H \times W \times C} \quad \text{for agent } i \quad (20)$$
$$z_0 = \text{STCM}([z_0, ..., z_M]) + \text{DPE}([\Delta t_0, ..., \Delta t_M]), \quad \text{for ego AV} \quad (21)$$
$$z_l' = z_{l-1} + \text{MSwin}(\text{HSMA}(\text{LN}(z_0))), \quad z_0 \in \mathbb{R}^{M \times H \times W \times C}, l=1, ..., L \quad (22)$$
$$z_l = z_l' + \text{MLP}(\text{LN}(z_l')), \quad l=1, ..., L \quad (23)$$
$$y = \text{Head}(z_L) \quad (24)$$

其中输入 $x_i$ 表示每个智能体捕获的原始 LiDAR 点云，它们被馈送到 PointPillar 编码器，为每个智能体 $i$ 产生视觉信息丰富的 2D 特征 $z_i$。然后这些张量被压缩、共享、解压缩，并进一步馈送到时空校正模块 (STCM) 以在空间上扭曲特征。然后，我们将以每个智能体的时间延迟 $\Delta t_i$ 为条件的延迟感知位置编码 (DPE) 特征添加到 STCM 的输出中。之后，来自 $M$ 个智能体的收集特征使用我们提出的 V2X-ViT 处理，该 V2X-ViT 由 $L$ 层 V2X-ViT 块组成。每个 V2X-ViT 块包含一个 HSMA、一个 MSwin 和一个标准 MLP 网络。遵循相关工作，我们在馈送到 Transformer/MLP 模块之前使用层归一化 (Layer Normalization)。我们在表 T2 中显示了 V2X-ViT 架构的详细规格。

*表 T2：V2X-ViT 的详细架构规格。* (表格内容在原文中，此处描述：包括 Point Pillar Encoder 的具体层参数，Delay-aware Pos. Encoding 的具体参数，Transformer Backbone 的具体参数如 HSMA dim 256 head 8，MSwin dim 256 head {16, 8, 4} ws {4, 8, 16}，以及 Detection Head 的参数。)



#### C. V2XSet 数据集

**统计数据。** 我们在 CARLA 中收集了覆盖 5 种不同道路类型和 8 个城镇的 55 个代表性场景。每个场景限制为 25 秒，在每个场景中，至少有 2 个、至多 7 个智能体可以相互通信。每个智能体配备 32 线 LiDAR，数据范围为 120 米。我们将传感器安装在每辆 AV 的顶部，而基础设施传感器仅部署在路口、路段中间和入口匝道，高度为 14 英尺，因为这些场景通常更拥堵且更具挑战性。我们以 10 Hz 记录 LiDAR 点云，并保存相应的位置数据和时间戳。

**基础设施部署。** 基础设施传感器安装在路口、路段中间和入口匝道的交通灯杆或路灯杆上，高度为 14 英尺。对于像乡村弯曲道路这样的道路类型，没有安装基础设施，仅存在 V2V 协作。

**数据集可视化。** 如图 9 所示，V2XSet 数据集中有 5 种不同的道路类型（即直线路段、弯曲路段、路段中间、入口匝道和路口），涵盖了现实生活中最常见的驾驶场景。我们收集的路口场景比其他类型多，因为由于高交通量和严重遮挡，路口通常更具挑战性。来自不同道路类型的数据样本见图 10。从图中我们可以观察到，入口匝道和路口的基础设施传感器具有不同的测量模式，特别是与其安装位置附近的车辆传感器相比。这是由车辆和基础设施传感器之间不同的安装高度造成的。这种观察再次表明了捕捉 V2X 系统异构性的必要性。



#### D. 更多实验结果

##### D.1 识别动态对象的性能

我们将测试集基于对象速度 $v (km/h)$ 分组，并比较所有中间融合模型在噪声设置下的 $AP@IoU=0.7$。如表 T3 所示，V2X-ViT 在各种速度范围内均优于所有其他中间融合方法。值得注意的是，速度范围较高的对象通常具有较低的 AP 分数，因为相同的时间延迟会对高速车辆产生更多的位置错位。

*表 T3：不同速度 $(km/h)$ 对象的感知性能，在噪声设置下以 AP@0.7 衡量。*

| Model    | $v < 20$  | $20 \le v \le 40$ | $v > 40$  |
| :------- | :-------- | :---------------- | :-------- |
| F-Cooper | 0.539     | 0.487             | 0.354     |
| OPV2V    | 0.552     | 0.498             | 0.346     |
| V2VNet   | 0.598     | 0.518             | 0.406     |
| DiscoNet | 0.639     | 0.580             | 0.420     |
| V2X-ViT  | **0.693** | **0.634**         | **0.488** |

##### D.2 不同道路类型的性能

我们还将测试场景基于其道路类型进行分组，并计算噪声设置下的 $AP@IoU=0.7$ 分数。如表 T4 所示，V2X-ViT 在所有 5 个道路类别中均排名第一，证明了其在不同场景下的检测鲁棒性。

*表 T4：不同道路类型的感知性能，在噪声设置下以 AP@0.7 衡量。*

| Model    | Straight  | Curvy     | Intersection | Midblock  | Entrance  |
| :------- | :-------- | :-------- | :----------- | :-------- | :-------- |
| F-Cooper | 0.483     | 0.558     | 0.458        | 0.431     | 0.375     |
| OPV2V    | 0.478     | 0.604     | 0.492        | 0.460     | 0.380     |
| V2VNet   | 0.496     | 0.556     | 0.517        | 0.489     | 0.360     |
| DiscoNet | 0.594     | 0.519     | 0.572        | 0.472     | 0.440     |
| V2X-ViT  | **0.645** | **0.686** | **0.615**    | **0.530** | **0.487** |

##### D.3 定性结果

图 11 至 13 展示了 V2VNet、OPV2V、F-Cooper、DiscoNet 和我们的 V2X-ViT 在噪声设置下不同场景中的更多检测可视化。V2X-ViT 总体上表现出更鲁棒的性能，具有更少的回归位移和更少的未检测对象。当场景具有高密度交通流和更多遮挡的挑战性时（例如图 13 中的场景 7），我们的模型仍然可以准确地识别大多数对象。

##### D.4 注意力可视化

图 14 显示了噪声设置下 V2X-ViT 的更多注意力图可视化。自车、其他连接的自动驾驶车辆 (cav) 和基础设施的 LiDAR 点分别用蓝色、绿色和红色绘制。注意力图中越亮的颜色意味着自车支付的注意力越多。通常，基础设施注意力图的颜色比其他的更亮，特别是对于其他智能体的遮挡区域，表明自车赋予基础设施的重要性更高。这一观察结果与我们的直觉一致，即基础设施的传感器观测具有较少的遮挡，从而导致更好的特征表示。

##### D.5 关于传输大小影响的解释

在这里，我们对论文中的数据传输大小实验提供更多解释。不同的融合策略通常具有不同的带宽要求，例如，早期融合需要大带宽来传输原始数据，而晚期融合仅传递最小大小的数据。这种通信量将显着影响时间延迟，因此我们需要模拟更现实的时间延迟设置来研究传输大小的影响。遵循相关研究，我们将总时间延迟分解为两部分：i) 广播期间的数据传输时间 $t_c$，ii) 由感知系统与通信系统之间缺乏同步引起的空闲时间 $t_i$。总时间延迟计算为

$$t_{total} = t_c + t_i \quad (25)$$

如论文中所述，数据传输时间为

$$t_c = f_s / v \quad (26)$$

其中 $f_s$ 是数据大小，$v$ 是传输速率。空闲时间 $t_i$ 可以进一步解耦为发送端的空闲时间和接收端的时间，即 $t_i = t_{i,1} + t_{i,2}$。对于 $t_{i,1}$，就延迟而言最坏的情况发生在通信系统刚刚错过一个感知周期并需要等待下一轮时。同样，对于 $t_{i,2}$，最坏的情况发生在感知系统的新周期刚刚开始后接收到新数据时。假设感知系统和通信系统都具有相同的 10Hz 速率，则 $0 ms < t_i < 200 ms$。我们采用均匀分布 $\mathcal{U}(0, 200)$ 来模拟这种不确定性。总之，我们使用以下等式来模拟现实世界的时间延迟：

$$t_c = f_s / v + \mathcal{U}(0, 200) \quad (27)$$

这捕捉了传输大小和异步引起的不确定性的影响。在实践中，我们根据公式 26 采样时间延迟，并将其离散化为观测到的时间戳，这些时间戳在 10Hz 更新系统中是离散的。