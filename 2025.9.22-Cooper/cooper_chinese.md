Cooper：基于 3D 点云的网联自动驾驶汽车协同感知

**Qi Chen*, Sihai Tang*, Qing Yang and Song Fu**
Department of Computer Science and Engineering
University of North Texas, USA
{*(QiChen, SihaiTang)@my.unt.edu, Qing. Yang, Song.Fu} @unt.edu



### 摘要

自动驾驶汽车可能会因为检测和识别不准确而做出错误的决定。因此，智能车辆可以结合自身数据与其他车辆的数据来增强感知能力，从而提高检测精度和驾驶安全性。然而，多车协同感知需要融合真实场景，且原始传感器数据交换的流量远远超过现有车载网络的带宽。据我们所知，我们是第一个对用于增强自动驾驶系统检测能力的原始数据级协同感知进行研究的团队。在这项工作中，依靠 LiDAR 3D 点云，我们融合了从不同位置和角度收集的网联车辆的传感器数据。我们提出了一种基于点云的 3D 目标检测方法，用于处理各种对齐的点云。在 KITTI 和我们收集的数据集上的实验结果表明，该系统通过扩展感知区域、提高检测精度和促进增强结果，优于（单车）感知。最重要的是，我们证明了通过现有的车载网络技术传输点云数据进行协同感知是可能的。

---

### I. 引言

推动自动驾驶汽车（或称无人驾驶汽车）发展的一个重要部分是基于这样一种前景，即它们将通过比人类驾驶的汽车卷入更少的车祸、造成更少的伤亡来挽救生命。然而，到目前为止，大多数人类驾驶汽车和自动驾驶汽车之间的比较都是不平衡的，并且包含各种不公平的因素。自动驾驶汽车不会经历疲劳、愤怒或沮丧等情绪衰弱。但是，它们无法以专注且经验丰富的人类驾驶员那样的技能或预判来应对不确定和模糊的情况。

同样，孤立的自动驾驶汽车可能会因为目标检测和识别的失败而做出错误的决定。就像人类驾驶员在受影响的情况下会做出错误的决定一样，车辆基于这些故障做出的决定将与其人类对应者一样糟糕甚至更糟。这种车辆必须完全依靠自己进行决策，因此将无法享有数据冗余的特权，即无法从附近的车辆接收信息。传感器故障或任何其他技术错误都将导致错误的结果，从而产生灾难性的影响。

#### A. 动机

单一来源导致的数据缺失最终也会产生负面影响。以特斯拉在加利福尼亚州的事故为例，这辆车做出了致命的决定，因为它的传感器检测到了混凝土障碍物，但由于它在雷达上是静止状态而丢弃了该信息 [26]。另一个致命决定的事件更加明显，原因是传感器和环境条件无法检测到车辆。例如，一辆特斯拉汽车在佛罗里达州发生的致命车祸，当时车辆和驾驶员都没能在明亮的天空背景下辨认出白色的卡车，导致了车祸 [8]。

当然，也有其他各种情况导致错误决定的例子，例如 Uber 训练事故 [17]。在这种情况下，车辆确实从远处检测到了一个未知物体，即行人。当车辆接近未知物体时，它逐渐辨别出该物体是一辆车，最后才识别出行人，但那时已经太晚了。

我们进一步探讨检测失败发生的原因。很容易确定，一些检测失败是由于物体被遮挡或存在于传感器的盲区中造成的。检测失败也可能是由于接收到的信号太弱或由于系统故障导致信号丢失而引起的识别错误。我们的动机来自于这些事件，因为与事故中的孤立自动驾驶汽车不同，网联自动驾驶汽车（CAV）可以相互共享收集到的数据，从而获得更多信息。

我们提出，信息共享可以改善驾驶性能和体验。建设性的数据冗余将为安全驾驶提供无限的可能性，多辆车可以协同工作以弥补数据稀缺，并为有需要的车辆提供全新的视野。自动驾驶汽车拥有强大的感知系统，它们共同可以实现一个适当的数据共享和分析平台，以获得更高的可靠性和准确性 [30]。

#### B. 现有工作的局限性

虽然增加车辆的连接性有其好处，但也面临挑战。通过增加连接性，由于访问和共享大量信息，可能会出现安全性、隐私以及数据分析和聚合方面的问题。

目前的多传感器融合状态包括三个不同的类别：低级融合、特征级融合和高级融合 [23]。这些类别中的每一个都拥有其独特的优点和缺点。顾名思义，低级融合由未经任何预处理的原始数据融合组成。特征级融合在融合之前提取原始数据的特征。最后，高级融合获取每个单独传感器检测到的对象，并对对象检测结果进行融合 [23]。

高级融合通常因其复杂性较低而被优先选择，但这并不适合我们的需求。对象级融合过于依赖单一车辆传感器，并且只有当两辆车在检测中共享一个参考对象时才有效。这并不能解决先前未检测到的对象的问题，这些对象即使在融合后仍将未被检测到。因此，我们将目光投向了其他两个类别。

#### C. 提出的解决方案

为了解决这个问题，我们研究了基础类别之一，即原始数据的低级融合。原始传感数据是自动驾驶汽车上所有传感器不可或缺的一部分，因此，它非常适合在来自不同制造商的不同汽车之间传输。因此，不同数据处理算法的异构性不会影响车辆之间共享数据的准确性。由于自动驾驶本身是一项至关重要的任务，与车辆高度集成，即使检测中一个微小的错误也可能导致灾难性的事故。因此，我们需要自动驾驶汽车尽可能清晰地感知环境。为了实现这一最终目标，它们将需要一个稳健可靠的感知系统。

在此过程中我们试图解决的两个主要问题如下：（1）我们需要在车辆之间共享的数据类型，以及（2）需要传输的数据量与接收车辆实际需要的数据量。第一个问题源于汽车原生数据集内的可共享数据。第二个问题存在于每辆车产生的海量数据中。由于每辆自动驾驶汽车每天将收集超过 1000GB 的数据 [2]，因此仅组装区域数据的挑战变得更加困难。同样，重建由附近感知系统从不同位置和角度收集的共享数据是另一个主要挑战。

在不同类型的原始数据中，我们建议使用 LiDAR（光探测和测距）点云作为解决方案，原因如下：
* LiDAR 点云在空间维度上比 2D 图像和视频具有优势。
* 在保留感知对象精确模型的同时，对人脸和车牌号码等实体或私人数据进行天然混淆。
* 在融合过程中，由于数据由点而非像素组成，因此比图像和视频更具多功能性。对于图像或视频融合，要求有清晰的重叠区域，而这对于点云数据来说是不必要的，这使得它成为一个更稳健的选择，尤其是在考虑汽车的不同视角时。

鉴于使用原始 LiDAR 数据作为融合基底的三个不同亮点，我们提出了基于 3D 点云的网联自动驾驶汽车协同感知（Cooper）系统。

#### D. 贡献

不准确的目标检测和识别是实现强大有效感知系统的主要障碍。自动驾驶汽车最终会屈服于这种无能，无法提供预期的结果，这对自动驾驶是不安全的。为了解决这些问题，我们提出了一种解决方案，即自动驾驶汽车将其自身的传感数据与其他网联车辆的数据相结合，以帮助增强感知。我们也相信，如前所述，数据冗余是解决这个问题的办法，我们可以通过自动驾驶汽车之间的数据共享和组合来实现它。提出的 Cooper 系统可以提高检测性能和驾驶体验，从而提供保护和安全。

具体来说，我们做出了以下贡献：
* 我们提出了稀疏点云目标检测（SPOD）方法，用于检测低密度点云数据中的对象。虽然 SPOD 是为低密度点云设计的，但它也适用于高密度 LiDAR 数据。
* 我们展示了提出的 Cooper 系统如何通过扩展感知区域和提高检测精度来优于单独感知。
* 我们证明了利用现有的车载网络技术促进车辆间感兴趣区域（ROI）LiDAR 数据的传输以实现协同感知是可能的。

---

### II. 协同感知 (COOPERATIVE SENSING)

鉴于目前自动驾驶汽车数据融合领域的前景和所做的工作，我们需要更进一步，定义我们将什么视为协同感知。我们将网联自动驾驶汽车（CAVs）的协同感知设想为一系列挑战和利益，这将是进步中不可避免的一部分。

#### A. 共享的益处

基于我们的观察，我们想知道是否可以利用多辆车的传感器数据来提高检测精度。众所周知，自动驾驶汽车上的传感设备协同工作以绘制局部环境并监控周围车辆的运动。根据收集到的数据，可以从这些车辆中提取可共享的资源。例如，道路上障碍物后面存在一个被遮挡的区域，一辆车无法感知到，但其他附近的车辆可以感知并提供该区域收集的数据。同时，相邻区域或拥挤区域的车辆可以保持较长时间的连接，从而增强协同感知，这将通过提供关键信息极大地帮助其他车辆。

因此，我们提出一种协同感知方法来提高自动驾驶性能。该框架促使车辆将其传感器数据与其合作者的数据相结合，以增强感知能力，从而提高检测精度和驾驶安全性。

#### B. 共享的困难

尽管可共享资源提供了有用的信息，但车辆更倾向于利用原始数据而不是提取的结果。来自其他汽车的检测结果很难验证，信任问题进一步使事情复杂化。此外，由于共享所有收集到的数据也是不切实际的，我们需要考虑车载网络的带宽和延迟。首先，车载网络的带宽和延迟必须满足协同感知的数据传输要求。然后，车辆需要重建接收到的数据，因为它是从不同的位置和角度获取的。带着这一系列问题，我们详细阐述了我们在构建协同感知方面的研究。

#### C. 数据选择

首先，我们论证哪种类型的传感数据适合协同感知。注意到感知系统主要基于图像和基于 LiDAR 的传感器数据开发。如前所述，图像数据在目标分类和识别方面具有优势，但在位置信息方面有所欠缺。在下一节中，我们提出的 SPOD 方法克服了点云过于稀疏而无法检测对象的缺点。基于上述原因，我们将这两种传感器数据作为协同感知的优先选择。

我们更喜欢 LiDAR 数据，因为它在提供位置信息方面具有优势 [22]。通过仅提取位置坐标和反射值，点云可以被压缩到每次扫描 200 KB。对于某些应用，例如小物体检测（如车牌跟踪），点云很难识别车牌信息。然而，当利用协同感知时，我们仍然能够在点云中定位车牌，并向网联车辆请求其图像数据。由于图像和 LiDAR 点云在感知系统的安装中是对齐的，我们整合了上述主要依赖点云的需求驱动策略。在某些情况下，有必要在协同感知中提取图像数据的片段。

#### D. 数据重建

此外，车辆需要重建接收到的数据，因为它是从不同的位置和角度获取的。通过交换 LiDAR 数据，可以通过将点云合并到其物理位置来直观地重建局部环境。

为了通过将点云映射到物理位置来重建局部环境，交换包中封装了额外的信息。所述数据包应由 LiDAR 传感器安装信息及其 GPS 读数构成，这决定了每帧点云的中心点位置。车辆的 IMU（惯性测量单元）读数也是必需的，因为它记录了车辆行驶过程中的偏移信息：它代表一个旋转，其偏航角（yaw）、俯仰角（pitch）和横滚角（roll）分别为 $\alpha$、$\beta$ 和 $\gamma$ [25]。旋转矩阵 $R$ 将在公式 1 中生成。

$$R=R_{z}(\alpha)R_{y}(\beta)R_{x}(\gamma) \quad (1)$$

这里 $R_{z}(\alpha), R_{y}(\beta), R_{x}(\gamma)$ 是三个基本的旋转矩阵，分别在三维空间中绕 z、y、x 轴旋转一个角度。

$$
R_y(\beta) = \begin{bmatrix} \cos\beta & 0 & \sin\beta \\ 0 & 1 & 0 \\ -\sin\beta & 0 & \cos\beta \end{bmatrix}, \quad
R_z(\alpha) = \begin{bmatrix} \cos\alpha & -\sin\alpha & 0 \\ \sin\alpha & \cos\alpha & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad
R_x(\gamma) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\gamma & -\sin\gamma \\ 0 & \sin\gamma & \cos\gamma \end{bmatrix}
$$

$$
\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = \begin{bmatrix} X_R \\ Y_R \\ Z_R \end{bmatrix} \cup \begin{bmatrix} X'_T \\ Y'_T \\ Z'_T \end{bmatrix} \quad (2)
$$

$$
\begin{bmatrix} X'_T \\ Y'_T \\ Z'_T \end{bmatrix} = R \times \begin{bmatrix} X_T \\ Y_T \\ Z_T \end{bmatrix} + \begin{bmatrix} \Delta d_{x_T} \\ \Delta d_{y_T} \\ \Delta d_{z_T} \end{bmatrix} \quad (3)
$$

当网联车辆交换消息时，协同感知利用公式 2 通过组合发送者和接收者的传感器数据产生一个新的帧，其中我们将所有坐标的集合设为接收者的坐标与发送者坐标的并集。然而，由于发送车辆处于与接收者不同的状态，我们必须对原始坐标应用变换，使它们与接收车辆的状态相匹配。为了获得发送者方向的正确状态，我们使用公式 1。注意，$[X Y Z]'$ 中的 X、Y 和 Z 代表 LiDAR 点云数据中每个点的 3D 空间值，$[X'_{T} Y'_{T} Z'_{T}]'$ 是应用了变换 R 并加上发送车辆平移坐标后的发送者点云。变换通过公式 1 计算，使用的是发送者和接收者之间的 IMU 值差。

---

### III. 协同感知 (COOPERATIVE PERCEPTION)

在本节中，我们将展示如何在协同稀疏 LiDAR 点云数据上检测对象。

#### A. 基于点云的目标检测

众所周知，每辆自动驾驶汽车都会提取传感器数据来感知局部环境中的细节，例如车道检测、交通标志检测以及汽车、骑行者和行人等物体。然而，在点云中准确检测对象是一个挑战，因为 LiDAR 点云是稀疏的，并且其点密度变化很大。例如，最近基于 KITTI 中的点云数据集 [9]，VoxelNet [31] 公布了其在汽车检测任务上的实验，其表现优于最先进的 3D 检测方法。其汽车检测平均精度为 89.60%，而对于较小的物体，如行人和骑行者，在完全可见（简单）的检测环境中，平均精度分别下降到 65.95% 和 74.41%。而在难以看清（困难）的检测条件下，汽车、行人和骑行者的检测率分别进一步下降到 78.57%、56.98% 和 50.49%。

这里的另一个见解是，LiDAR 提供了带有位置信息的稀疏 3D 点云，但很难分类和识别。为了分析上述工作的结果，我们不能忽视检测失败。这使我们能够从另一个角度探讨这个问题——利用协同传感方法来提高检测精度。

#### B. 稀疏点云目标检测 (SPOD)

通常，自动驾驶汽车使用单一的端到端深度神经网络来处理原始点云。然而，在协同传感之后，从不同 LiDAR 设备重建的数据可能具有不同的特征，如点密度。例如，Velodyne [3] 生产 64 线、32 线和 16 线 LiDAR 设备，它们提供不同密度的点云。类似于图像的分辨率，使用深度神经网络的 3D 检测器在用于低密度点云时可能会产生不准确的识别结果。我们注意到，提供最高分辨率 LiDAR 数据的 64 线 LiDAR 被研究人员和公司广泛用于 3D 目标检测 [31], [29]。而其他一些人，如我们的情况，使用 16 线 LiDAR，它输出稀疏数据，但比高端同类产品具有价格优势。

这就要求我们在其组装的 3D 检测模型上提出的检测方法不仅要在高密度数据上工作，还要能从更稀疏的点云中检测对象。遗憾的是，这些基于卷积神经网络 (CNN) 的目标检测方法不适合低密度数据，因为输入特征不足。受 [29] 提出的 SECOND（一种学习点云逐点特征的端到端深度神经网络）的启发，我们提出了可以适应低密度点云的稀疏点云目标检测（SPOD）方法。

#### C. SPOD 的架构

提出的检测器如图 1 所示，由三个组件组成。我们采用的 3D LiDAR 点云表示为一组带有反射值的笛卡尔坐标 (x, y, z)。点云的分布过于稀疏和不规则。具体来说，在预处理中，为了获得更紧凑的表示，使用 [27] 中的方法将点云投影到一个球面上以生成密集表示。在体素特征提取器组件中，我们的框架将表示的点云作为输入，将提取的逐体素特征馈送到体素特征编码层，这一点 Voxelnet [31] 已经很好地证明了。然后应用稀疏卷积中间层 [15]。稀疏 CNN 在基于 LiDAR 的检测中提供了计算优势，因为点云的分组步骤会产生大量稀疏体素。在这种方法中，如果没有相关的输入点，则不计算输出点。

最后，区域建议网络 (RPN) [21] 是使用单次多框检测器 (SSD) 架构 [16] 构建的。来自稀疏 CNN 的特征图作为 RPN 的输入，并被连接成一个特征图进行预测。每辆车中的框架都使用这个单一的端到端可训练网络，不仅从密集的 LiDAR 数据中产生 3D 检测结果，而且还从附近车辆的低分辨率 LiDAR 数据中产生结果。

最终，我们成功地采用 SPOD 在我们收集的稀疏数据和密集的 KITTI 数据上检测对象。在下一节中，我们将展示 SPOD 检测的完整评估。

---

### IV. 评估与结果分析

在本节中，我们将使用两个真实的 LiDAR 数据集评估提出的 Cooper 系统的性能。

#### A. 数据集

在实验中，我们在两个数据集上测试 Cooper：由卡尔斯鲁厄理工学院和芝加哥丰田技术研究所提供的 KITTI 数据集，以及由我们的半自动驾驶高尔夫球车收集的 T&J 数据集。因此，我们获得了两种类型（密集和稀疏）的点云。在密集的 KITTI 数据集中，使用了 64 线 LiDAR 传感器来收集点云。但在我们的 T&J 数据集中，提供 16 线点云，收集到的点云比 KITTI 稀疏 4 倍，当然，数据量也相应减少了 4 倍。有了这两个数据集，我们总共针对 19 种场景全面评估了 Cooper 系统的性能。基于 KITTI 测试集数据，我们选择了四组不同的道路驾驶测试场景。同时，为了丰富实验内容并验证我们的设计效果，我们使用 T&J 数据集对 Cooper 进行了 15 次实验。注意，Cooper 也可以应用于异构点云输入。由于缺乏合适的 LiDAR 数据集，我们选择不进行此测试。

我们将单次拍摄（single shot）定义为单个车辆收集的点云，将协同传感定义为合并来自附近车辆的所有点云。我们系统地分析了单次拍摄和协同传感的测试结果，以展示目标检测性能的提升。Cooper 在两个实验数据集下的定性结果将在以下章节中展示。

#### B. 在 KITTI 数据集上的评估

在本节中，我们使用 KITTI 数据集评估 Cooper 的性能。众所周知，KITTI 提供了多种场景下的原始连续 3D Velodyne 点云。我们选择文件夹 2011/09/26/0009 中的一段传感数据作为示例，如图 2 所示。

为了对应 $120^{\circ}$ 的前视图像，我们评估了前视区域的 LiDAR 数据。在开始时间 t1，采集了一帧 64 线原始点云单次拍摄，如图 2a 所示。随着测试车辆向前移动两秒后，在时间 t2 采集了另一帧 64 线原始点云单次拍摄，如图 2b 所示。通过合并 t1 和 t2 的点云，我们模拟了两辆车之间的协同传感过程。我们利用 SPOD 目标检测器来检测汽车，并在图 2c 中用红框绘制结果以框定检测到的汽车。同时，为了比较 Cooper 上的检测结果，我们也对在时间 t1 和 t2 收集的单次拍摄点云应用 SPOD。检测到的汽车用蓝框绘制，分别如图 2a 和图 2b 所示。

从图中，我们可以观察到采用协同感知的两个主要改进。首先，通过数据共享扩展了感知范围。我们可以看到在 t1 我们观察到 6 个蓝框，在 t2 我们再次观察到 6 个蓝框。然而，当结合在一起时，我们在合并数据中观察到总共 9 辆被检测到的汽车（红框），其中包括 t1 和 t2 检测到的所有汽车。其次，一些被检测车辆的检测分数/置信度值增加了。例如，图 2a 中的一辆车在 t1 以 0.76 的检测分数被检测到，同一辆车在图 2c 中也被检测到，但这辆车的检测分数增加（增加了 13%）到了 0.86。我们还在图 2a 和图 2b 的底部提供了相应的图像作为地面真值。

接下来是计算在四种不同场景（T 形路口、停车标志、左转和弯道场景）中通过单次拍摄和协同传感检测到的车辆数量。两辆车收集的单次拍摄数据分别标记为 t1 和 t2，t3 和 t4，t5 和 t6，t7 和 t8。因此，标记为 $t1+t2$，$t3+t4$，$t5+t6$ 和 $t7+t8$ 的数据是结合了单次拍摄点云的协同数据。然后我们将每种情况下的车辆检测结果与地面真值（在图像中捕获）进行比较，并在图 3 中描绘结果。$\Delta d$ 的值表示车辆在两个不同时间的位置之间的距离。每三列代表一个协同过程，类似于我们在图 2 中演示的示例。我们使用每列中的单元格绘制检测结果的分布。每个单元格中的数字是检测分数，分数越高，结果越肯定。符号 X 表示漏检，即检测分数太低。没有分数的单元格意味着对象在检测区域之外。此外，使用不同的颜色来指示距离。颜色越深，距离越远。根据 LiDAR 的实际检测距离，我们将其分为近 (<10m)、中 (10-25m) 和远 (>25m) 三个尺度，在插图中分别用白色、灰色和黑色表示。很明显，协同数据中检测到的汽车数量等于或超过了单独单次拍摄中的数量。

然后，我们使用定性结果来分析检测到的车辆数量和准确性的表现，如图 4 所示。提出的 Cooper 方法不仅检测到了更多的汽车，而且由于协同点云中没有漏检，因此赋予了更好的检测精度。

#### C. T&J 数据集

不幸的是，KITTI 数据集没有提供足够的实验场景，因为它是由孤立的单辆车收集的视觉基准。我们致力于多车协同，因此，为了提高 CAV 的驾驶安全性和体验，我们建立了一个适合车辆协作的数据集，将其命名为 T&J 数据集。我们的测试车配备了高精度传感系统，如 LiDAR 系统、雷达系统、视觉系统以及辅助系统如 GPS 和 IMU 传感器。更具体地说，我们的传感器框架包括以下传感器：
* 2 X 前视摄像头
* 4 X 环视鱼眼摄像头
* 1 X 惯性和 GPS 传感器
* 1 X 前视 $120^{\circ}$ 雷达
* 1 X Velodyne VLP-16 $360^{\circ}$ LiDAR
* 1 X Nvidia PX2

Velodyne VLP-16 360 LiDAR [3] 与雷达一起使用，雷达利用无线电波测量距离。LiDAR 提供低分辨率图像信息。另一方面，摄像头提供非常高分辨率的图像信息，但是，它在极端天气或环境条件下会失效。四个鱼眼镜头摄像头用于感知和导航周围环境。IMU 传感器提供监控车辆动态变化运动的系统。此外，GPS 传感器数据可用于获得汽车位置或定位的粗略估计。Nvidia Drive PX2 [24] 是用于我们自动驾驶的可扩展 AI 超级计算机。

#### D. 在 T&J 数据集上的评估

我们在 T&J 数据集上评估 Cooper 的性能。我们选择了一系列连续的前视 LiDAR 点云帧，如图 5 所示。可以清楚地发现，我们的点云比 KITTI 的点云稀疏得多。T&J 数据集中的所有数据都是在停车场收集的。

图 5a 显示了一帧 16 线原始点云数据。图 5b 显示了另一帧收集到的单次拍摄数据。通过合并这两帧点云，我们产生了两个车辆的协同传感。3D 检测器检测汽车并在图 5c 中用红框绘制结果以框定检测到的汽车位置。与图 2 类似，SPOD 在两次单次拍摄中检测汽车并用蓝框绘制它们。SPOD 还在协同传感中用红框绘制结果以框定检测到的汽车。同时，地面真值图像显示在图 5a 和图 5b 的底部。通过研究这个案例，我们得出结论，通过数据共享扩大了传感区域，因为图 5c 检测到了单次拍摄中的所有对象。最重要的是，我们看到发现了新车的存在，这些车在之前的单次拍摄中并不存在。这种现象直接证明了对象级融合的缺点。由于两辆车都没有自己检测到对象，对象级融合就不可能检测到被遗漏的对象。这一点，我们通过低级融合避免并克服了。

我们将时间 t1 和 t2 检测到的汽车分别标记为数字 1 和 2。值得注意的是，图 5c 中有三辆未标记的车辆。这是一个重大发现，因为这种现象表明协同感知的检测能力有所提高。我们可以推断并假设，通过接收来自附近车辆的感知信息，Cooper 可以极大地增强车辆的感知范围，从而更好地检测交通信息。

T&J 数据集提供了四组测试数据，这些数据是在我们校园停车场周围的道路上收集的。在这四个场景中，我们进行协同感知实验。与 KITTI 不同，在每个实验场景中，我们在不同距离采样融合数据，以便更好地显示不同区域车辆收集信息的差异。如图 6 所示，在每个场景中，我们列出了 Cooper 在不同距离的详细检测结果。与图 3 类似，每三列对应两次单次拍摄和一次协同传感的 SPOD 检测结果，代表一个协同感知案例。测试车可以接收附近的传感数据和相对长距离的传感数据。例如，在图 6a 中，从左到右，有三种情况，其中一辆车与位于三个距离的其他三辆车协作。可以看出，在相邻区域的协同感知中，如场景 1 和 4 的左侧案例，两次单次拍摄的单独检测结果相似，但都输出了未检测到的目标，因为这些目标在单次拍摄中被未知方式遮挡。通过协同感知，遮挡区域的点云相互补充，从而检测到了这些目标。此外，在这两种情况下，经过协同感知后，检测到的目标的检测分数都有显著提高。我们将这种现象归因于数据的冗余以及通过收集详细的点云聚集了更多的特征。

在图 6 所示的所有场景中，我们进行了两辆车的协同感知，两辆车彼此相距相对较远。结果，检测区域扩大得更大。每辆车都可以检测到其前方的目标。但对于远距离目标，由于点云的稀缺或遮挡，它们无能为力。协同感知能够对位于远、中、近距离的对象进行全局检测。对象出现在不同颜色的单元格中。同样，一些单次拍摄未检测到的对象在协同传感中被检测到了。这加强了一个事实，即一些通过传统手段未被检测到的对象可以通过数据融合被发现。这表明我们的设计可以补充一些关键特征。这是关于协同感知的一个重大发现。

然后，我们使用定性结果来分析检测到的车辆数量和检测分数的表现，如图 7 所示。从场景 1 中，我们得到了三种不同情况的单次拍摄分析结果。很明显，基于融合数据检测到的汽车数量远高于任何一辆单独的汽车。尽管检测率很高，但我们也看到，即使在融合时，仍有一些汽车未被检测到。在场景 2 中，我们发现有大量汽车很难被任何一辆单独的汽车检测到，但在融合时会出现。这种环境变化与常见的区域高度相关，例如满载的停车场或拥挤的路口，在这些地方每辆车都受到周围汽车的限制。如果有一辆超速行驶的汽车无视停车标志或闯红灯，融合将降低附近所有相关车辆漏检的可能性。在场景 3 和 4 中，我们发现，与场景 1 和 2 中显示的趋势类似，融合与目标检测增加之间存在密切关系。由于每个场景发生在不同的环境、一天中的不同时间、不同程度的拥堵中，融合方法被证明是稳健的，并且能够适应不同的环境，同时保留其增强现状的能力。这与我们上面的发现一致，我们的检测分数提高最差也是原始检测分数平稳增加 50%，仅此一项就足以让自动驾驶汽车注意到该物体以进行避让预防，因为它们只需要知道那里有一个物体，而此前并未发现。

#### E. 统计分析

我们的统计分析结果表明，在 KITTI 和 T&J 数据集的实验场景中，协同感知中的一些目标被两者都检测到，一些仅被其中一个检测到，而一些则两者都未检测到。检测难度因此分别被分类为简单、中等和困难。具体来说，简单是指一辆或多辆车能够检测到同一物体。中等是指只有一辆车能够清晰地检测到该物体。最后，困难是指没有车能够检测到该物体。

在图 8 中，我们计算了这三类物体检测性能的提升。例如，从标记为简单的线来看，我们可以看到 80% 的时间里检测分数提高了 10%。直接从我们的测试来看，我们看到简单和中等目标的检测分数实现了微小但持续的检测率增长；主要分布在 10% 的检测分数提升内。这是因为简单和中等物体都包含从单一场景捕获的详细且饱和的点云，导致融合仅对检测结果提供微小的改进。然而，注意当我们测试第三类物体，即两者都未检测到的困难物体时，我们发现检测分数的提高是一个显著的飞跃。

我们记录了基于单次拍摄和协同数据的检测时间成本，如图 9 所示。由于延迟严重影响所有自动驾驶汽车的性能和可靠性，我们针对 KITTI 数据和我们自己的数据测试了我们的融合方法。在测试中，用于 3D 汽车检测的 SPOD 模型在配备 GeForce GTX 1080 Ti GPU 的计算机上执行 [1]。在两个实验中，我们比较了单次拍摄与融合数据中目标检测所需的时间。在这两种情况下，融合数据比基线数据多使用了 5 毫秒，对于检测到的对象数量和准确性的显著增加来说，检测时间的增加非常微小。

#### F. 融合稳健性

从现实的角度来看，我们将不可避免地要处理传感器漂移，因此为了处理这种现象，我们必须测试我们的融合方法对传感器漂移的稳健性。在集成 GPS 和 IMU 时，我们观察到位置误差小于 10 厘米 [6]。为了测试我们融合方法的稳健性，我们对 GPS 读数进行了程序化的人工倾斜。我们按如下方式倾斜 GPS 数据：
* 将 x 和 y 坐标都倾斜到已知 GPS 漂移的最大边界。
* 仅将一个轴倾斜到 GPS 漂移的极限。
* 通过将最大 GPS 漂移加倍来突破该边界，以模拟异常情况。

在 GPS 读数倾斜的情况下，我们随后针对基线 GPS 读数测试了每种不同类型的漂移场景的检测分数。从图 10 可以看出，除了已知未检测到的车辆外，我们看到倾斜的检测分数与基线分数有类似的聚类，绝大多数实现了成功检测。然而，应该注意的是，倾斜读数令人惊讶地提高了几种情况下的检测分数，可能掩盖了基线 GPS 读数的固有漂移。正如一些倾斜有助于结果一样，它也导致了两个实例的检测失败。

#### G. 网络需求

尽管点云可以简化为坐标值，但我们仍然需要考虑自动驾驶汽车产生的数据与有限的无线网络吞吐量之间的差距，例如 DSRC 提供的有限带宽 [12]。我们采用基于感兴趣区域（ROI）提取数据的策略，例如交通信号灯、遮挡区域、附近车辆和行驶路径中的自由空间，以进一步将数据大小减少到每帧数百 KB。背景数据如建筑物、树木被减去，因为这些信息可以在每辆车多次测绘测量后构建。这允许保留静止物体的有价值信息，同时保持 ROI 数据的大小较小。出于目标检测的目的，每当该区域发生检测失败时，将提取 ROI 数据。

如图 11 所示，我们有三种不同的场景，每种都代表一种普遍现象。对于第一种，我们看到两辆车彼此相距相当远，横向较远但水平方向相当近。我们通常会在由单车道分隔线分隔的相反方向的双车道驾驶中看到这种情况。在这种情况下，我们理想地希望获得尽可能多的信息，因为我们缺乏车辆之间物理缓冲的安全性。在这种情况下，我们传输整个 LiDAR 数据帧，这是所有场景中成本最高的，如图 12 所示。从同一场景中，我们可以计算出对于最昂贵的数据交易，每辆车的总数据大小可以压缩到每帧约 1.8 Mbit。

接下来，我们有车辆横向距离较近的情况，代表典型的路口，所有方向的汽车都可以看到对面的车。在这种情况中，ROI 通常是驾驶员视角的视野，这使得仅 120 度的视野成为我们的最低要求。由于两辆车都需要交换此信息，因此交易成本对于每辆参与的车辆来说是累加的。

最后，我们有最常见的情况，即一辆车需要前车的视野。后车是需要信息的一方，因此交易是单向的，消耗的带宽是三种场景中最少的。因此，从三种不同案例的模拟推导得出，这三种呈现的情况都在 DSRC 带宽的容量范围内，如实际测试中所见 [5]。

#### H. 实验结果总结

总之，我们证明了 Cooper 方法在扩展传感区域、提高检测精度和补充目标检测方面优于单独感知。我们发现协作提供了更多信息，甚至一些个人未感知到的信息。最重要的是，我们进行了可行性研究，并证明 DSRC 的带宽可以满足协同感知的点云传输。我们想提一下，我们的设计在隐私保护方面取得了成功，因为只涉及 LiDAR 数据的共享。

---

### V. 相关工作

自动驾驶汽车的快速发展促使研究机构开发表示方法来感知局部环境，例如基于开放数据集 [7], [9], [10] 进行车道检测、交通标志检测以及检测汽车、骑行者和行人等物体 [19], [20], [29], [31]。众所周知，这些数据集是由个别车辆的多个传感设备收集的。为了实现自动驾驶，我们非常重视周围局部环境的认知准确性。然而，即使利用最先进的卷积神经网络 (CNN) [13]，检测结果仍有巨大的改进空间。

当前的工作利用传感器的低级融合来提取特征或对象以进行跟踪 [14]。然而，这并没有为了融合和目标检测的目的而按原样使用原始数据。诸如 [11] 和 [4] 之类的论文讨论了为低级融合和检测构建理论架构的融合方法。其他方法，如 [28] 和 [18]，提出了通过融合来自同一辆车的图像和点云来进行 3D 目标检测的方法。与改进单辆车的检测方法不同，我们将方法集中在不同车辆之间的数据融合上。

据我们所知，之前没有任何工作实现利用多车原始传感器数据进行目标检测的概念。这种改进空间也是造成严重后果的原因，因为自动驾驶汽车可能会因目标检测失败而做出错误的决定。用于网联自动驾驶汽车的 Cooper 框架可以通过协同传感解决上述问题。然而，没有任何公共数据集和相关检测方法明确考虑将低级融合方法作为解决方案。

---

### VI. 结论

我们提出了用于网联自动驾驶汽车的 Cooper，作为通往更广泛 CAV 平台的入口。该方法促使具有 CAV 能力的车辆将其传感数据与其合作者的数据相结合，以增强感知能力，从而提高检测精度和驾驶安全性。为了重建局部环境，我们将点云映射到其相应的对象位置。这将合并并对齐从附近车辆收集的共享数据，这些数据可能提供来自不同位置和角度的数据范围。我们将基于深度学习的 SPOD 与 Cooper 相结合，从对齐的 LiDAR 数据中检测 3D 对象，标记并发现以前未检测到的对象。最后，我们在 KITTI 和我们收集的数据集上评估了 Cooper，表明 Cooper 能够通过扩展有效传感区域、捕获多个场景中的关键信息并提高检测精度来增强检测性能。

---

### 致谢

这项工作得到了 Nvidia GPU 捐赠计划的支持。