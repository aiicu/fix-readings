#### Cooper

##### 文档

论文： [cooper.pdf](2025.9.22-cooper\cooper.pdf) ，(ICDCS 2019)

AI+个人修正详细理解：[cooper-reading](2025.9.22-cooper\reading.md) 



##### 简要理解

第一次使用点云数据检测，使用稀疏卷积方法，并且是原始级（使用原始点云数据）融合，设计了 SPOD 点云检测架构

验证了模型 信息传输量 在现有 车辆网络的带宽限制内，提出 ROI 方法压缩带宽

在 KITTI 数据集和自建 T&J 数据集上进行测试，效果很好，尤其是可以测出原先没有测出的车



##### 名词解释

SPOD：看论文的图一，分三块，Point clouds preprocessing & Voxel feature extractor -> Sparse Convolutional Middle Layers -> Region Proposal Network (RPN，就是检测头Detection Head)

ROI：region of interest，看图11，就是选感兴趣的区域，只发送这一区域的点云数据

Sparse Convolutional Middle Layers：由于体素化后的数据中绝大多数体素是空的，使用传统的3D卷积会造成巨大的计算浪费。该模块采用“稀疏卷积”技术，**仅在包含特征的非空体素上进行卷积运算**，从而高效地聚合局部特征，逐步构建出更高级、更抽象的场景特征图（Feature Maps）



#### Who2com

##### 文档

论文：  [Who2com.pdf](2025.9.23-Who2com\Who2com.pdf) 

AI+个人修正详细理解： [Who2com-reading](2025.9.23-Who2com\reading.md) 



##### 简要理解

首次系统性提出在带宽受限下的多智能体协同感知与通信学习问题

设计了**三阶段握手通信**机制，并引入可学习的非对称压缩策略

- 三阶段握手通信：见论文图2
  - **Request**：目标智能体生成压缩后的请求消息（query），广播给其他正常智能体。
  - **Match**：正常智能体基于自己的观测生成键值（key），计算与请求消息的匹配分数（value注意力分数），并返回给目标智能体。
  - **Connect**：目标智能体根据匹配分数选择top-k个最佳智能体，从其获取高带宽特征图，并与自身信息融合完成感知任务。
- 非对称压缩：见论文图3
  - 智能体经过一个可学习矩阵G提取query和key，分数计算改成全局计算 $s_{ji}=\mu_j^TW_ak_i$ ，4个参数从左到右指分数、query，学习矩阵W，key。
  - 根据分数选取最佳正常智能体，目标和最佳智能体经过编码器E，拼接，再解码D得到融合图



#### When2com

##### 文档

论文：  [When2com.pdf](2025.9.26-When2com\When2com.pdf) 

AI+个人修正详细理解： [When2com-reading](2025.9.26-When2com\reading.md) 

AI翻译：[When2com_chinese](2025.9.26-When2com\When2com_chinese.md) 



##### 简要理解

该模型不使用全连接通信，而是采用类似于“握手”的机制来修剪无关的连接，过程主要分为三个阶段：

- **请求 (Request)**：智能体 $i$ 将其局部观测 $x_i$ 压缩为一个极小的查询向量 (Query) $\mu_i$ 和一个较大的键向量 (Key) $\kappa_i$ ，具体见公式1。

- **匹配 (Match)**：

  * 决定与谁通信 (Who)：计算智能体 $i$ 的 Query 与其他智能体 $j$ 的 Key 之间的相似度 $m_{i,j}$，具体见公式2（通用注意力），相似度信息会全部互相发送。如果匹配分数高，说明智能体 $j$ 能提供有用的信息。
    

  * 决定何时通信 (When)：计算智能体自身的 Query 与自身的 Key 的相似度 $m_{i,i}$。如果自身匹配分数高，说明自身信息已足够，无需进行外部通信。

* **选择 (Select)**：利用前面计算的相似度，修剪掉低分数的连接，构建稀疏的通信图。
  - 通信图 $\bar{M}$ ：将相似度 $m_{i,j}$ 全部结果收集，取softmax，构造出匹配矩阵 $M$ (具体见公式5)，最后取一个阈值 $\delta$ ，$M$ 小于这个阈值 $\delta$ 的元素都置为0，得到新矩阵 $\bar{M}$ ，所得矩阵 $\bar{M}$ 可视为有向图的邻接矩阵，其元素表示何时通信，元素为零表示不与之通信。（可以看图4，很好理解）
* 根据通信图获取 自身 对 agent 的特征图 $f$ ，相似度作为系数，获取的所有特征图累加得到 $f^{all}$ ，与自身 $f_i$ 拼接，再解码D得到融合图



##### 与who2com区别

ai理解：[diff](2025.9.26-When2com\diff.md) 





#### FRLCP

##### 文档

论文：  [FRLCP.pdf](2025.11.25-FRLCP\FRLCP.pdf) 

AI+个人修正详细理解： [FRLCP-reading](2025.11.25-FRLCP\reading.md) 

AI翻译：[FRLCP_chinese](2025.11.25-FRLCP\FRLCP_chinese.md) 



##### 简要理解

本文提出了一个利用**深度强化学习（DRL）**的框架，具体是 **branching DQN** ，两两匹配车辆，分配资源块RB（一个车辆最多用一个RB），最大化车辆对接收到的传感信息的满意度。优化了很多情况。

- **车辆传感数据压缩**：使用区域四叉树压缩信息，参考论文的图2理解
  - 感应范围被递归分解为不同分辨率的正方形块，一个节点代表一个块，节点的儿子就是把这个节点代表的正方块切分成4块，4个子节点分别代表4块其中一块。
  - 每个块的状态为：占用、未占用或未知。覆盖情况：占用>未知>未占用
  - 车辆仅需传输特定的四叉树块，而非整个点云。块的选择也用 RL 学习
- **问题分解与 DRL 建模**
  - 为了解决复杂的联合优化问题，作者将其分解为两个层级的 RL 问题：
    - **路侧单元（RSU）层级：** 负责**车辆关联**（决定谁与谁配对）和**资源块（RB）分配**。
    - **车辆层级：** 负责**内容选择**，即决定传输哪些四叉树块给关联车辆。
  - **复杂度优化**：
    - 原因：RSU层级中，车辆配对可能情况是 $\prod_{n=1}^{\frac{N}{2}}(2n-1)$ ，资源块分配可能情况是 $K^N$ ，其中 $N$ 是车辆个数，$K$ 是资源块块数
    - 解决：使用 **branching DQN**，把配对情况的列举降为线性，具体见图3
    - 车辆配对：创立 $\lfloor \frac{N}{2} \rfloor$ 个 branches 分支，每 $i$ 个分支只有 $j_i=N-2i+1$ 个选择，假如选择 $t$ ，就是除开前面分支选择的车辆，选第一车辆，和 $t+1$ 车辆配对，具体例子见 图3 左边，枚举的可能情况降成 $\sum_{n=1}^{\frac{N}{2}}(2n-1)$ 
    - 资源块分配：创立 $\lfloor \frac{N}{2} \rfloor$ 个 branches 分支，一个branch视作一对车辆对，每辆车一个RB，一共两个RB，枚举 $C(k,2)$ 种可能
    - RB的第 j 个 branch 和 RSU 的第 j 个 branch配对
- **联邦强化学习 (Federated RL)：**
  * **目的：** 上面的训练是RSU独立和车辆独立的，引入RL为了加速训练并利用所有车辆的经验。
  * **机制：** 车辆在本地训练模型，并定期将参数上传给 RSU；RSU 聚合（平均）参数后广播回车辆，更新全局模型。具体见 Algorithm 2



#### MMW-RCSF

##### 文档

论文：  [MMW-RCSF.pdf](2025.11.29-MMW-RCSF\MMW-RCSF.pdf) 

AI+个人修正详细理解： [MMW-RCSF-reading](2025.11.29-MMW-RCSF\reading.md) 

AI翻译：[MMW-RCSF_chinese](2025.11.29-MMW-RCSF\MMW-RCSF_chinese.md) 



##### 简要理解

这篇论文提出了一种针对路侧异步**毫米波雷达**和**相机**的新型**时空同步**方法，旨在解决路侧不同传感器在时间和空间上缺乏统一基准的问题，从而实现更精确的**传感器数据融合**。

时间上对齐雷达和相机的时间，空间上都转换成世界坐标（如carla的坐标格式）

整个流程见图2，分以下五个步骤

* **数据预处理**：雷达和相机的采样率是不一样的，例如相机是25fps (40ms) ，雷达是20HZ (50ms) 。采用线性拟合，就是假设两个采样点之间变化率平均，雷达拟合相机采样率，如图3和公式1所示。
* **基于场景特征的预标定：** 取四个距离相近的车道线角点，依据已知的车道线标准等静态特征，计算出雷达坐标转世界坐标的转换矩阵各系数的值，用计算出的转换矩阵，把相机的像素坐标转成世界坐标
* **多目标跟踪与噪声估计：** 对两个传感器的数据进行独立跟踪，选一条连续轨迹点，用卡尔曼滤波估计各自的固有噪声（空间偏移和速度偏移），最后去噪。
* **虚拟检测线匹配：** 在道路上设置多条“虚拟检测线”，基于车辆通过的时间和车头时距（Time Headway）来匹配雷达和相机中的同一目标，从而**计算粗略的时间和空间偏差**（$\Delta T$ 和 $\Delta Y$）。如公式11所示。
* **时空同步优化模型：** 把雷达点云坐标转换成世界坐标，建立了一个包含12个参数（时间偏差、空间偏移、旋转角、图像缩放比例因子等）的非线性优化模型，通过最小化轨迹误差来微调同步结果。Loss如公式17所示（该公式都转成雷达坐标算Loss）



#### FPV-RCNN

##### 文档

论文：  [FPV-RCNN.pdf](2025.12.1-FPV-RCNN\FPV-RCNN.pdf) 

AI+个人修正详细理解： [FPV-RCNN-reading](2025.12.1-FPV-RCNN\reading.md) 

AI翻译：[FPV-RCNN_chinese](2025.12.1-FPV-RCNN\FPV-RCNN_chinese.md) 



##### 简要理解

这篇文章提出了一种名为 **FPV-RCNN**（Fusion PV-RCNN）的深度特征融合框架，作用如下

- 降低传输的数据量，同时不降低目标检测效果。
- 车辆有定位误差，校正了这个误差



全文核心见图2，图2注意只有到c部分才有数据共享是吗，b部分只是看着像共享，实际是无关的。



- **FPV-RCNN**：核心特征融合框架，提取出图二右下角的4个部分，下面一个一个讲解
  1. Sensor pose：传感器位姿，基本信息
  2. Proposals：锚框，用CIA-SSD方法提出，效果更好，便于后续锚框合并计算预测分数，这些 Proposals 随后在被用来筛选第4部分的关键点特征，只有位于这些 Proposals 内部的特征点会被选中 。
  3. **selected keypoints coordinates**：用于**坐标误差校正**，设计了一个基于最大一致性原则（Maximum Consensus Principle）的定位误差校正模块，利用环境特征（杆、墙、车辆）来修正车辆间的定位偏差，环境特征通过第4部分提取的3D关键点特征经过分类器分类，因为只用于坐标误差校正，因此只共享各类关键点的坐标。
  4. **selected features**：文章提出基于3D物体检测器 PV-RCNN 的框架，先同样提取出BEV特征图，通过Furthest Points Sampling (FPS) 方法先选取**关键点**，把与关键点有关的信息组合组成**关键点特征**，后续融合只用这个关键点特征，达到进一步精简信息的效果，过程见图3。
- **数据压缩**：提炼关键点特征是压缩，后续用**Draco编码**进一步压缩共享信息



#### Coopernaut
