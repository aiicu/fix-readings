# DAIR-V2X：一个用于车路协同3D目标检测的大规模数据集

**作者：** Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, Zaiqing Nie
**机构：** 清华大学智能产业研究院 (AIR), 百度公司, 清华大学计算机科学与技术系, 中国科学院大学

---

### 摘要

[cite_start]自动驾驶由于缺乏全局视角以及远距离感知能力的限制，面临着巨大的安全挑战 [cite: 8][cite_start]。业界广泛认为，要实现 L5 级自动驾驶，车路协同是必不可少的 [cite: 9][cite_start]。然而，目前仍然**没有**来自真实场景的数据集可供计算机视觉研究人员用于研究车路协同相关问题 [cite: 10][cite_start]。为了加速车路协同自动驾驶（VICAD）的计算机视觉研究和创新，我们发布了 DAIR-V2X 数据集，这是第一个用于 VICAD 的大规模、多模态、多视角的真实场景数据集 [cite: 11][cite_start]。DAIR-V2X 包含 71254 帧激光雷达（LiDAR）数据和 71254 帧相机图像，所有帧都捕获自带有 3D 标注的真实场景 [cite: 12][cite_start]。文中介绍了车路协同 3D 目标检测问题（VIC3D），将其定义为利用来自车辆和路端基础设施的传感器输入，协同定位和识别 3D 问题 [cite: 13][cite_start]。除了解决传统的 3D 目标检测问题外，VIC3D 的解决方案还需要考虑车辆和路端传感器之间的时间异步问题以及它们之间的数据传输成本 [cite: 14][cite_start]。此外，我们提出了时间补偿后融合（TCLF），这是一个基于 DAIR-V2X 的 VIC3D 任务后融合框架基准 [cite: 15][cite_start]。数据、代码和更多最新信息请访问 https://thudair.baai.ac.cn/index 和 https://github.com/AIR-THU/DAIR-V2X [cite: 16]。

---

### 1. 引言

[cite_start]自动驾驶（AD）无疑是当前占据公众视野和想象力的最热门话题之一 [cite: 18][cite_start]。深度神经网络的成功带来了解决自动驾驶核心需求的希望，即通过点云 [15, 20, 28]、图像 [7,18] 或多模态数据 [21, 24] 感知周围环境 [cite: 20][cite_start]。尽管最近取得了巨大进展，但由于缺乏全局视角和远距离感知能力的限制，自动驾驶仍然面临着巨大的安全挑战 [cite: 33][cite_start]。业界广泛认为，要实现 L5 级自动驾驶，车路协同是必不可少的 [cite: 34][cite_start]。同时利用车辆和路端传感器会带来许多显著优势，包括提供远超当前视野的全局视角以及覆盖盲区 [cite: 35][cite_start]。V2X（车联网）等通信技术的进步使得利用路端传感器数据成为可能 [3, 22] [cite: 36][cite_start]。然而，目前仍然**没有**来自真实场景的数据集可供研究人员用于研究车路协同相关问题 [cite: 37]。

[cite_start]为了加速车路协同自动驾驶（VICAD）的计算机视觉研究和创新，我们发布了 DAIR-V2X 数据集，这是第一个用于 VICAD 的大规模、多模态、多视角的真实场景数据集 [cite: 42][cite_start]。它包含在交叉路口场景中捕获的 71254 帧激光雷达数据和 71254 帧相机图像，其中装备精良的车辆通过部署了路端传感器的交叉路口 [cite: 43][cite_start]。40% 的帧捕获自路端传感器，60% 的帧捕获自车辆传感器 [cite: 44][cite_start]。所有数据均由专家标注员精确标注 [cite: 45][cite_start]。该数据集覆盖了 10 公里的城市道路、10 公里的高速公路、28 个交叉路口和 38 平方公里的驾驶区域，包含多样的天气和光照变化 [cite: 46][cite_start]。更多细节见表 1 [cite: 47]。

[cite_start]在本文中，我们介绍了车路协同 3D 目标检测（VIC3D）任务，该任务定义为利用来自车辆和路端的传感器输入，协同定位和识别 3D 目标的问题 [cite: 48][cite_start]。除了解决传统的 3D 目标检测问题外，VIC3D 的解决方案还需要考虑车辆和路端传感器之间的时间异步问题和数据传输成本 [cite: 49]。

[cite_start]为了解决 VIC3D 目标检测任务并促进未来的研究，我们还在本文中引入了 VIC3D 目标检测基准 [cite: 50][cite_start]。对于时间异步问题较少的数据，我们同时实现了前融合（early fusion）和后融合（late fusion）方法 [cite: 51][cite_start]。结果表明，融合方法的平均精度比仅使用单一视角信息的检测器高出 10 到 20 个点 [cite: 52][cite_start]。结果还表明，前融合可以比后融合获得更好的性能，但需要更多的数据传输 [cite: 53][cite_start]。通过 DAIR-V2X 数据集，我们期待未来的研究能实现性能与带宽的权衡 [cite: 54][cite_start]。对于具有严重时间异步问题的数据，我们提出了时间补偿后融合（TCLF）框架，该框架可以有效缓解时间异步问题 [cite: 55]。

我们工作的主要贡献如下：
* [cite_start]我们发布了 DAIR-V2X 数据集，这是第一个用于车路协同自动驾驶的大规模数据集 [cite: 57][cite_start]。所有帧均捕获自带有 3D 标注的真实场景 [cite: 58]。
* [cite_start]我们将利用车辆和路端传感器输入协同定位和识别 3D 目标的问题定义为 VIC3D [cite: 59]。
* [cite_start]我们引入了 VIC3D 目标检测和单视角 3D 目标检测任务的基准 [cite: 60][cite_start]。结果显示了车路协同在 VIC3D 目标检测中的有效性 [cite: 61][cite_start]。特别是，我们提出了时间补偿后融合框架以缓解时间异步问题 [cite: 62]。

---

### 2. 相关工作

#### 2.1. 自动驾驶数据集
[cite_start]近年来，越来越多的自动驾驶数据集发布，极大地推动了自动驾驶研究的发展 [cite: 65][cite_start]。像 SYNTHIA [19] 和 Cityscapes [5] 这样的数据集主要关注图像的 2D 标注 [cite: 66][cite_start]。KITTI [10] 和 nuScenes [2] 是多模态数据集，提供相机图像以及激光雷达点云 [cite: 67][cite_start]。然而，上述所有数据集仅提供来自单车视角的数据 [cite: 68][cite_start]。V2X-SIM [16] 是生成多车视角数据集的一次尝试，但该数据集是由模拟器生成的，而非从真实场景捕获 [cite: 69][cite_start]。与这些数据集相比，我们的 DAIR-V2X 数据集是第一个从真实场景捕获的、用于 VICAD 的大规模、多模态、多视角数据集，并包含从车路协同视角捕获的数据 [cite: 70][cite_start]。表 1 展示了我们的数据集与其他数据集的比较。在 DAIR-V2X 中，我们还提供了一个由多源路端图像和 3D 标注组成的 Rope3D [29] 数据集，供那些对单目 3D（Mono3D）目标检测和领域适应感兴趣的人使用 [cite: 90]。

[cite_start]**表 1. 自动驾驶相关数据集的详细比较** [cite: 39] [cite_start][cite: 41]
*DAIR-V2X 由 DAIR-V2X-C、DAIR-V2X-V 和 DAIR-V2X-I 组成，其中 DAIR-V2X-C 由车辆和路端传感器共同捕获，DAIR-V2X-V 由车辆传感器捕获，DAIR-V2X-I 由路端传感器捕获。*

| 数据集             | 年份     | 真实/模拟 | 视角         | 图像    | 点云    | 3D 框    | 类别   |
| :----------------- | :------- | :-------- | :----------- | :------ | :------ | :------- | :----- |
| KITTI [10]         | 2012     | 真实      | 单车         | 15k     | 15k     | 200k     | 8      |
| nuScenes [2]       | 2019     | 真实      | 单车         | 1.4M    | 400k    | 1.4M     | 23     |
| Waymo Open [23]    | 2019     | 真实      | 单车         | 1M      | 200k    | 12M      | 4      |
| ApolloScape [12]   | 2018     | 真实      | 单车         | 144k    | 0       | 70k      | 8-35   |
| BBD100K [30]       | 2020     | 真实      | 单车         | 100M    | 0       | 0        | 10     |
| ONCE [17]          | 2021     | 真实      | 单车         | 7M      | 1M      | 417k     | 5      |
| SYNTHIA [19]       | 2016     | 模拟      | 单车         | 213k    | 0       | -        | 13     |
| V2X-Sim [16]       | 2021     | 模拟      | 多车         | 0       | 10k     | 26.6k    | 2      |
| highD [13]         | 2018     | 真实      | 路端(无人机) | 1.53M   | 0       | 0        | 1      |
| **DAIR-V2X (Our)** | **2021** | **真实**  | **车路协同** | **71k** | **71k** | **1.2M** | **10** |
| DAIR-V2X-C         | 2021     | 真实      | 车路协同     | 39k     | 39k     | 464k     | 10     |
| DAIR-V2X-V         | 2021     | 真实      | 单车         | 22k     | 22k     | 239k     | 10     |
| DAIR-V2X-I         | 2021     | 真实      | 路端         | 10k     | 10k     | 493k     | 10     |

#### 2.2. 3D 检测
[cite_start]3D 目标检测是自动驾驶成功的先决条件 [cite: 92][cite_start]。许多技术已被引入，大致可分为三类 [cite: 93]。
[cite_start]a) **基于图像的 3D 检测**是指直接从 2D 图像检测 3D 目标的方法 [cite: 94][cite_start]。ImVoxelNet [7] 是一个从图像进行预测的好例子 [cite: 95]。
[cite_start]b) **基于点云的 3D 检测**代表仅从点云进行 3D 目标检测的方式 [cite: 96][cite_start]。PointPillars [15]、SECOND [27] 和 3DSSD [28] 就是这样从点云获得令人信服的检测结果的方法 [cite: 97]。
[cite_start]c) **基于多模态的 3D 检测**同时使用图像和点云进行预测 [cite: 98][cite_start]。Pointpainting [24] 和 MVXNet [21] 是融合图像和 LiDAR 特征来预测 3D 边界框的实践 [cite: 99]。

[cite_start]虽然 3D 目标检测最近取得了很大进展，但仍有一些棘手的问题有待解决，如盲区和远距离感知弱 [cite: 100][cite_start]。为了探索如何利用基础设施信息来解决上述问题，我们基于本文提出的数据集进行了 VIC3D 目标检测 [cite: 101]。

#### 2.3. 多传感器融合
[cite_start]多传感器融合 [26] 是整合不同传感器收集的异构信息，以缓解依赖单一传感器的系统的不确定性和脆弱性 [cite: 103][cite_start]。根据融合阶段，多传感器融合可分为前融合、中融合和后融合 [cite: 104]。
[cite_start]a) 在**前融合**中，来自不同传感器的原始数据被直接传输和融合 [9] [cite: 105]。
[cite_start]b) 在**中融合**中，融合的是从模型中提取的特征等中间表示 [4,21] [cite: 106]。
[cite_start]c) 在**后融合**中，融合的是预测输出，如目标的 3D 信息 [cite: 107]。

[cite_start]VIC3D 可以被视为多传感器问题的一个变体，因此可以考虑之前的融合方法来整合基础设施信息 [cite: 108][cite_start]。然而，除了多传感器融合的挑战外，VIC3D 还面临着由时间异步问题和数据传输限制引起的困难 [cite: 109]。

#### 2.4. V2X 协同感知
[cite_start]V2X 旨在在复杂交通环境中建立车辆与其他设备之间的通信系统 [cite: 111][cite_start]。目前的 V2X 研究主要集中在 V2V（车对车）和 V2I（车对路）领域 [cite: 112][cite_start]。V2VNet [25] 是 V2V 的开创性工作，它广播压缩的中间特征并传播从附近车辆接收到的消息以生成运动预测 [cite: 113][cite_start]。V2I 的工作 [6,31] 利用路端 LiDAR 数据生成并广播检测结果 [cite: 114][cite_start]。然而，这些方法都没有在从真实场景捕获的数据集上进行过验证 [cite: 115][cite_start]。这可能会导致理论与实践之间存在巨大差距 [cite: 116][cite_start]。因此，我们发布了 DAIR-V2X 数据集以推动该领域的进一步研究 [cite: 117]。

---

### 3. DAIR-V2X 数据集

[cite_start]为了促进 VICAD 的研究，我们发布了 DAIR-V2X，这是一个用于车路协同的大规模、多模态、多视角真实场景 3D 标注数据集 [cite: 119][cite_start]。在这里，我们将描述我们如何设置路端和车辆传感器、选择感兴趣的场景、标注数据集以及保护第三方隐私 [cite: 121]。

#### 3.1. 设置
**设备。** 数据采集设备由路端传感器和车辆传感器组成。
[cite_start]a) **路端传感器。** 从北京高级别自动驾驶示范区选出的 28 个交叉路口中，每个都部署了四对 300 线 LiDAR 和高分辨率相机 [cite: 124, 127][cite_start]。DAIR-V2X 数据集仅选取其中的一对 [cite: 128]。
[cite_start]b) **车辆传感器。** 一个 40 线 LiDAR 和一个前视高质量相机安装在自动驾驶车辆的顶部 [cite: 129][cite_start]。具体布局如图 2 所示，精确细节显示在表 2 中 [cite: 130]。

[cite_start]**表 2. DAIR-V2X 中的关键传感器规格** [cite: 125, 126]
*Veh. 代表车辆视角，Inf. 代表路端视角。*

| 传感器             | 详情                                                         |
| :----------------- | :----------------------------------------------------------- |
| **路端 LiDAR**     | 300 线, 10Hz 采集频率, 100° 水平 FOV, -30° 到 10° 垂直 FOV   |
| **路端相机**       | $\le280m$ 范围, ±3cm 精度, RGB, 25Hz 采集频率, 1920x1080 分辨率, JPEG 压缩 |
| **车端 LiDAR**     | 40 线, 10Hz 采集频率, 360° 水平 FOV, -30° 到 10° 垂直 FOV, $\le200m$ 范围, $\pm0.33^{\circ}$ 垂直分辨率 |
| **车端相机**       | RGB, 20Hz 采集频率, 1920x1080 分辨率, JPEG 压缩              |
| **车端 GPS & IMU** | 1000HZ 更新率                                                |

[cite_start]**坐标系。** DAIR-V2X 上有 5 种类型的坐标系，即 LiDAR 坐标系、相机坐标系、图像坐标系、世界坐标系和定位坐标系 [cite: 131][cite_start]。LiDAR 坐标系的原点位于 LiDAR 传感器的中心，x 轴向前为正，y 轴向左为正，z 轴向上为正 [cite: 132][cite_start]。路端 LiDAR 坐标系是从其原始系统转换而来的，原始系统与地面有一个倾斜角 [cite: 133][cite_start]。装备车辆的实时相对位姿是从 GPS/IMU 结合 SLAM 和局部地图获得的 [cite: 134][cite_start]。还有人工二次标注确认以确保标定精度 [cite: 135][cite_start]。LiDAR 到相机的变换是通过乘以 LiDAR 到世界和世界到相机的变换获得的 [cite: 136]。

#### 3.2. 数据采集
[cite_start]**收集。** 我们在采集区域驾驶装备精良的车辆，并分别保存相应的车辆帧和路端帧 [cite: 138][cite_start]。原始数据收集后，我们人工选择了 100 个具有代表性的、时长为 20 秒的场景 [cite: 139][cite_start]。这些场景包括车辆和路端数据，其中车辆驶过部署了设备的交叉路口 [cite: 140][cite_start]。我们从两侧以 10Hz 采样关键帧以形成 DAIR-V2X-C [cite: 141][cite_start]。在 DAIR-V2X-C 中，需要注意的是，由于车辆传感器和路端传感器之间的异步触发，车辆帧与其最近的路端帧之间的时间戳差异可能会略有不同 [cite: 142][cite_start]。我们从大约 350 个时长为 60 秒的仅车辆片段中额外采样 22K 帧以形成 DAIR-V2X-V，并从大约 150 个仅路端片段中采样 10K 帧以形成 DAIR-V2X-I，以扩充数据集 [cite: 143, 144][cite_start]。与 DAIR-V2X-C 中的单视角数据相比，DAIR-V2X-V 和 DAIR-V2X-I 包含更多样化的场景，仅提高单视角性能将更具挑战性 [cite: 145]。

[cite_start]**标注。** 经过多重验证步骤和修正过程，专家标注员分别为路端帧和车辆帧制作了高质量的标注 [cite: 146][cite_start]。具体来说，标注员详尽地标注了每个图像和点云帧中 10 个目标类别，包括其类别属性、遮挡状态、截断状态以及建模为 x, y, z, 宽度, 长度, 高度和偏航角的 7 维立方体 [cite: 147][cite_start]。10 个类别包括不同的车辆、行人、不同的骑行者 [cite: 148][cite_start]。此外，专家还精心标注了相机图像中的物体，使用建模为 x, y, 宽度和长度的矩形边界框 [cite: 149]。

[cite_start]值得一提的是，我们还对车辆和路端帧对的协同标注进行了半自动标注 [cite: 150][cite_start]。我们首先从 DAIR-V2X-C 中选择车辆和路端帧对 [cite: 151][cite_start]。所选对的两帧之间的时间戳差异小于 10ms（我们称之为同步情况，定义在 4.1 节。为了获得更多协同标注，我们将阈值从 10ms 扩展到 30ms）[cite: 152][cite_start]。接下来，我们将路端 3D 框转换为车辆 LiDAR 坐标系，并融合车辆标注和路端标注 [cite: 153][cite_start]。对于路端标注中的每个 3D 框，如果我们在车辆标注中找不到任何具有相同位置和类别的 3D 框，我们将路端 3D 框添加到车辆标注中 [cite: 154][cite_start]；通过这种方式，我们获得了车路协同标注 [cite: 155][cite_start]。我们人工监督并调整协同标注以生成更准确的标注 [cite: 156][cite_start]。这里我们选取了 9331 个路端帧和车辆帧以及协同标注来形成用于我们的 VIC3D 目标检测基准的 VIC-Sync 数据集 [cite: 157]。

[cite_start]**隐私保护。** 整个数据集在公开发布前都经过了脱敏处理。为了符合当地法律法规，我们删除了所有定位信息，包括道路名称、地图数据和定位信息，以确保我们的数据集符合要求 [cite: 158][cite_start]。此外，我们利用专业的标注工具对所有涉嫌侵犯隐私的信息（包括路牌、车牌和人脸）进行模糊处理，以保护隐私并避免侵犯个人权利 [cite: 159]。

---

### 4. 任务与指标

[cite_start]自动驾驶由于缺乏全局视角和远距离感知能力的限制，面临着巨大的安全挑战 [cite: 161][cite_start]。由于 3D 目标检测是自动驾驶中的关键感知任务之一，在本文中，我们关注车路协同（VIC）3D 目标检测任务，即车辆接收并整合来自路端的信息以定位和识别其周围的物体 [cite: 163]。与传统的多传感器 3D 目标检测任务相比，VIC3D 目标检测具有以下替代特征：
* [cite_start]**传输成本。** 受物理通信条件的限制，应从路端传输较少的数据以减少带宽消耗，缓解时间延迟，并满足实时性要求 [cite: 165][cite_start]。因此，VIC3D 目标检测的解决方案需要平衡性能和传输成本之间的权衡 [cite: 166]。
* [cite_start]**时间异步。** 由于异步触发和传输成本造成的时间延迟，车辆传感器和路端传感器数据的时间戳是不同的，从而产生时空误差 [cite: 167][cite_start]。因此，在解决 VIC3D 时应考虑时间同步 [cite: 168]。

[cite_start]为了更好地定义 VIC3D 目标检测任务，我们将在本节中给出 VIC3D 目标检测的详细定义，然后提供两个指标来衡量检测性能和传输成本 [cite: 169]。

#### 4.1. VIC3D 目标检测
[cite_start]VIC3D 目标检测可以被定义为一个优化问题，即在考虑传输成本的同时，有效地整合路端和车辆信息来定位和识别 3D 目标 [cite: 171][cite_start]。这里我们讨论 VIC3D 的输入和输出应该是什么 [cite: 172]。

[cite_start]**输入。** VIC3D 的输入由来自车辆和路端的数据组成 [cite: 173]。
* [cite_start]**车辆帧 $I_{v}(t_{v})$：** 在 $t_{v}$ 时刻捕获及其相对位姿 $M_{v}(t_{v})$，其中 $I_{v}(\cdot)$ 表示车辆传感器的捕获函数 [cite: 174]。
* [cite_start]**路端帧 $I_{i}(t_{i})$：** 在 $t_{i}$ 时刻捕获及其相对位姿 $M_{v}(t_{i})$，其中 $I_{i}(\cdot)$ 表示路端传感器的捕获函数 [cite: 175]。
[cite_start]注意，$t_{i}$ 应该早于 $t_{v}$，因为从路端到车辆的数据传输会造成时间延迟 [cite: 176][cite_start]。考虑到物体在微小的时间间隔内移动非常轻微，空间偏移可以忽略不计，我们将 $|t_{v}-t_{i}|\le10ms$ 的情况称为**同步情况**（即 $t_{v}\approx t_{i}$）[cite: 177][cite_start]。同样，我们将 $|t_{v}-t_{i}|>10ms$ 的情况称为**异步情况** [cite: 178][cite_start]。此外，我们允许在解决 VIC3D 时使用 $I_{i}(t_{i})$ 之前的更多路端帧，以充分利用路端计算资源 [cite: 179]。

[cite_start]**真值 (Ground Truth)。** VIC3D 目标检测的输出包含 3D 信息，如车辆周围物体的位置、类别和方向 [cite: 180]。VIC3D 相应的真值是路端和车辆真值的融合结果，可以公式化为：
$$GT=GT_{v}\cup GT_{i}, \quad (1)$$
[cite_start]其中 $GT_{v}$ 是车辆传感器感知的真值，$GT_{i}$ 是路端传感器感知的真值 [cite: 185]。

[cite_start]VIC3D 主要用于提高自动驾驶车辆的感知性能 [cite: 186][cite_start]。相比于 $t_{i}$ 时刻，我们更关心 $t_{v}$ 时刻以自我为中心的周围环境和物体的 3D 信息。因此，$GT_{v}$ 和 $GT_{i}$ 都应基于 $t_{v}$ 时刻 [cite: 187][cite_start]。然而，从路端捕获的和从车辆捕获的输入帧的时间戳可能不同，即 $t_{v}\ne t_{i}$ [cite: 188][cite_start]。这不仅给模型预测中融合路端信息带来了挑战，也给生成真值带来了巨大问题 [cite: 189][cite_start]。这是因为在 $t_{i}$ 时刻路端帧中标注的物体可能在 $t_{v}$ 时刻移动到了不同的位置，而我们无法直接获得 $t_{v}$ 时刻的路端帧来进行标注 [cite: 190]。

[cite_start]针对这些困难，我们讨论如何基于 DAIR-V2X 生成 VIC3D 的真值 [cite: 191]。
* [cite_start]**同步情况 (即 $t_{v}\approx t_{i}$)。** 在这种条件下，出现在车辆帧 $I_{v}(t_{v})$ 中的物体应与其出现在路端帧 $I_{i}(t_{i})$ 中的空间位置相同 [cite: 192][cite_start]。因此，我们可以直接采用 3.2 节中说明的通过半自动标注获得的车辆-路端协同 3D 标注作为真值 [cite: 193]。
* [cite_start]**异步情况 (即 $t_{v}\ne t_{i}$)。** 如果我们可以找到满足 $|t_{v}-t_{i}|\le10ms$ 的路端帧 $I_{i}(t_{\overline{z}}^{\prime})$，我们可以用 $I_{i}(t_{i})$ 生成真值 [cite: 194][cite_start]。如果不能，我们必须估计物体在 $t_{v}$ 时的 3D 状态以生成真值 [cite: 195][cite_start]。这项工作可以在我们未来提供跟踪 ID 后，基于跟踪 ID 和运动学方程进行 [cite: 196]。

#### 4.2. 评估指标
[cite_start]VIC3D 目标检测有两个主要目标：更好的检测性能和更少的传输成本 [cite: 198][cite_start]。我们在下面描述这两个目标的指标 [cite: 199]。

[cite_start]**平均精度 (Average Precision)。** AP（平均精度）是衡量目标检测器性能的流行指标 [8] [cite: 200][cite_start]。我们也使用 AP 来评估以协同标注为真值的 3D 检测性能 [cite: 201][cite_start]。由于我们更关心以自我为中心的周围环境，我们移除了设计区域之外的物体 [cite: 202]。这里我们将设计区域设置为矩形区域 [0, -39.12, 100, 39.12]。

[cite_start]**传输成本。** 我们使用 AB（平均字节数）来衡量传输成本 [cite: 204][cite_start]。这里字节（Byte）是一个由八位组成的数字信息单位 [cite: 205][cite_start]。为了简化问题，我们忽略了传输过程中数据编码器和解码器的时间消耗 [cite: 206][cite_start]。这意味着传输成本越低，时间延迟越小 [cite: 207][cite_start]。从路端传输的数据可以是以下形式之一或组合 [cite: 208]：
* [cite_start]**原始数据**（如图像或点云）包含完整信息，但需要大量的传输成本 [cite: 211]。
* [cite_start]**中间表示**需要的传输成本较低，同时保留了有价值的信息，可能会实现更好的性能-传输权衡 [cite: 212][cite_start]。当然，这需要更复杂的设计来提取合适的中间表示 [cite: 213]。
* [cite_start]**目标级输出**直接提供 3D 目标信息。虽然传输效率高，但可能会丢失有价值的信息 [cite: 214]。
* [cite_start]其他辅助信息如场景流有助于缓解时间异步问题 [cite: 215]。

---

### 5. 基准测试

[cite_start]在本节中，我们在 DAIR-V2X 数据集上提供了 VIC3D 目标检测基准和单视角（SV）3D 目标检测基准，分析了它们的特点并为未来的研究提出了建议 [cite: 217]。

#### 5.1. VIC3D 目标检测基准
[cite_start]我们在从 DAIR-V2X-C 中提取的 VIC-Sync 数据集上提供了 VIC3D 目标检测的基准，如 3.2 节所述 [cite: 219][cite_start]。该数据集由 9311 对路端和车辆帧及其作为真值的协同标注组成 [cite: 220][cite_start]。此外，我们在基准测试中考虑了路端帧和车辆帧之间的时间异步，这主要是由采样率差异和传输延迟引起的 [cite: 221][cite_start]。为了模拟时间异步现象，我们将 VIC-Sync 数据集中的每个路端帧替换为原始路端帧之前的第 k 帧，以构建用于基准测试的 VIC-Async-k 数据集 [cite: 222][cite_start]。在我们的实验中，我们设置 $k=1, 2$。我们将 VIC-Sync 和 VIC-Async-k 数据集按 5:2:3 分别划分为训练/验证/测试部分 [cite: 223][cite_start]。我们使用协同标注来评估车辆中心视角下的检测结果 [cite: 224][cite_start]。实验结果如表 3 所示 [cite: 225]。

[cite_start]**表 3. DAIR-V2X-C 上的 VIC3D 目标检测基准** [cite: 209, 210]

| 模态     | 融合方式 | 模型              | 数据集      | 3D AP (IoU 0.5) 总体 | 3D AP (0-30m) | 3D AP (30-50m) | 3D AP (50-100m) | BEV AP (IoU 0.5) 总体 | BEV AP (0-30m) | BEV AP (30-50m) | BEV AP (50-100m) | AB (Byte)  |
| :------- | :------- | :---------------- | :---------- | :------------------- | :------------ | :------------- | :-------------- | :-------------------- | :------------- | :-------------- | :--------------- | :--------- |
| **图像** | 仅车辆   | ImvoxelNet [7]    | VIC-Sync    | 12.03                | 19.93         | 7.25           | 2.28            | 17.66                 | 27.34          | 17.20           | 2.82             | 0          |
| **图像** | 仅路端   | ImvoxelNet [7]    | VIC-Sync    | 26.56                | 16.25         | 14.43          | 25.31           | 8.58                  | 34.20          | 9.81            | 31.40            | 102.32     |
| **图像** | 后融合   | ImvoxelNet [7]    | VIC-Sync    | 17.61                | 32.02         | 23.28          | 21.21           | 37.75                 | 20.38          | 12.99           | 102.32           |            |
| **点云** | 仅车辆   | PointPillars [15] | VIC-Sync    | 31.33                | 50.03         | 25.58          | 16.54           | 35.06                 | 53.73          | 28.65           | 16.00            | 0          |
| **点云** | 仅路端   | PointPillars [15] | VIC-Sync    | 17.62                | 27.48         | 10.98          | 12.63           | 24.40                 | 30.55          | 21.47           | 14.16            | 336.16     |
| **点云** | 后融合   | PointPillars [15] | VIC-Sync    | 41.90                | 37.65         | 32.72          | 9.17            | 47.96                 | 37.65          | 42.40           | 13.07            | 336.16     |
| **点云** | 前融合   | PointPillars [15] | VIC-Sync    | 53.07                | 60.38         | 33.05          | 18.84           | 64.08                 | 55.80          | 22.08           | 36.17            | 1382275.75 |
| **点云** | 后融合   | PointPillars [15] | VIC-Async-1 | 40.21                | 34.17         | 28.07          | 15.50           | 46.41                 | 34.10          | 35.62           | 19.20            | 341.08     |
| **点云** | 后融合   | PointPillars [15] | VIC-Async-2 | 35.29                | 29.40         | 32.16          | 13.44           | 40.65                 | 38.05          | 32.35           | 15.88            | 306.79     |
| **点云** | 前融合   | PointPillars [15] | VIC-Async-1 | 47.47                | 48.88         | 58.86          | 30.89           | 51.67                 | 63.09          | 52.70           | 34.72            | 1362216.0  |
| **点云** | TCLF     | PointPillars [15] | VIC-Async-1 | 40.79                | 34.67         | 33.91          | 15.76           | 46.80                 | 38.24          | 33.36           | 19.40            | 539.60     |
| **点云** | TCLF     | PointPillars [15] | VIC-Async-2 | 36.72                | 29.69         | 29.41          | 14.52           | 41.67                 | 34.27          | 36.78           | 17.18            | 506.70     |

##### 5.1.1 基线方法
[cite_start]这里我们展示了几种不同模态和融合方法的 VIC3D 目标检测基线 [cite: 227]。

[cite_start]**LiDAR 检测后融合基线。** 为了展示利用路端和车辆数据带来的性能提升，我们实现了一个包含路端检测器和车辆检测器的后融合框架 [cite: 228][cite_start]。首先，我们选择 PointPillars [15] 作为 3D 检测器，并分别使用 VIC-Sync 中的路端视角和车辆视角数据训练两个检测器 [cite: 229][cite_start]。然后，我们将路端预测结果转换为车辆 LiDAR 坐标系，并使用基于欧几里得距离测量和匈牙利算法 [14] 的匹配器合并预测结果以生成融合结果 [cite: 230][cite_start]。为了说明时间异步问题，我们还在 VIC-Async-k 数据集上实现了 LiDAR 检测后融合基线 [cite: 231][cite_start]。此外，基于跟踪和状态估计，我们提出了**时间补偿后融合（TCLF）**框架 [cite: 232][cite_start]。TCLF 主要由以下三部分组成：1) 利用两个相邻的路端帧估计物体的速度 [cite: 233][cite_start]。2) 估计路端物体在 $t_{v}$ 时刻的状态。3) 按照 LiDAR 后融合基线的方式融合估计的路端预测和车辆预测 [cite: 234][cite_start]。TCLF 框架的细节如图 3 所示 [cite: 235][cite_start]。注意，我们还报告了仅使用路端数据和仅使用车辆数据的评估结果，分别命名为 Veh.-Only 和 Inf.-Only [cite: 236][cite_start]。评估结果见表 3 [cite: 237]。

[cite_start]**图像检测后融合基线。** 为了检查纯图像 VIC3D 目标检测，我们也实现了仅使用路端图像和车辆图像的后融合框架 [cite: 238][cite_start]。我们选择 ImvoxelNet [7] 作为 3D 检测器，并分别使用 VIC-Sync 训练数据的相应部分训练路端检测器和车辆检测器 [cite: 239][cite_start]。我们参照 LiDAR 检测后融合实现了图像检测后融合 [cite: 240]。

[cite_start]**LiDAR 检测前融合基线。** 为了探索在原始数据层面的融合效果，我们在 VIC-Sync 数据集上实现了以 PointPillars [15] 为 3D 检测器的前融合 [cite: 263][cite_start]。我们首先将 VIC-Sync 数据集中的路端点云转换为车辆 LiDAR 坐标系，然后融合路端点云和车辆点云 [cite: 264][cite_start]。我们直接使用融合点云训练和评估检测器 [cite: 265][cite_start]。为了进一步说明时间异步问题，我们还在 VIC-Async-k 数据集上实现了 PointPillars [15] 的前融合 [cite: 266]。

##### 5.1.2 分析
[cite_start]这里我们分析 5.1.1 节中 VIC3D 目标检测基准方法的特性 [cite: 271]。

[cite_start]**协同视角 vs. 单视角。** 我们比较了是否同时使用路端数据和车辆数据的方法性能 [cite: 272][cite_start]。在表 3 中，无论是基于图像还是基于 LiDAR，无论是基于 VIC-Sync 数据集还是 VIC-Async-k 数据集，后融合的检测性能都远优于 Veh.-Only 或 Inf.-Only [cite: 273][cite_start]。例如，在 VIC-Sync 数据集上，带有后融合的 LiDAR 检测在 3D 检测上获得了 41.90 的总 AP，在 BEV 检测上获得了 47.96 的总 AP [cite: 274][cite_start]。然而，仅使用车辆数据的 LiDAR 检测在 3D 检测上仅获得 31.33% 的总 AP，在 BEV 检测上仅获得 35.06% 的总 AP；仅使用路端数据的 LiDAR 检测在 3D 检测上仅获得 17.62% 的总 AP，在 BEV 检测上仅获得 24.40% 的总 AP [cite: 275][cite_start]。实验结果表明，融合路端信息可以有效提高车辆的感知性能 [cite: 276][cite_start]。这主要是因为路端数据提供了补充信息，弥补了车辆的感知视野 [cite: 277][cite_start]。图 4 展示了一个可视化示例 [cite: 278]。

[cite_start]**时间异步 vs. 时间补偿。** 时间异步给融合路端数据带来了挑战 [cite: 279][cite_start]。与 VIC-Sync 数据集上的结果相比，融合 LiDAR 检测的性能在 VIC-Async-k 上显著下降（在 VIC-Async-1 上下降 2 个点，在 VIC-Async-2 上下降 6 个点）[cite: 280][cite_start]。下降主要是由于移动物体的状态变化，导致匹配困难和融合错误 [cite: 281][cite_start]。然而，我们的 TCLF 可以有效地将 VIC-Async-1 和 VIC-Async-2 上的后融合性能分别提高 0.5% AP 和 1.5% AP，这表明时间补偿可以有效缓解时间异步问题，尤其是当时间延迟较大时 [cite: 282][cite_start]。图 5 提供了一个可视化示例 [cite: 287]。

[cite_start]**前融合 vs. 后融合。** 与后融合相比，无论是在 BEV 还是 3D 基准下，无论是在 VIC-Sync 数据集还是 VIC-Async-1 数据集上，前融合都实现了高达 8% 的 AP 提升 [cite: 288][cite_start]。然而，前融合需要传输整个点云，并承受极高的传输成本，这大约是后融合的 4000 倍 [cite: 289][cite_start]。对于更实际的应用，我们鼓励未来的研究在消耗更少传输带宽的同时实现更好的性能 [cite: 290][cite_start]。我们将来也会发布用于基准测试的特征融合 [cite: 291]。

#### 5.2. SV3D 检测基准
[cite_start]我们为那些对基于 DAIR-V2X-V 和 DAIR-V2X-I 数据集的单视角（SV）3D 检测任务感兴趣的人提供了广泛的 3D 检测基准 [cite: 293][cite_start]。与 DAIR-V2X-C 中的单侧数据相比，这两个数据集更加多样化，实施 3D 目标检测可能更具挑战性 [cite: 294][cite_start]。因此，我们鼓励研究人员致力于提高 DAIR-V2X-V 和 DAIR-V2X-I 上车辆 3D 目标检测或路端 3D 目标的性能 [cite: 295]。

[cite_start]我们将 DAIR-V2X-V 和 DAIR-V2X-I 数据集按 5:2:3 分别划分为训练/验证/测试部分 [cite: 296][cite_start]。我们在两个数据集上分别展示了基于不同模态的多种基线方法：ImvoxelNet [7]、PointPillars [15]、SECOND [27] 和 MVXNet [21] [cite: 297][cite_start]。我们使用与 KITTI [10] 相同的 PASCAL 标准评估 3D 目标检测性能，即根据图像平面中的边界框高度过滤掉远处的物体 [cite: 298][cite_start]。评估使用了三种模式，包括简单（Easy）、中等（Moderate）和困难（Hard）模式 [cite: 299][cite_start]。我们使用 MMDetection3D 框架 [1] 实现了这些基线 [cite: 300][cite_start]。评估结果如表 4 和表 5 所示 [cite: 301]。

[cite_start]**表 4. DAIR-V2X-V 上的 SV3D 检测基准** [cite: 283, 284]

| 模态          | 模型              | 车辆 3D (IoU=0.5) (中/难/易) | 行人 3D (IoU=0.25) (难/易/中) | 骑行者 3D (IoU=0.25) (难/中/易) |
| :------------ | :---------------- | :--------------------------- | :---------------------------- | :------------------------------ |
| **图像**      | ImvoxelNet [7]    | 38.37 / 24.28 / 21.54        | 4.54 / 4.54 / 4.54            | 10.38 / 9.09 / 9.09             |
| **点云**      | PointPillars [15] | 61.76 / 43.45 / 49.02        | 22.39 / 33.40 / 24.68         | 38.24 / 33.80 / 32.35           |
| **点云**      | SECOND [27]       | 69.44 / 57.63 / 59.63        | 38.78 / 43.45 / 39.06         | 44.21 / 39.49 / 37.74           |
| **图像+点云** | MVXNet [21]       | 69.86 / 60.74 / 59.31        | 43.37 / 47.73 / 42.49         | 45.68 / 41.84 / 40.55           |

[cite_start]**表 5. DAIR-V2X-I 上的 SV3D 检测基准** [cite: 285, 286]

| 模态          | 模型              | 车辆 3D (IoU=0.5) (中/难/易) | 行人 3D (IoU=0.25) (难/易/中) | 骑行者 3D (IoU=0.25) (难/易/中) |
| :------------ | :---------------- | :--------------------------- | :---------------------------- | :------------------------------ |
| **图像**      | ImvoxelNet [7]    | 44.78 / 37.58 / 37.55        | 6.81 / 6.746 / 6.73           | 21.06 / 13.17 / 13.57           |
| **点云**      | PointPillars [15] | 63.07 / 54.01 / 54.00        | 38.53 / 37.28 / 37.20         | 38.46 / 22.49 / 22.60           |
| **点云**      | SECOND [27]       | 54.00 / 71.47 / 53.99        | 52.52 / 55.16 / 52.49         | 31.19 / 54.68 / 31.05           |
| **图像+点云** | MVXNet [21]       | 53.76 / 71.04 / 53.71        | 55.83 / 54.45 / 54.40         | 31.06 / 30.79 / 54.05           |

---

### 6. 结论

[cite_start]在本文中，我们介绍了 DAIR-V2X，这是第一个用于车路协同自动驾驶的大规模、多模态、多视角数据集，所有帧均捕获自带有 3D 标注的真实场景 [cite: 303][cite_start]。我们还定义了 VIC3D 目标检测，以此阐述利用来自车辆和路端的传感器输入协同定位和识别 3D 目标的问题 [cite: 304][cite_start]。除了解决传统的 3D 目标检测问题外，VIC3D 的解决方案还需要考虑车辆和路端传感器之间的时间异步问题以及它们之间的数据传输成本 [cite: 305][cite_start]。为了促进未来的研究，我们为检测模型提供了 VIC3D 基准和我们提出的时间补偿后融合框架，以及车辆视角和路端视角数据集上的广泛 3D 检测基准 [cite: 306][cite_start]。结果表明，整合路端传感器数据比单车 3D 检测平均提高了 15% 的 AP，且 TCLF 可以缓解时间异步问题 [cite: 307]。

### 致谢
[cite_start]我们感谢百度公司的 Fan Yang, Ruiwen Zhang, Wenyue Wu, 和 Xiao Wang 在数据处理方面的支持 [cite: 309][cite_start]。我们感谢 Jilei Mao, Taohua Zhou, Yingjuan Tang, Zan Mao, 和 Zhiwen Yang 在基准构建方面的支持 [cite: 310][cite_start]。感谢北京高级别自动驾驶示范区、北京车网科技发展有限公司、百度 Apollo 和北京智源人工智能研究院在整个数据集建设和发布过程中给予的支持 [cite: 311]。