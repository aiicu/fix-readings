CoBEVT: 基于稀疏 Transformer 的协作式鸟瞰图语义分割

**作者**: Runsheng Xu¹, Zhengzhong Tu², Hao Xiang¹, Wei Shao³, Bolei Zhou¹, Jiaqi Ma¹†
**机构**: 
1. 加州大学洛杉矶分校 (University of California, Los Angeles)
2. 德克萨斯大学奥斯汀分校 (University of Texas at Austin)
3. 加州大学戴维斯分校 (University of California, Davis)

---



### 摘要

鸟瞰图 (BEV) 语义分割在自动驾驶的空间感知中起着至关重要的作用。尽管最近的文献在 BEV 地图理解方面取得了显著进展，但它们都基于单智能体（single-agent）的摄像头系统。这些解决方案在处理复杂交通场景中的遮挡或检测远距离物体时有时会遇到困难。车对车 (V2V) 通信技术使自动驾驶车辆能够共享感知信息，与单智能体系统相比，显着提高了感知性能和范围。在本文中，我们提出了 **CoBEVT**，这是第一个通用的多智能体、多摄像头感知框架，能够协作生成 BEV 地图预测。为了在底层的 Transformer 架构中高效融合来自多视角和多智能体数据的摄像头特征，我们设计了一个融合轴向注意力模块 (FAX)，它可以捕捉视图和智能体之间稀疏的局部和全局空间交互。在 V2V 感知数据集 OPV2V 上的广泛实验表明，CoBEVT 在协作式 BEV 语义分割方面达到了最先进的性能。此外，CoBEVT 被证明可以推广到其他任务，包括 1) 单智能体多摄像头 BEV 分割，以及 2) 基于多智能体激光雷达 (LiDAR) 系统的 3D 物体检测，均以实时推理速度达到了最先进的性能。代码已在 https://github.com/DerrickXuNu/CoBEVT 开源。

**关键词**: 自动驾驶，BEV 地图理解，车对车 (V2V) 应用

---



### 1. 引言

自动驾驶车辆 (AV) 需要准确的环境感知和强大的在线建图能力，以实现稳健和安全的自动驾驶。AV 通常位于地面平面上，因此自然而在然地会在鸟瞰图 (BEV) 地图中表示周围环境的语义和几何信息。将多摄像头视图投影到整体 BEV 空间中，在空间和时间上保留道路元素的位置和尺度方面具有明显的优势，这对包括场景理解和规划在内的各种自动驾驶任务至关重要。它还为实际部署提供了一种可扩展的基于视觉的解决方案，而不依赖昂贵的 LiDAR 传感器。

地图视图（或 BEV）语义分割是一项基本任务，旨在从单个或多个校准的摄像头输入中预测道路部分。人们已经为精确的基于摄像头的 BEV 语义分割做出了巨大努力。最流行的技术之一是利用深度信息推断摄像头视图与规范地图之间的对应关系。另一类工作则直接学习摄像头到 BEV 空间的转换，无论是隐式还是显式地使用基于注意力的模型。尽管结果令人鼓舞，但基于视觉的感知系统具有内在的局限性：摄像头传感器已知对物体遮挡和有限的景深敏感，这可能导致在严重遮挡或远离摄像头镜头的区域性能较差。

车对车 (V2V) 通信技术的最新进展使得克服单智能体视线感知的局限性成为可能。也就是说，多个互联的自动驾驶车辆可以通过广播相互共享其感知信息，从而提供同一场景的多个视点。之前的一些工作已经证明了利用 LiDAR 传感器进行协作感知的有效性。然而，这种 V2V 协作是否、何时以及如何使基于摄像头的感知系统受益尚未得到探索。

在本文中，我们提出了 CoBEVT，这是首个利用多智能体多摄像头传感器，通过稀疏视觉 Transformer 协作生成 BEV 分割地图的框架。图 1 展示了所提出的框架。每个 AV 使用 SinBEVT Transformer 从其摄像头组计算自己的 BEV 表示，并在压缩后将其传输给其他车辆。接收者（即其他 AV）将接收到的 BEV 特征转换到其坐标系上，并采用提出的 FuseBEVT 进行 BEV 级聚合。这两个 Transformer 的核心成分是一个新颖的融合轴向注意力 (FAX) 模块，它可以通过局部和全局空间稀疏性，在所有智能体或摄像头视图的整个 BEV 或摄像头图像空间上进行搜索。FAX 包含用于建模长距离依赖关系的全局注意力，以及用于聚合区域详细特征的局部注意力，且计算复杂度低。我们在 V2V 感知数据集上的广泛实验表明，CoBEVT 比单智能体基线和领先的多智能体融合模型分别获得了 22.7% 和 6.9% 的性能提升。

此外，我们展示了所提出框架在两个额外任务中的泛化能力。首先，我们单独评估 SinBEVT 用于单智能体多视图 BEV 分割。其次，我们在不同的传感器模态——多智能体 LiDAR 融合上验证了注意力融合。我们在 nuScenes 数据集和 OPV2V 的 LiDAR 赛道上的实验表明，CoBEVT 表现出色的性能，并能够很好地推广到许多其他任务。

**我们的贡献包括：**
* 我们提出了用于协作式基于摄像头的 BEV 语义分割的通用 Transformer 框架 (CoBEVT)。CoBEVT 提供了卓越的性能和灵活性，在多智能体基于摄像头、单车多视图 BEV 语义分割以及多智能体基于 LiDAR 的 3D 检测方面均取得了最先进的结果。
* 我们提出了一种名为融合轴向 (FAX) 注意力的新型稀疏注意力模块，它可以高效地捕获不同智能体或摄像头之间的局部和全局关系。我们构建了两个实例——自注意力 (FAX-SA) 和交叉注意力 (FAX-CA)，以适应不同的应用场景。
* 我们在协作式 BEV 地图分割任务上构建了一个大规模的基准研究，包含总共八个强大的基线模型。广泛的实验结果和消融研究表明了所提出模型的强大性能和效率。

---



### 2. 相关工作

#### 2.1 V2V 感知
V2V 感知利用通信技术使 AV 能够共享其感知信息以增强感知能力。之前的工作主要集中在基于 LiDAR 的协作式 3D 物体检测。一种直接的共享策略是传输原始点云（即早期融合）或检测输出（即后期融合）。然而，它们要么需要大带宽，要么忽略了上下文信息。最近，V2VNet 提出循环从 3D 主干网络提取的中间特征（即中间融合），然后利用空间感知图神经网络进行多智能体特征聚合。OPV2V 遵循类似的传输范式，采用简单的基于智能体的单头注意力来融合所有特征。F-Cooper 使用简单的 maxout 操作来融合特征。DiscoNet 探索了知识蒸馏，通过约束中间特征图以匹配早期融合教师模型中的对应关系。

与之前的多智能体算法相比，我们的 CoBEVT 是第一个采用稀疏 Transformer 来高效且详尽地探索车辆之间相关性的算法。此外，以前的方法主要集中在基于 LiDAR 的协作感知，而我们的目标是提出一种不依赖 LiDAR 设备的低成本基于摄像头的协作感知解决方案。



#### 2.2 BEV 语义分割

BEV 语义分割旨在以摄像头视图为输入，并在 BEV 视图下预测带有周围语义的栅格化地图。该任务的一种常见方法是使用逆透视映射 (IPM) 来学习视图变换的单应性矩阵。由于摄像头图像缺乏显式的 3D 信息，另一类模型包括深度估计以注入辅助 3D 信息。最近，研究人员开始使用 Transformer 或 MLP 直接建模图像到地图的对应关系。VPN 在扁平化的摄像头视图图像特征上使用空间 MLP 模块学习地图视图转换。CVT 根据各自的内参和外参校准为每个单独的摄像头开发位置嵌入。BEVFormer 利用摄像头内参和外参，使用可变形 Transformer 显式计算跨摄像头视图的 BEV 网格感兴趣区域中的空间特征。我们的 CoBEVT 建立在 CVT 之上，但通过我们提出的 3D FAX 注意力进一步改进了 CVT，该注意力更高效，因此支持更大的 BEV 嵌入尺寸以获得更好的精度。此外，我们开发了一种分层架构，可以聚合多尺度摄像头特征，仅以较低的计算成本保留更精细的图像细节。



#### 2.3 视觉中的 Transformer

Transformer 最初是为了自然语言处理而提出的。ViT 首次证明，通过大规模预训练，简单地将图像块视为视觉单词的纯 Transformer 足以完成视觉任务。Swin Transformer 通过在局部（移位）窗口中限制注意力场，进一步提高了纯 Transformer 的通用性和灵活性。对于高维数据，Video Swin Transformer 将 Swin 方法扩展到移位的 3D 时空窗口，以低复杂度实现了高性能。最近的工作集中在改进注意力模型的架构上，包括稀疏注意力、扩大的感受野、金字塔设计、高效替代方案等。我们的工作属于针对高维数据的高效 3D Transformer 模型设计。虽然我们仅验证了所提出的 FAX 注意力在多视图和多智能体自动驾驶感知中的功效，但我们期望其能广泛应用于其他视觉任务，如视频和多模态。

---



### 3. 方法论

我们考虑一个 V2V 通信系统，其中所有 AV 都可以与其他车辆交换感知信息。假设所有智能体的位姿都是准确的，并且传输的消息是同步的，我们提出了一个稳健的协作框架，可以利用多个智能体之间共享的信息来获得整体 BEV 分割地图。CoBEVT 的整体架构如图 1 所示，包括：用于 BEV 特征计算的 SinBEVT（第 3.2 节）、特征压缩和共享（第 3.3 节）以及用于多智能体 BEV 融合的 FuseBEVT（第 3.3 节）。我们提出了一种新颖的 3D 注意力机制，称为融合轴向注意力（FAX，第 3.1 节），作为 SinBEVT 和 FuseBEVT 的核心组件，它可以高效地在局部和全局范围内聚合跨智能体或摄像头视图的特征。稍后我们将展示这种 FAX 注意力具有极大的通用性，在多种感知任务的不同模态上显示出功效，包括基于多视图摄像头的协作/单智能体 BEV 分割和协作式 3D LiDAR 物体检测。



#### 3.1 融合轴向注意力 (FAX)

融合来自多个智能体的 BEV 特征需要跨所有智能体空间位置的局部和全局交互。一方面，相邻的 AV 通常对同一物体有不同的遮挡水平；因此，更关注细节的局部注意力有助于构建该物体上的像素对像素对应关系。以图 2(a) 中的场景为例。主车（Ego vehicle）应聚合附近 AV 每个位置的所有 BEV 特征，以获得可靠的估计。另一方面，长期全局上下文感知也有助于理解道路拓扑语义或交通状态——车辆前方的道路拓扑和交通密度通常与后方高度相关。这种全局推理也有利于多摄像头视图的理解。例如在图 2(b) 中，同一辆车被分散在多个视图中，全局注意力非常有能力将它们连接起来进行语义推理。

为了高效地获得这种局部-全局属性，受 [28, 44, 45] 的启发，我们提出了一种称为融合轴向注意力 (FAX) 的稀疏 3D 注意力模型，它执行基于局部窗口的注意力和稀疏全局交互。形式上，设 $X \in \mathbb{R}^{N \times H \times W \times C}$ 为来自 $N$ 个智能体的具有空间维度 $H \times W$ 的堆叠 BEV 特征。在局部全分支中，我们将特征图划分为 3D 非重叠窗口，每个窗口大小为 $N \times P \times P$。形状为 $(\frac{H}{P} \times \frac{W}{P}, N \times P^2, C)$ 的分区张量随后被送入自注意力模型，表示沿第二轴（即局部 3D 窗口内）混合信息。同样，在全局分支中，使用统一的 3D 网格 $N \times G \times G$ 将特征 $X$ 划分为形状 $(N \times G^2, \frac{HW}{G^2}, C)$。在该张量的第一轴上采用注意力表示关注稀疏采样的令牌。图 2 分别用红色和蓝色框说明了局部和全局分支的关注区域。将这种 3D 局部和全局注意力与 Transformer 的典型设计（包括层归一化 (LN)、MLP 和跳跃连接）相结合，构成了我们提出的 FAX 注意力块，如图 3b 所示。假设 $P \sim G$（通常 $N \le 5, P, G \in \{8, 16\}$），我们的 3D FAX 注意力仅需要 $\mathcal{O}(2(NP)^2 HWC)$ 的复杂度，明显低于全注意力 $\mathcal{O}((NHW)^2 C)$。尽管如此，它通过透视所有智能体享有非局部 3D 交互，这比局部注意力方法更具表现力。3D FAX 自注意力 (FAX-SA) 块可以表示为：

$$\hat{z}^l = 3DL\text{-}Attn(LN(z^{l-1})) + z^{l-1}$$
$$z^l = MLP(LN(\hat{z}^l)) + \hat{z}^l$$
$$\hat{z}^{l+1} = 3DG\text{-}Attn(LN(z^l)) + z^l$$
$$z^{l+1} = MLP(LN(\hat{z}^{l+1})) + \hat{z}^{l+1}$$

其中 $\hat{z}^l$ 和 $z^l$ 表示第 $l$ 块的 3DL(G)-Attn 模块和 MLP 模块的输出特征。3DL-Attn 和 3DG-Attn 分别代表上述定义的 3D 局部和全局注意力。



#### 3.2 SinBEVT 用于单智能体 BEV 特征计算

给定第 $i$ 个智能体上 $m$ 个摄像头的单目视图 $(I_k^i, K_k^i, R_k^i, t_k^i)_{k=1}^m$，表示输入图像 $I_k \in \mathbb{R}^{h \times w \times 3}$，摄像头内参 $K_k \in \mathbb{R}^{3 \times 3}$，旋转外参 $R_k \in \mathbb{R}^{3 \times 3}$ 和平移 $t_k \in \mathbb{R}^3$，每个智能体在进行任何跨智能体协作之前都需要计算 BEV 特征表示 $F_i \in \mathbb{R}^{H \times W \times C}$（高度 H，宽度 W，通道数 C）。$F_i$ 可以被送入解码器以执行单智能体预测，或者共享给主车用于多智能体特征融合。

我们采用类似于 CVT 的 BEV 处理架构，其中可学习的 BEV 嵌入被初始化为查询 (Query)，以与编码的多视图摄像头特征进行交互，如图 3a 所示。我们观察到 CVT 使用低分辨率 BEV 查询来完全交叉关注图像特征，尽管效率高，但导致小物体上的性能下降。因此，CoBEVT 学习高分辨率 BEV 嵌入，然后使用分层结构来细化分辨率降低的 BEV 特征。为了在高分辨率下高效地从摄像头编码器查询特征，FAX-SA 模块被进一步扩展以构建 FAX 交叉注意力 (FAX-CA) 模块（图 3b），其中查询向量使用 BEV 嵌入获得，而键/值向量由多视图摄像头特征投影。在应用交叉注意力之前，我们添加了一个源自摄像头内参和外参的摄像头感知位置编码，以学习从单个摄像头视图到规范地图视图表示的隐式几何推理，遵循 CVT 的方法。这种相当简单、隐式的方法展示了性能和效率的良好平衡，我们的 FAX 注意力允许在分层网络中进行全局交互，与低分辨率各向同性方法（如 CVT）相比显示出更好的准确性。



#### 3.3 FuseBEVT 用于多智能体 BEV 特征融合

**特征压缩与共享。** 传输数据大小对于 V2V 应用至关重要，因为大带宽需求可能会导致严重的通信延迟。因此，有必要在广播之前压缩 BEV 特征。与 [11, 12] 类似，我们应用简单的 1x1 卷积自动编码器来压缩和解压 BEV 特征。一旦接收到包含中间 BEV 表示和发送者位姿的广播消息，主车应用可微空间变换算子 $\Gamma_\xi$ 将接收到的特征几何变换到主车的坐标系上：$H_i = \Gamma_\xi(F_i) \in \mathbb{R}^{H \times W \times C}$。

**特征融合。** 我们设计了一个定制的 3D 视觉 Transformer，称为 FuseBEVT，它可以专注地融合来自多个智能体的接收到的 BEV 特征信息。主车首先将接收并投影的 BEV 特征 $H_i, i=1,...,N$ 堆叠成高维张量 $h \in \mathbb{R}^{N \times H \times W \times C}$，然后将它们送入 FuseBEVT 编码器，该编码器由多层 FAX-SA 块组成（图 3a）。受益于 FAX 注意力的线性复杂度（第 3.1 节），这种基于智能体的融合 Transformer 也是高效的。每个 FAX-SA 块通过公式 1-2 执行 3D 全局和局部 BEV 特征变换。如图 2(a) 所示，3D FAX-SA 可以关注从多个智能体提取的同一估计区域（红框），以得出最终的聚合表示。此外，稀疏采样的令牌（蓝框）可以进行全局交互，以获得地图语义（如道路、交通等）的上下文理解。

**解码器。** 我们在聚合的 BEV 表示上应用一系列轻量级卷积层和双线性上采样操作，并生成最终的分割输出。

---



### 4. 实验

我们在 V2V 感知数据集 OPV2V 的摄像头赛道上评估了所提出的 CoBEVT 的有效性。为了展示 CoBEVT 的灵活性和通用性，我们还在 OPV2V 的 LiDAR 赛道和自动驾驶数据集 nuScenes 上进行了实验。



#### 4.1 数据集和评估

**OPV2V** 是一个大规模 V2V 感知数据集，是在 CARLA 和协作驾驶自动化工具 OpenCDA 中收集的。它包含 73 个不同的场景，平均持续时间为 25 秒。在每个场景中，不同数量（2 到 7）的 AV 同时出现，每个 AV 配备一个 LiDAR 传感器和 4 个不同方向的摄像头，以覆盖 360° 水平视野。我们的主要实验仅利用数据集的摄像头组，并使用地图预测与真值地图视图标签之间的交并比 (IoU) 作为性能指标。由于 OPV2V 在同一场景中有多个 AV，我们在测试期间选择一个固定的 AV 作为主车，并在其周围 100m×100m 的区域内以 39cm 的地图分辨率进行评估。为了证明其通用性，我们还在 OPV2V LiDAR 赛道 3D 检测任务上评估了我们提出的 CoBEVT。我们使用与 [10, 51] 相同的评估范围，检测性能通过 IoU 阈值为 0.7 时的平均精度 (AP) 来衡量。对于摄像头和 LiDAR 赛道，训练/验证/测试集分别有 6764/1981/2719 帧。

**nuScenes** 数据集包含 1000 个不同的场景，每个场景长约 20 秒。该数据集总共有 40K 个采样帧，转储的数据使用 6 个摄像头捕获周围 360° 的视图。我们使用 [2] 中的真值。X 和 Y 轴的评估范围为 [-50m, 50m]，BEV 网格的分辨率为 0.5m。



#### 4.2 实验设置

**实现细节。** 我们假设所有 AV 遵循 [17] 具有 70m 的通信范围，并且主车广播半径之外的所有车辆将没有任何协作。对于 OPV2V 摄像头赛道，我们选择 ResNet34 作为 SinBEVT 中的图像特征提取器。传输的 BEV 中间表示分辨率为 $32 \times 32 \times 128$。对于多智能体融合，我们的 FuseBEVT 组件有 3 个编码层，局部和全局注意力的窗口大小均为 8。我们使用 Adam 优化器和余弦退火学习率调度器端到端地训练整个模型。我们使用加权交叉熵损失，并将所有模型训练 60 个轮次，每个 GPU 的批次大小为 1。更多详细信息以及 nuScenes 和 OPV2V LiDAR 赛道的配置请参阅补充材料。

**比较方法。** 对于多智能体感知任务，我们考虑单智能体感知系统 **No Fusion** 作为基线。我们与最先进的多智能体感知算法进行比较：**F-Cooper**、**AttFuse**、**V2VNet** 和 **DiscoNet**。我们还实现了一个直接的融合策略 **Map Fusion**，它传输分割地图而不是 BEV 特征，并通过为每个像素选择最近智能体的预测来融合所有地图。对于 nuScenes 数据集，我们与最先进的模型进行比较，包括 **CVT**、**FIERY**、**View Parsing Network (VPN)**、**Orthographic Feature Transform (OFT)** 和 **Lift-Splat-Shoot**。所有模型仅利用单步时间戳数据以进行公平比较。我们特意使用了与 CVT 和 FIERY 相同的图像特征提取器 Efficient-B4 和解码器。



#### 4.3 定量评估

**OPV2V 摄像头赛道结果。** 为了进行公平比较，我们首先采用 CVT 从所有方法的摄像头组中提取 BEV 特征，并且仅使用 CoBEVT 的融合组件（即 FuseBEVT）与其他融合模型进行比较。然后我们将其与完整的 CoBEVT 进行比较，以展示 SinBEVT 的有效性。如表 1 所示，所有协作方法都比 No Fusion 表现更好，这证明了多智能体感知系统的好处。在所有融合模型中，我们的 FuseBEVT 在所有类别中均取得了最佳 IoU，在车辆、可行驶区域和车道方面分别比第二好的方法高出 5.5%、1.4% 和 3.4%。更重要的是，通过用我们的 SinBEVT 替换 CVT 进行特征提取，与仅使用 FuseBEVT 相比，我们的 CoBEVT 可以将这三个类别的准确率进一步提高 1.4%、0.9% 和 3.8%。

*表 1: OPV2V 摄像头赛道上的地图视图分割。我们报告了所有类别的 IoU。除 CoBEVT 使用 SinBEVT 主干外，所有融合方法均采用 CVT 主干。*

| 方法         | 车辆 (Veh.) | 可行驶区域 (Dr.Area) | 车道 (Lane) |
| :----------- | :---------- | :------------------- | :---------- |
| No Fusion    | 37.7        | 57.8                 | 43.7        |
| Map Fusion   | 45.1        | 60.0                 | 44.1        |
| F-Cooper     | 52.5        | 60.4                 | 46.5        |
| AttFuse      | 51.9        | 60.5                 | 46.2        |
| V2VNet       | 53.5        | 60.2                 | 47.5        |
| DiscoNet     | 52.9        | 60.7                 | 45.8        |
| **FuseBEVT** | **59.0**    | **62.1**             | **49.2**    |
| **CoBEVT**   | **60.4**    | **63.0**             | **53.0**    |

**OPV2V LiDAR 赛道结果。** 如表 2 所示，我们的 FuseBEVT 在 LiDAR 赛道任务上也具有最佳性能，比单智能体系统提高了 25.0%，并且比领先的算法 DiscoNet 高出 1.7%。此外，我们的方法对 LiDAR 特征压缩表现出极大的鲁棒性，在 64 倍压缩率下仅下降 0.3%。

*表 2: OPV2V LiDAR 赛道上的 3D 检测结果。(C) 表示使用 64 倍特征压缩。*

| 方法         | AP0.7    | AP0.7(C) |
| :----------- | :------- | :------- |
| No Fusion    | 60.2     | 60.2     |
| Late Fusion  | 78.1     | 78.1     |
| Early Fusion | 80.0     | -        |
| F-Cooper     | 79.0     | 78.8     |
| AttFuse      | 81.5     | 81.0     |
| V2VNet       | 82.2     | 81.4     |
| DiscoNet     | 83.6     | 83.1     |
| **FuseBEVT** | **85.2** | **84.9** |

**nuScenes 车辆地图视图分割。** 我们的 SinBEVT 可以在 RTX2080 上以 35 FPS 运行，IoU 得分为 37.1，参数量为 1.6 M，实现了实时性能下的最佳准确度。与最先进的方法 CVT 相比，我们在参数和延迟相似的情况下高出 1.1%。

*表 3: nuScenes 上的车辆地图视图分割。所有模型仅使用单个时间戳。*

| 方法        | 车辆 (Veh.) | 参数量 Par(M) | FPS    |
| :---------- | :---------- | :------------ | :----- |
| VPN*        | 29.3        | 4.            | 31     |
| OFT         | 30.1        | -             | -      |
| Lift-Splat  | 32.1        | 14            | 25     |
| FIERY       | 35.8        | 7             | 8      |
| CVT         | 36.0        | 1.2           | 35     |
| **SinBEVT** | **37.1**    | **1.6**       | **35** |

**压缩率的影响。** 数据传输大小是 V2V 应用中的关键因素。这里我们通过调整 $1 \times 1$ 卷积来研究不同压缩率对我们 CoBEVT 的影响。表 4 显示 CoBEVT 对压缩不敏感，即使在 64 倍的大压缩率下，它仍然可以击败其他融合方法。

*表 4: 压缩对 OPV2V 摄像头数据的影响。*

| CPR-rate | Size (KB) | IoU  |
| :------- | :-------- | :--- |
| 0x       | 524       | 60.4 |
| 8x       | 66        | 60.1 |
| 16x      | 33        | 58.9 |
| 32x      | 16        | 56.2 |
| 64x      | 8         | 54.8 |



#### 4.4 定性分析

图 4 展示了 CoBEVT 在包含 3 个 AV 的场景上的定性结果。在每一行中，我们绘制了每个 AV 的前置摄像头图像以及真值和预测对。得益于我们从所有智能体和视图中学习的 Transformer 设计，我们的框架可以克服大多数遮挡并准确感知远处物体。然而，我们观察到的一个局限性是多个附近车辆的预测“合并”，这可能归因于低分辨率 BEV 嵌入和密集交通中复杂真值的综合影响。



#### 4.5 消融研究

**组件分析。** 表 5 显示了局部和全局注意力在多智能体融合模型 FuseBEVT 中的重要性，而 CoBEVT 中的其他组件保持不变。两个注意力块都对最终性能做出了显着贡献。

*表 5: 组件消融。*

| Local | Global | Veh./Dr.Area/Lane  |
| :---- | :----- | :----------------- |
|       |        | 52.6/57.9/42.0     |
| ✓     |        | 57.8/61.5/49.2     |
|       | ✓      | 57.9/60.8/48.6     |
| ✓     | ✓      | **60.4/63.0/53.0** |

**对摄像头丢失的鲁棒性。** 驾驶过程中的传感器故障可能导致致命事故。因此，这里我们调查 CoBEVT 处理这种情况的能力。我们随机丢弃主车的 $n \in [1, 4]$ 个摄像头，并在图 5a 中展示了 SinBEVT（无协作）和 CoBEVT 的性能下降情况。可以看出，通过引入感知协作，驾驶安全性得到了显着提高，即使所有主车摄像头都发生故障，CoBEVT 仍然可以达到 44.3 的 IoU 分数。

**智能体数量。** 这里我们研究协作者数量对 CoBEVT 的影响。如图 5b 所述，增加协作者通常可以带来性能提升，而当智能体数量大于 4 时，这种增益将变得微乎其微。

**FuseBEVT 的推理速度。** 实时多智能体特征融合对于实际部署至关重要。这里我们在 RTX3090 上检查了具有不同 BEV 特征图空间分辨率（从 16 到 64）和智能体数量的 FuseBEVT 的推理速度。图 5c 显示我们的融合算法可以在不同的协作场景下实现实时性能。

---



### 5. 结论与局限性

在本文中，我们提出了一个名为 CoBEVT 的整体视觉 Transformer，用于多视图协作语义分割。我们提出了一种融合轴向注意力 (FAX) 机制，允许跨所有视图和智能体进行局部和全局交互。在模拟和真实世界数据集上的广泛实验表明，CoBEVT 在多摄像头协作 BEV 分割方面取得了卓越的性能。它还可以适应其他任务，并大幅改进多智能体 LiDAR 检测和单智能体地图视图分割。

**局限性。** 尽管提出的单智能体模型在真实世界的 nuScenes 数据集上表现优于其他模型，但整个协作框架仅在模拟数据集上进行了训练和验证，因此其在现实世界中的泛化能力仍然未知。所提出的方法没有明确建模现实中的 V2V 挑战，如不同步和位置误差，这可能会削弱其在这些噪声下的鲁棒性。针对不同领域（如恶劣天气或光照条件）的感知鲁棒性需要进一步检查。解决这些局限性需要未来在真实、现实和多样化的协作数据集和基准上进行研究。

---



---

### 附录

在本补充材料中，我们将首先提供有关 OPV2V 数据集摄像头赛道的更多详细信息（A 节）。之后，所提出的 FAX 注意力的模型细节以及我们 CoBEVT 模型在不同数据集上的实现细节将在 B 节和 C 节中进行说明。最后，我们在 D 节中展示了正文中测试的所有三个任务的更多定性结果。

#### A. OPV2V 数据集的摄像头赛道
**传感器配置。** 在 OPV2V 中，每辆 AV 配备 4 个指向不同方向的摄像头，以覆盖 360° 周围环境，如图 6 所示。每个摄像头的空间分辨率为 $800 \times 600$，视场角 (FOV) 为 110°，这在任何相邻对之间引入了 10° 的视图重叠。

**真值。** BEV 语义分割真值掩码具有 $256 \times 256$ 的像素分辨率，覆盖主车周围 $100 \times 100$ 米的区域，这代表 $0.39 m/pixel$ 的地图采样分辨率。作者还提供了相应的可见性掩码，其中任何 AV 摄像头组可以看到的所有动态物体都被标记为可见，反之则为不可见。与之前的工作类似，我们只考虑在训练和测试期间都可见的物体。



#### B. 模型细节

我们在下面给出关于所提出的 3D 融合轴向注意力 (FAX) 的更多细节。

**3D 相对注意力。** 原始定义中的注意力机制是基于所有空间位置加权和的全局混合算子，其中权重由归一化的成对相似度计算。形式上，注意力算子可以定义为：
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
其中 Q, K, V 是从输入张量投影的查询、键和值矩阵。多头注意力是上述公式的扩展，其中我们将通道拆分为多个“头”，并行地在每个头上分别运行注意力。为了简单起见，这里我们只使用单头方程，但在实际实现中我们总是使用多头变体。

我们在 CoBEVT 中采用的 3D 相对注意力是一种改进的注意力，在 3D 空间中添加了相对位置编码。给定 3D 输入张量 $z \in \mathbb{R}^{(N \times H \times W) \times C}$，3D 相对注意力可以表示为：
$$3D\text{-}Rel\text{-}Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}} + B)V$$
其中 B 是相对位置偏差，其值取自具有可学习参数的 $\hat{B} \in \mathbb{R}^{(2N-1) \times (2H-1) \times (2W-1)}$。

**3D FAX 注意力。** 我们假设上述定义的 3D-Rel-Attention 遵循 1D 输入序列的惯例，即始终将输入的倒数第二个维度视为“空间轴”。所提出的 FAX 注意力可以在不修改注意力算子的情况下实现。我们首先定义参数为 P 的 Fused-Block() 算子，将输入 3D 特征 $x \in \mathbb{R}^{N \times H \times W \times C}$ 划分为非重叠的 3D 窗口，每个窗口大小为 $N \times P \times P$。注意，在窗口划分后，我们将所有空间维度聚集在所谓的“空间轴”中：
$$Fused\text{-}Block: (N, H, W, C) \rightarrow (N, \frac{H}{P} \times \frac{W}{P}, P \times P, C) \rightarrow (\frac{HW}{P^2}, N \times P^2, C)$$

然后，我们将 Fused-Unblock(-) 操作表示为上述 3D 窗口划分过程的逆过程。同样，对于全局注意力分支，我们定义另一个 3D 网格划分算子为 Fused-Grid，网格参数为 G，表示使用大小为 $N \times G \times G$ 的统一 3D 网格划分输入特征。注意，与 Fused-Block 不同，我们需要应用额外的 Transpose 将网格维度放置在假设的“空间轴”中：
$$Fused\text{-}Grid: (N, H, W, C) \rightarrow (N, G \times \frac{H}{G}, G \times \frac{W}{G}, C) \rightarrow (N \times G^2, \frac{HW}{G^2}, C) \rightarrow (\frac{HW}{G^2}, N \times G^2, C)$$
这里使用了 swapaxes(axis1=-2, axis2=-3)。其逆算子 Fused-Ungrid 将 3D 网格化的输入反转回原始张量形状。

现在我们准备展示整个 3D FAX 注意力模块。3D 局部块注意力可以表示为：
$$x \leftarrow x + Fused\text{-}Unblock(3D\text{-}Rel\text{-}Attention(Fused\text{-}Block(LN(x))))$$
$$x \leftarrow x + MLP(LN(x))$$
而稀疏全局 3D 注意力可以表示为：
$$x \leftarrow x + Fused\text{-}Ungrid(3D\text{-}Rel\text{-}Attention(Fused\text{-}Grid(LN(x))))$$
$$x \leftarrow x + MLP(LN(x))$$
其中 QKV 矩阵是从输入 x 线性投影的，为简单起见省略。LN 表示层归一化，其中 MLP 是由应用于通道的两个线性层组成的标准 MLP 网络：$x \leftarrow W_2 GELU(W_1 x)$。



#### C. 实现细节

在下文中，我们将分别展示三个实验的详细架构。

##### C.1 OPV2V 摄像头赛道
我们在表 A2 中说明了 CoBEVT 的架构规格。进一步的说明如下所示。

**模型分离。** 与之前的工作一样，我们对动态物体和静态布局 BEV 语义分割有单独的模型。除了网络的最后一层外，两个模型具有相同的配置。

**图像编码器。** 我们首先将输入图像调整为 $512 \times 512$，并利用 ResNet34 提取图像特征。然后我们从 layer1, layer2 和 layer3 获取输出 $I_0, I_1, I_2$ 与 BEV 查询交互。

**SinBEVT。** BEV 查询 $Q_0$ 是一个可学习的嵌入。$Q_0$ 作为查询被送入我们的 FAX-CA 块，而 $I_0$ 被视为键和值以将图像特征投影到 BEV 空间。我们随后使用分层结构来细化 BEV 特征。

**FuseBEVT。** 来自 N 个智能体的 BEV 特征将被堆叠在一起，并送入三个连续的 FAX-SA 块以获得融合特征 H。所有 FAX-SA 块的窗口/网格大小均设置为 8。

**解码器。** H 将通过 3 倍 [双线性插值, conv3x3, BN] 进行上采样，以检索最终的分割掩码。

##### C.2 nuScenes
为了进行公平比较，我们严格遵循与 CVT 相同的实验设置。
**图像编码器。** 我们遵循 CVT 和 Fiery 使用 EfficientNet B-4 作为图像特征提取器。
**SinBEVT。** BEV 查询以 $100 \times 100 \times 32$ 的大小开始，以 $25 \times 25 \times 128$ 的大小结束。主要架构与表 A2 中所示的 SinBEVT 规格相同。
**解码器。** 解码器结构与 CVT 相同。
**训练。** 我们使用焦点损失（focal loss）训练模型，每个 GPU 批次大小为 4，训练 30 个轮次。
**评估。** 我们评估车辆周围 100m×100m 的区域，采样分辨率为 50cm。

##### C.3 OPV2V LiDAR 赛道
除融合组件外，所有比较方法均具有相同的配置。
**点云编码器。** 我们选择 PointPillar 作为点云特征提取器。
**FuseBEVT。** FuseBEVT 的配置与 OPV2V 摄像头赛道中的配置相同。
**检测头和训练。** 我们简单地应用两个 $3 \times 3$ 卷积层分别进行分类和回归。



#### D. 更多定性结果

**OPV2V 摄像头赛道。** 图 7 和图 8 展示了我们的 CoBEVT 与其他方法在 OPV2V 摄像头赛道上的视觉比较。在大多数场景中，我们的方法在动态物体预测和道路拓扑分割方面均显着优于其他方法。

**OPV2V LiDAR 赛道。** 我们在图 9 和图 10 中展示了 OPV2V LiDAR 赛道中 4 个不同繁忙路口的检测可视化结果。与包括 AttFuse, F-Cooper, V2VNet 和 DiscoNet 在内的其他最先进融合方法相比，我们的 CoBEVT 总体上实现了更稳健的性能。与之前的 SOTA 方法 DiscoNet 相比，我们的结果具有更少的未检测物体和更少的位移。

**nuScenes。** 图 11 描绘了我们的 SinBEVT 在 nuScenes 上在不同道路拓扑、交通状况和光照条件下的定性结果。我们的方法可以识别大多数物体并稳健地估计复杂的道路布局，展示了所提出的 FAX 注意力在各种自动驾驶任务中的强大泛化能力。