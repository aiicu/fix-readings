V2XP-ASG：为车联网感知生成对抗性场景



### 摘要

——车联网（Vehicle-to-Everything, V2X）通信技术的最新进展使得自动驾驶车辆能够共享感知信息，从而获得更好的感知性能。随着自动驾驶车辆和智能基础设施的快速增长，V2X感知系统将很快大规模部署，这引发了一个关键的安全问题：在实际部署之前，我们如何在具有挑战性的交通场景下评估和改进其性能？收集多样化的大规模真实世界测试场景似乎是最直接的解决方案，但这种方法昂贵且耗时，并且收集到的数据只能覆盖有限的场景。为此，我们提出了第一个开放的对抗性场景生成器 **V2XP-ASG**，它可以为现代基于激光雷达（LiDAR）的多智能体感知系统生成逼真且具有挑战性的场景。V2XP-ASG 学习构建对抗性协作图，并同时以对抗性和合理的方式扰动多个智能体的位姿。实验表明，V2XP-ASG 可以有效地识别出各种 V2X 感知系统的挑战性场景。同时，通过在有限数量的生成挑战性场景上进行训练，V2X 感知系统的准确率在挑战性场景下可进一步提高 12.3%，在正常场景下可提高 4%。我们的代码将在 https://github.com/XHwind/V2XP-ASG 发布。



### I. 引言

在过去十年中，我们见证了自动驾驶汽车（AVs）和智能交通系统的巨大进步。不久之后，这些自主系统将大规模部署在道路上，为其间的合作开辟机会。之前的研究表明，通过利用车联网（V2X）通信技术，自动驾驶汽车和基础设施可以通过使用共享的传感信息进行协作感知，从而显著提高感知性能。

尽管取得了显著的改进，但这些工作是在包含自然场景的数据集上评估所提出的系统的，这些场景不包含足够的安全关键场景。在具有挑战性的场景下，这些系统的性能可能会较差，因此识别挑战性场景以充分了解现有协作感知系统的鲁棒性至关重要。

最直接的解决方案是在现实世界中收集广泛的测试场景并识别关键场景。然而，与单智能体系统相比，多智能体系统收集和标记数据的成本和时间消耗可能要高得多。一个更具成本效益的优选方案是在高保真模拟器中生成大规模的逼真场景。然而，这些方法只考虑了普通场景，缺乏对目标系统针对长尾情况（corner cases）进行压力测试的能力。

为了解决这个问题，受最近通过扰动周围车辆轨迹为单智能体规划器生成安全关键场景的工作的启发，我们要自动为 V2X 感知系统生成多样化且具有挑战性的场景。尽管如此，直接将单智能体场景生成方法转移到 V2X 系统是很困难的。与具有单一视点的单智能体系统不同，V2X 系统涉及具有多个视点的协作者。来自不同协作者的这些视图之间的几何关系会极大地影响感知性能，这在以前的工作中没有被考虑。此外，协作者的选择会影响视点，因此与智能体位姿扰动相耦合，导致更复杂的交互。

处理单智能体感知和协作感知之间的这些差异是有效的 V2X 对抗性场景生成器的关键。然而，目前还没有文献报道对它们进行过研究。此外，大多数现有方法都是为规划量身定制的，并且没有公开可用的感知场景生成框架，这阻碍了为现代单智能体和 V2X 感知系统开发高效场景生成方法的进展。

为此，我们引入了 **V2XP-ASG**，这是第一个用于基于激光雷达的 V2X 感知系统的开放式对抗性场景生成器。据我们所知，这是第一个旨在自动生成 V2X 感知挑战性场景的框架。如图 1 所示，给定数据集中的初始场景，V2XP-ASG 首先通过搜索对抗性协作者来构建对抗性协作图，这些协作者的视点组合会导致性能下降。随后，我们以对抗性和合理的方式扰动多个智能体的位姿。为了反映由传感器视点变化和位姿扰动引起的更新后的激光雷达观测结果，我们基于高保真 CARLA 模拟器进行了实验。广泛的实验表明，V2XP-ASG 可以创建具有挑战性的场景，严重恶化现代 V2X 感知系统的性能。更重要的是，使用这些对抗性场景进行训练可以提高系统的准确性。

我们的主要贡献总结如下：
* 我们提出了 V2XP-ASG，这是第一个开放的对抗性场景生成框架，用于测试现代 V2X 感知系统。我们的 V2XP-ASG 可以合理地为各种 V2X 感知系统智能地生成具有挑战性的场景。我们将在未来发布代码。我们将此场景生成任务公式化为一个新颖的两阶段优化问题，这使得优化更容易，并提供了对系统弱点的可解释性。
* 我们在第一阶段通过利用可学习的协作图权重，提出了一种针对新颖的对抗性协作者搜索（ACS）任务的高效搜索策略。此外，在第二阶段，我们采用针对感知任务定制对抗性目标的黑盒优化算法，以对抗性和可行的方式扰动车辆位姿。这两个阶段相结合，有效地提高了场景的挑战性水平。
* 正如我们的实验所证明的那样，在生成的对抗性场景上进行训练可以大大提高系统在正常场景和挑战性场景下的精度。



### II. 相关工作

**V2X 感知：** V2X 感知研究如何利用来自附近自动驾驶车辆和智能基础设施的视觉信息来增强感知能力。根据协作策略，主要分为三类：前融合（early）、后融合（late）和中融合（intermediate fusion）。前融合方法在该智能体之间传输原始点云，每个智能体将聚合的点云输入网络进行 3D 检测。尽管保留了完整的原始信息，但前融合通常需要大带宽，使其在现实中难以部署。相反，后融合的数据传输量最小，因为它只传播预测输出的元数据。然而，由于未能提供有价值的场景上下文，其准确性受到限制。为了在带宽和准确性之间取得良好的平衡，广播压缩的中间神经特征的中融合最近得到了最多的研究。[41] 提出了一种用于联合感知和预测的空间感知图神经网络，[44] 采用知识蒸馏，在前融合的监督下推进学习。[42] 提出了一种位置感知自注意力机制来融合来自不同自动驾驶车辆的特征。本工作评估了所有三种融合策略以及单智能体感知方法。

**对抗性场景生成：** 由于安全性对自动驾驶至关重要，最近，已有多项工作被提出用于生成安全关键场景以识别规划故障。[32] 使用贝叶斯优化在 CARLA 中基于分层图路线搜索碰撞场景。STRIVE [33] 通过基于图的条件 VAE 模型在潜在空间中表示智能体，并通过基于梯度的优化扰动潜在变量，以生成与给定规划器碰撞的轨迹。AdvSim [34] 对几种黑盒优化算法进行基准测试，以搜索对抗性轨迹，从而为完整的自动驾驶堆栈获得安全关键场景，但其对抗性目标是针对规划的，并且是为单车系统设计的。另一类工作将场景生成问题公式化为稀见事件模拟，以便为单智能体自动驾驶系统采样故障场景。相比之下，我们专注于为基于激光雷达的多智能体 V2X 感知系统生成具有挑战性的场景，在该系统中，智能体的位姿和协作者的选择都被搜索以优化针对感知定制的对抗性目标。

**基于激光雷达的对抗性攻击：** 以物理上可实现的方式进行的基于激光雷达的对抗性攻击越来越受到关注。[57] 通过激光雷达欺骗器更改原始激光雷达点，对自动驾驶设置中的基于激光雷达的感知进行了首次安全研究。[56] 研究了运动补偿的后门，并对智能体的轨迹进行对抗性欺骗以降低感知性能。在 [55] 中，对抗性网格被放置在车辆的车顶上，并优化网格位姿使车辆不可见。除了上述单智能体攻击外，[58] 通过向智能代理共享的中间特征添加对抗性噪声来攻击 V2X 感知系统。与以前的工作不同，我们直接攻击智能体协作选择和车辆位姿，以降低现代 V2X 感知系统的性能，这确保了生成的对抗性示例的可行性和合理性。

**多智能体协作图：** 经典的多智能体协作图侧重于融合来自选定智能体或所有连接智能体的信息，以提高系统性能。Who2com [35] 提出了一种握手通信机制，神经网络可以学习并确定哪两个智能体应该压缩每个阶段所需的相关信息。When2com [59] 构建了基于学习的通信组以学习通信，并使用非对称注意力机制来决定何时在全连接图上进行通信。DiscoGraph [44] 利用了一种将早期和中间协作结合到知识蒸馏框架中的方法，使早期协作的知识能够指导中间融合模型的训练。然而，在本工作中，我们感兴趣的不是通过寻找鲁棒的协作者来提高感知性能，而是搜索对抗性协作者以降低任务性能，这有助于暴露多智能体系统的潜在脆弱性。



### III. 对抗性场景生成

V2XP-ASG 旨在为给定的 V2X 感知模型生成逼真、具有挑战性的场景。我们框架的整体架构如图 1 所示，它包括对抗性协作者搜索（ACS）和对抗性扰动搜索（APS）。给定一个具有固定主车（ego agent）的现有场景，我们首先利用基于注意力的采样方法搜索对抗性协作者组合，然后利用选定的智能体构建对抗性协作图。随后，我们扰动多个智能体的位姿，并将更新后的激光雷达观测结果输入目标感知系统以生成 3D 边界框。最终，我们使用对抗性目标评估预测，并执行黑盒优化以更新位姿。



#### A. 问题公式化

给定一个包含一组智能体 $\mathcal{A}=\{a_{1},...,a_{N}\}$ 的初始场景，其中智能体 $a_{i}$ 可以是车辆或基础设施，我们假设其中 $k$ 个配备了传感和通信设备，并表示为智能智能体 $\mathcal{I}\subset\mathcal{A}$。在通信范围内，所有属于 $\mathcal{I}$ 的智能体都可以相互共享信息，其中一个被选为主车以聚合传感信息从而形成统一的检测。更正式地说，来自这些智能体的观测集是：
$$\mathcal{X}(\mathcal{S},\mathcal{I})=\{X_{i}(\mathcal{S})|\forall i\in\mathcal{I}\} \quad (1)$$
其中 $\mathcal{S}=\{s_{1},...,s_{N}\}$ 是场景中所有智能体的位姿集合。$X_{i}(\mathcal{S})$ 是来自智能智能体 $i$ 的传感观测，它反映了智能体位姿 $\mathcal{S}$ 的变化。拥有相邻协作者的主车将基于聚合的传感观测 $\mathcal{X}$ 协同预测边界框 $\hat{Y}=f(\mathcal{X}(\mathcal{S},\mathcal{I});\tau)$，其中 $f$ 是 V2X 3D 检测模型，$\tau$ 是融合策略，包括前融合、后融合和中融合。

我们的目标是通过调整初始正常场景使其更具挑战性来攻击 V2X 感知模型。本工作中有两类对手，即智能体协作组合 $\mathcal{I}$ 和位姿扰动 $\Delta$。这两个对手将以物理上合理的方式修改传感器视点和车辆位姿，并利用 CARLA [36] 来模拟更新后的观测结果 $\mathcal{X}(\mathcal{S}+\Delta,\mathcal{I})$。然后我们定义对抗性目标 $\mathcal{L}_{adv}$（第三节 D），将其最小化以生成挑战性场景：

$$\min_{\Delta,\mathcal{I}} \mathcal{L}_{adv}(f(\mathcal{X}(\mathcal{S}+\Delta,\mathcal{I});\tau),Y)$$
$$\text{s.t. } ||\Delta||\le\delta, \mathcal{I}\subset\mathcal{A}, |\mathcal{I}|=k \quad (2)$$

其中 $|\cdot|$ 是集合基数，$Y$ 是真实边界框。我们解耦上述公式并优化以下分解近似：

$$\mathcal{I}^* = \arg \min_{\mathcal{I}} \mathcal{L}_{adv}(f(\mathcal{X}(\mathcal{S},\mathcal{I});\tau),Y) \quad (3)$$
$$\text{s.t. } \mathcal{I}\subset\mathcal{A}, |\mathcal{I}|=k$$

$$\mathcal{A}^* = \arg \min_{\Delta} \mathcal{L}_{adv}(f(\mathcal{X}(\mathcal{S}+\Delta,\mathcal{I}^{*});\tau),Y) \quad (4)$$
$$\text{s.t. } ||\Delta||\le\delta$$

这种分解可以将优化分为两个后续任务：对抗性协作者搜索（公式 3）和对抗性位姿扰动（公式 4），这不仅可以简化优化，而且提供了单独分析两个阶段以更好地了解目标系统性能的可能性。由于位姿扰动带来的性能下降很容易通过改变传感器视点而减弱，我们选择先搜索协作，而不是相反的顺序。对于多智能体系统，$k=1$ 显然会导致性能较差。然而，我们有兴趣研究 V2X 系统在大规模部署时的鲁棒性。因此，在这项工作中我们固定 $k=3$。



#### B. 对抗性协作者搜索

尽管检查所有智能体组合起初看起来很吸引人，但由于巨大的组合数量和高昂的计算成本，这是令人望而却步的。因此，我们设计了一种高效的搜索算法。我们采用中间融合模型 AttFuse [42] 来构建协作图，其中可学习的边权重被认为代表该智能体的特征对主车贡献的重要性。我们利用这些可学习的边权重来定义每个智能体的弱点水平，以便较低的值代表对整体感知系统的贡献较小，因此，为这些弱智能体分配较高的采样概率可以增加找到具有较差性能的对抗性协作者的机会。尽管其他融合方法（如前融合和后融合）无法生成具有重要性权重的协作图，但我们通过实验证明，所提出的基于注意力的概率采样方法可以以极高的搜索效率转移到其他模型（IV）。此外，我们的框架是通用的，其他对抗性协作者搜索方法也可以集成到该框架中，我们鼓励更多研究人员调查这一新方向。

为了获得所有智能体的边权重，我们首先在模拟器中为所有智能体配备激光雷达传感器。每个智能体将基于其原始传感观测推理中间特征，然后将其中间特征 $H_{i}\in\mathbb{R}^{H\times W\times C}$ 传输给主车。然后采用自注意力模型来捕获主车坐标下同一空间位置不同智能体的特征重要性。形式上，设 $h_{mn}^{i}=[H_{i}]_{mn}\in\mathbb{R}^{C}$ 为智能体 $i$ 在位置 $(m, n)$ 的特征，$h_{mn}=\{h_{mn}^{1},...,h_{mn}^{N}\}\in\mathbb{R}^{N\times C}$ 是所有 $N$ 个智能体在位置 $(m,n)$ 的聚合特征。然后注意力操作如下：

$$a_{mn}=\text{softmax}\left(\frac{q_{mn}k_{mn}^{T}}{\sqrt{C}}\right) \quad (5)$$

其中 $q_{mn}$、$k_{mn}$ 和 $v_{mn}$ 是 $h_{mn}$ 沿通道维度的线性投影。融合后的特征 ${h^{\prime}}_{mn}=a_{mn}v_{mn}$。它可以被切片并重新排列以获得更新后的特征 ${H^{\prime}}_{i}$。

在我们的场景生成器中应用此自注意力模型之前，我们将在增强的 OPV2V 数据集 [42] 上对其进行训练，通过将 ${H^{\prime}}_{i}$ 发送到检测头以产生边界框预测来学习协作图构建。在对抗过程中，我们固定网络权重并直接利用计算出的 $a_{mn}$ 来检测对抗性协作者。由于注意力权重是位置相关的，因此采用平均池化来聚合所有空间位置的权重。

$$s_{j}=\text{Avg}([a_{11}]_{i_{*}j},...,[a_{HW}]_{i_{*j}}) \quad (6)$$

这里 $s_{j}$ 代表智能体 $j$ 的重要性，因此 $1/s_{j}$ 是该智能体的相关弱点水平。对于每个协作者组合 $\mathcal{I}$，我们可以将其弱点水平建模为单个智能体弱点水平的总和，并应用 softmax 形成概率分布：

$$w_{\mathcal{I}}=\sum_{i\in\mathcal{I}}\frac{1}{s_{i}} \quad (7)$$
$$p = \text{Softmax}(w) \quad (8)$$

其中是温度参数，用于控制分布的形状。我们根据概率 $p$ 不放回地采样 $k_{0}$ 个组合，检查所有对抗性目标值，并保留对抗性值最低的最佳组合作为对抗性协作组合 $\mathcal{I}^{*}$。该组合最终用于构建对抗性协作图，其中只有选定的智能体才能相互共享信息。



#### C. 对抗性扰动搜索

**搜索空间：** 我们在物理合理的界限内扰动多个智能体的位姿。每个智能体的扰动参数化为 $\delta_{i}=(\delta x_{i},\delta y_{i},\delta\theta_{i})$，其中 $(\delta x_{i},\delta y_{i})$ 是位置的扰动，$\delta\theta_{i}$ 是偏航角的变化。多智能体扰动即为：
$$\Delta=\{\delta_{1},\delta_{2},...,\delta_{m}\} \quad (9)$$
其中 $m$ 是受扰动智能体的数量，这 $m$ 个智能体是根据遮挡启发式从集合 $\mathcal{A}$ 中采样的。我们按照遮挡水平降序排列 $N$ 个智能体，然后选择前 $m$ 个智能体进行扰动。具体来说，每个智能体的遮挡水平是其内在遮挡分数和外在遮挡分数的总和。如果智能体被场景中的任何其他智能体部分重叠，则内在遮挡分数为 1，表示受扰动智能体（被遮挡者）的遮挡水平。外在遮挡分数衡量场景中有多少其他智能体可以被该被检查智能体（遮挡者）部分遮挡。对于多智能体 V2X 系统，我们将不同视点的遮挡分数相加作为智能体的总分。通过这种方式，采样的智能体更有可能观察到部分被遮挡的物体。我们设置 $||\Delta||\le\delta$ 以将扰动限制在有限范围内。在这项工作中，采用 $m=3$ 以增加搜索空间和场景配置。

**黑盒搜索算法：** 我们的框架适用于任何黑盒搜索算法。搜索算法旨在通过最小化对抗性目标 $\mathcal{L}_{adv}$ 来为 V2X 感知模型寻找具有挑战性的场景。在本文中，我们对 3 种黑盒搜索算法进行了基准测试：随机搜索（RS）、遗传算法（GA）和贝叶斯优化（BO）。随机搜索将从可行集中随机采样扰动。遗传算法将维护候选扰动的种群，通过选择具有高适应度分数的父代来进化种群，并且始终保留最佳候选者以增强性能。贝叶斯优化将建立一个代理模型，并使用采集函数来平衡探索和利用以生成候选者。

**搜索流程：** 整体方法总结在算法 1 中。为了增加扰动的合理性，我们为采样的 $m$ 个智能体构建了一个上下文感知的可行扰动集 $Q$。为了生成这样一个可行集，我们首先在边界 $(\delta x,\delta y\in[-2.5,2.5] \text{ m 和 } \delta\theta\in[-45^{\circ},45^{\circ}])$ 内均匀采样 $N_{\mathcal{Q}}=1000$ 个潜在扰动，并移除可能导致潜在碰撞的扰动。由于位置和角度具有不同的大小，为了消除这种偏差，我们还通过除以扰动范围对扰动 $\Delta$ 进行归一化，使得 $||\Delta||_{\infty}\le1$。在整个优化过程中维护 $(\Delta,\mathcal{L}_{adv})$ 对的历史观测 $\mathcal{D}$。在每次迭代期间，搜索算法生成潜在扰动 $\Delta$，并通过在集合中找到以 $l_{2}$ 距离测量的最近元素将其投影到可行集 $Q$ 上。然后它将查询目标 V2X 感知模型以获得对抗性损失 $\mathcal{L}_{adv}$。搜索算法将更新历史观测 $\mathcal{D}$，用于生成下一次迭代的扰动。选择具有最低对抗性损失的最佳观测扰动作为最终扰动。

**算法 1：对抗性扰动搜索**
1: 根据启发式方法采样 $m$ 个智能体进行扰动
2: 为 $m$ 个智能体生成上下文感知的可行集 $Q$
3: 初始化历史观测 $\mathcal{D}=\{\}$
4: **for** $i=1...M$ **do**
5:     基于历史观测 $\mathcal{D}$ 和黑盒搜索算法生成 $\Delta^{(i)}$
6:     将 $\Delta^{(i)}$ 投影到可行集 $Q$ 上
7:     从 LiDAR 模拟器获取更新后的传感观测 $\mathcal{X}^{(i)}=\mathcal{X}(\mathcal{S}+\Delta^{(i)},\mathcal{I}^{*})$
8:     $\mathcal{L}_{adv}^{(i)}=\mathcal{L}_{adv}(f(\mathcal{X}^{(i)}),Y)$
9:     $\mathcal{D}=\mathcal{D}\cup\{(\Delta^{(i)},\mathcal{L}_{adv}^{(i)})\}$
10: **end for**
11: $\Delta^{(*)}=\arg \min_{\Delta^{(i)},i\in\{1,...,M\}}\mathcal{L}_{adv}^{(i)}$



#### D. 对抗性目标

我们采用加权平均精度作为对抗性损失。
$$\mathcal{L}_{adv}=\sum_{t\in\mathcal{T}}w_{t}AP@t \quad (10)$$
其中 $\mathcal{T}$ 是一组交并比（IoU）阈值，在我们的实验中，$\mathcal{T}=\{0.3,0.5,0.7\}$，$AP@t$ 是 IoU 阈值为 $t$ 时的平均精度（AP），$w_{t}$ 是相关权重。使用这种平均精度的加权和可以导致更平滑的分数变化，从而降低优化难度。在这项工作中，我们设置 $w_{0.3}=1$，$w_{0.5}=0.8$ 和 $w_{0.7}=0.5$，以便为较低的 IoU 阈值赋予更高的权重，因为它们的 AP 通常更难降低。



### IV. 实验

#### A. 实验设置

**数据集：** 实验在 CARLA [36] 模拟器中进行。实验中使用了两个数据集：1）我们用基础设施传感器数据增强了现有的大规模车对车（V2V）感知数据集，即 OPV2V。OPV2V 的训练集用于训练现代 V2X 感知模型直到收敛。从增强的 OPV2V 测试集中，我们有 94 个驾驶日志，每个日志有 70 帧，覆盖 8 个道路区域。由于运行 CARLA 模拟器和深度网络的高计算成本，我们从这些收集的场景中采样了 331 个正常场景，用于评估 V2XP-ASG 的训练/验证拆分为 219/112。主要实验在训练集拆分上进行，验证集拆分仅用于报告微调模型的性能。2）除上述数据外，我们构建了另一个启发式生成的挑战性数据集，称为 Heuristic，它具有高交通密度、严重遮挡和稀疏观测。这个保留数据集包括从 14 个驾驶日志中采样的 321 帧。我们首先收集了 14 个驾驶日志，共 980 帧，在 4 个 CARLA 城镇中交通密集（每帧 20~30 辆车）。然后对于每一帧，我们计算边界框内的平均激光雷达点数，并通过仅保留平均点数小于阈值 $N_{thred}=25$ 的场景来整理原始数据。由于该数据集仅在推理期间可见，因此它可以更公平地评估微调后的模型是否可以在看不见的挑战性场景中取得更好的结果（表 II）。

**V2X 感知模型：** V2XP-ASG 在 4 个模型上进行评估：1）无融合（No Fusion），仅基于主车 LiDAR 点云预测边界框。2）后融合（Late Fusion），其中传输每个智能体的检测提议，并应用非极大值抑制来生成最终预测。3）前融合（Early Fusion），将来自不同智能体的原始 LiDAR 点聚合成一个整体视图并将其输入检测器。4）AttFuse [42]，一种中间融合模型，它相互广播中间特征，并使用位置感知自注意力来融合接收到的特征。所有模型均使用 PointPillar [62] 主干网络实现。

**V2XP-ASG：** 我们采用 AttFuse 的注意力权重为所有 3 个 V2X 感知模型构建对抗性协作图。温度参数 $\tau=0.03$，我们检查根据公式 8 采样的 $k_{0}=3$ 个组合。对于主要实验，我们采用贝叶斯优化 [61] 作为黑盒搜索算法。对于 LiDAR 模拟，为了减少噪声的影响，我们将噪声设置为零，即零丢失率和零噪声水平，以便感知性能仅受内在场景配置的影响。感知性能在 $x, y\in[-48,48]$ 米的范围内进行评估。为了保持地面真值一致，在扰动智能体时，我们确保更新后的位姿仍保持在评估范围内。黑盒优化的参数可以在补充视频中找到。



#### B. 实验结果

**V2X 感知模型评估：** 不同 V2X 感知模型的评估结果如表 I 所示。结果表明，我们的 V2XP-ASG 可以为单智能体和 V2X 感知系统生成具有挑战性的场景，AP@0.7 平均下降 28.7%，表明 V2XP-ASG 可以有效地为具有不同融合策略的各种模型识别挑战性场景。

**利用挑战性场景进行改进：** 我们现在调查我们生成的挑战性场景是否有助于提高 V2X 感知系统的性能。我们首先在其生成的挑战性场景的训练集上微调原始 AttFuse 模型，然后在正常和挑战性场景的验证集上测试模型。为了更公平的比较，我们还在一个单独的具有挑战性的保留集（Heuristic）上评估其性能。如表 II 所示，微调后的模型在所有 3 个数据集上都显示出改进，说明了 V2XP-ASG 对改进感知系统的巨大益处。

**挑战性场景的可迁移性：** 表 III 显示了 V2XP-ASG 生成场景的可迁移性。源模型用于生成挑战性场景，目标模型在这些生成的场景上进行测试。当在同一模型生成的场景上测试目标模型时，性能最好。此外，这三个模型在彼此生成的挑战性场景上的性能下降都比它们在正常场景上的性能下降大，这表明模型在场景是否具有挑战性方面具有一定程度的一致性，从而证明了 V2XP-ASG 生成的挑战性场景具有良好的可迁移性。

**基线比较：** 我们比较了所提出的基于 AttFuse 的协作者选择策略（ACS-A）与随机选择 $k_{0}=3$ 个智能体组合的随机选择（ACS-R）。实验结果如表 IV 所示。所提出的方法可以将检测性能分别降低 9.5%、8.9%、7.7%，而随机选择只能将分数降低 6.5%、6.8%、4.1%，显示了所提出的搜索策略的高效率。

**组件分析：** 如表 V 所示，所提出的两个组件 ACS 和 APS 都有助于寻找具有挑战性的场景，这表明视点和位姿扰动对于识别多智能体系统的挑战性场景都至关重要。

**黑盒搜索基准测试结果：** 不同黑盒搜索策略的基准测试结果如表 VI 所示。GA 和 BO 都优于 RS，并且 BO 达到了最佳性能。我们认为这是由于 BO 在探索和利用之间取得了良好的平衡。

**可视化结果：** 图 2 显示了 V2XP-ASG 生成的挑战性场景和相关正常场景的可视化。如图 2b 所示，对抗性协作者选择可以改变传感器视点，使物体难以检测。在图 2c 中，三个智能体受到扰动，其中两个受扰动的智能体同时右转，这在现实世界中既逼真又罕见。它表明 V2XP-ASG 生成的场景对于测试感知模块既具有挑战性又有意义。



### V. 结论

在这项工作中，我们提出了第一个用于基于激光雷达的 V2X 感知系统的开放式对抗性场景生成框架，称为 V2XP-ASG。实验表明，V2XP-ASG 可以为各种感知模型（包括单智能体和 V2X 感知系统）创建具有挑战性的场景。特别是，通过在生成的场景上进行训练，可以进一步提高感知系统在常见场景和挑战性场景下的性能。此外，消融研究验证了所提出的基于注意力的概率采样方法可以有效地找到对抗性协作者，并且这两个新颖的组件都有利于生成具有挑战性的场景。

