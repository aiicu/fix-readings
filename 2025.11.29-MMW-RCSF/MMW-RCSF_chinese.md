一种用于传感器融合的路侧异步毫米波雷达-相机新型时空同步方法

**来源:** IEEE Transactions on Intelligent Transportation Systems, Vol. 23, No. 11, November 2022
**作者:** Yuchuan Du, Bohao Qin, Cong Zhao, Yifan Zhu, Jing Cao, and Yuxiong Ji



---

### 摘要 (Abstract)

路侧传感器，如相机和毫米波（MMW）雷达，为车路协同系统中的智能车辆提供了视距之外的交通信息。与车载设备不同，路侧传感器隶属于不同的系统，并且在空间和时间上缺乏同步。本文提出了一种用于传感器融合的异步路侧毫米波雷达与相机的新型时空同步方法。该方法利用场景特征提取车道线角点以对相机进行预标定。基于各传感器一致的时间流率，设置多条虚拟检测线以匹配连续车辆的车头时距，并对跟踪数据进行目标匹配。最后，建立了一个同步优化模型，并应用约束非线性最小化求解器对参数进行调整。通过上海东海大桥的实测数据验证了该方法的可行性和有效性。结果表明，在该案例中，相机和雷达之间存在33帧（33*40毫秒）的时间偏差。经过同步后，X方向的平均空间偏差从2.47米降低到0.42米，Y方向的平均空间偏差从64.06米降低到2.34米。本研究为解决路侧传感器的时空同步问题提供了一种经济有效的途径。

**关键词:** 路侧传感器融合，相机，毫米波雷达，时空同步，目标匹配。

---



### I. 引言 (Introduction)

路侧传感器具有广泛的检测范围，并且可以配备强大的边缘计算能力。5G通信技术带来了信息传输速率的巨大提升，使得实时提供智能车辆视距之外的路侧感知信息成为可能。车路协同系统的发展也需要路侧传感器提供高质量的感知数据，以实现安全高效的驾驶和交通运行。目前迫切需要进行智能高速公路改造，有效利用现有的路侧设备或新传感器进行数据融合，以提高路侧感知的准确性。

作为最普遍的路侧传感器，相机主要依靠目标检测算法进行感知。在过去的十年中，随着神经网络的发展，最先进的目标检测算法（如Yolo-V4, EfficientNet, SSD, DetectoRS等）已被广泛用于检测道路上的车辆。这些算法可以为视频中的每辆车分配一个边界框，并输出其类型和置信度（在本文中，我们也称相机数据为视频数据）。对于已标定的相机，我们可以进一步计算物体的相对坐标，在无遮挡的视野条件下，检测距离甚至可以超过1公里。此外，还可以获取物体的图像数据以提取特征，甚至识别车牌。然而，基于视频的检测的缺点也很明显。恶劣的光照条件和局部特征遮挡严重影响检测精度和轨迹连续性。

激光雷达（Lidar）是自动驾驶领域另一种非常流行的传感器。它的检测距离比相机更远，感知信息也更全面。然而，激光雷达的价格通常很高，并且需要巨大的计算能力和数据存储容量来处理大量的点云数据。因此，激光雷达尚未在路侧广泛安装。与激光雷达相比，毫米波雷达（简称MMW雷达或雷达）价格便宜得多，对物体速度极其敏感，并已广泛应用于实际工程场景中。MMW雷达的结构化数据包括物体在雷达坐标系中的位置和精确速度。MMW雷达几乎不受环境因素影响，并且对部分遮挡物体的检测具有鲁棒性。因此，雷达数据的轨迹连续性更好。然而，MMW雷达的检测距离有限，通常不超过300米，且雷达附近的大型车辆容易被检测为两个或多个物体。

相机和MMW雷达传感器各有优势，在一定程度上可以弥补彼此在检测方面的弱点。因此，这两个传感器的数据融合被认为是提高路侧检测精度的有效手段。

这两个传感器的时空同步是雷达-相机融合技术中一个基础但关键的过程，直接影响后续数据融合的准确性。该过程的目标是在时空维度上匹配两个传感器对同一物体的感知数据，并最大限度地减少系统偏差。大多数研究（如张氏标定法、四点标定法、伪逆法、直接线性变换、外参标定等）都集中在两个传感器的空间标定上，并假设两个传感器在时间上是同步的。关于时间维度，其他学者主要讨论了由于相机和雷达采样频率不一致导致的时间同步问题。Fu等人提出了一种多线程时间同步方法，将相机和雷达的数据存储在缓冲区中并实时更新。Feng等人也使用了类似的方法。Ma等人使用结合卡尔曼滤波和拉格朗日插值的算法来实现两个传感器数据的时间同步。Zhang和Cao融合了时间戳非常接近（Δt ≤ 0.005秒）的传感器数据。

许多类型的传感器（即相机和MMW雷达）已经安装在路侧。MMW雷达最初安装用于测量速度和交通流，而相机用于观察路况和检测异常事件，此前并未考虑雷达-相机的融合。它们隶属于不同的系统并分别工作。因此，一组中的相机和雷达可能在不同的时间线上工作，这对使用雷达测量数据标定相机提出了挑战，因为它们在同一时间戳的数据可能偏差数秒。

在本文中，我们提出了一种用于路侧相机和MMW雷达时空同步的新方法。该方法适用于相机和雷达在空间和时间上都不同步的场景。本研究做出了以下主要贡献：

1.  **基于场景特征的相机预标定方法：** 利用具有规则特征的车道线先验信息，估计角点坐标以生成预标定文件并标定相机。
2.  **基于多虚拟检测线的目标匹配：** 基于相机和雷达一致的时间流率，通过场景中的多条虚拟检测线匹配连续车辆的车头时距。
3.  **时空同步优化模型：** 建立了一个12参数的时空同步优化模型来修正预标定文件，并通过约束非线性最小化求解器进行求解。
4.  **实际工程场景测试：** 使用上海东海大桥的实测数据对所提出的方法进行了验证和评估。通过比较同步前后的时空偏差，证明了该方法的可行性和有效性。

我们提出了一种低成本且易于实施的方法，无需现场操作或额外的计时设备，只需同时记录一段时间的相机和雷达数据即可进行同步。

本文结构如下：我们在第二节介绍研究场景和框架。在第三节详细描述所提出的时空同步方法。然后，在第四节展示案例的实测数据结果和性能分析。最后，我们在第五节总结结论。



---

### II. 研究场景与框架 (Study Scenario and Framework)

本文重点研究如图1所示的场景。相机和雷达安装在路侧。它们不在同一位置，但其检测区域有一定的交集，通过该交集可以实现传感器融合。此外，它们可能隶属于不同的系统，在时间上存在数秒的偏差。此外，它们的数据采样频率也不一致（例如，相机以25 fps的帧率输出视频，而雷达的采样频率为20 Hz）。对于相机，捕获视频的每一帧，并执行目标检测算法以获取边界框和像素坐标进行定位。雷达获得的物体位置数据是雷达坐标系中物体的相对坐标，这也与相机的像素坐标系在空间上不匹配。道路中的车道线清晰可见，这在本研究中发挥了重要作用。

在所提出的研究场景背景下，我们的目标是以MMW雷达的时间和坐标系为基准，通过调整相机时间和变换像素坐标来实现相机和雷达在时间和空间上的同步。图2展示了本研究的框架，包括数据采集与预处理、基于场景特征的相机预标定、多目标跟踪与轨迹噪声估计、基于多虚拟检测线的目标匹配以及时空同步优化模型。每一步的实施过程将在第三节中描述。



---

### III. 时空同步方法 (Spatio-Temporal Synchronization Method)

#### A. 数据采集与预处理
使用一段时间内同时采集的相机和雷达数据进行预处理和后续分析。相机以25 fps的帧率记录视频，并对每一帧执行目标检测算法以分配边界框。我们取每个边界框中特定锚点的像素坐标来代表图像中的物体位置。该点可以是角点、中心点或边缘中点等。锚点的选择可以针对不同场景进行定制，但必须确保该点相对于物体的相对位置不会随着物体在空间位置的变化而发生显著改变。

MMW雷达在其二维坐标系中输出20 Hz的检测结果，包括每个物体的位置、速度和ID。雷达坐标系位于与道路相同的平面上。它以雷达的位置为原点，道路方向为Y轴，垂直于道路的方向为X轴。为了保持雷达数据采样频率与视频帧的一致性，我们对每个车辆轨迹进行插值以重采样雷达数据，如图3所示。插值在两个雷达数据点之间进行，它们之间的间隔为50毫秒。在如此短的时间内，车辆速度变化很小，我们假设车辆处于匀速状态。因此，我们选择了线性插值方法。如图3中的示例所示，假设原始雷达数据在 t=50ms 和 t=100ms 处的X坐标已知，我们根据公式(1)计算 t=80ms 时的X坐标（Y坐标和速度插值方法相同）。

另一方面，基于雷达检测的机制，不可避免地会获得一些额外的检测或虚警。为了处理这些数据，我们统计车辆ID，并剔除ID数量非常少的数据。



#### B. 相机预标定
相机预标定的目的是将像素坐标系转换为世界坐标系。世界坐标系是一个自定义的笛卡尔坐标系。我们可以选择相机图像中的一个平面作为坐标系所在的平面，并定义坐标系的原点和轴方向。

在本文中，我们选择视频图像中的道路平面来建立世界坐标系，该平面与MMW雷达的相对坐标系在同一平面上。

我们通过选择车道线角点在真实道路上构建垂直网格，以进行相机的初步标定。通过参考车道线标准，我们可以获得车道线在空间中相对位置的先验信息（在中国，沿车道方向的车道线纵向长度为6米，车道线之间的纵向距离为9米，车道宽度在3-4米范围内），并估计每个点的近似世界坐标。同时，我们从视频图像中提取所有这些点的像素坐标。

在获得像素坐标和世界坐标的对应点后，它们之间的关系如公式(2)所示。通过计算单应性矩阵获得两个平面坐标之间的映射关系。

其中H表示单应性矩阵，通过乘以相机的内参和外参获得（公式(3)）。Z表示比例因子，u和v表示像素坐标，U和V表示对应的世界坐标。

至少需要四个对应的点对来求解单应性矩阵。对于涉及长直道路的场景，可以选择更多的车道角点来估计其世界坐标，并使用最小二乘法计算最佳单应性矩阵。然而，在涉及弯道的场景中，由于道路曲率未知，远处车道线角点的世界坐标变得难以估计。为了确保该方法在直道和弯道场景中都具有鲁棒性，我们专门选择了靠近相机的四个点，如图4所示。我们选择一个点作为原点，并估计其余三个点的世界坐标。需要注意的是，此时建立的世界坐标系仍然与雷达坐标系不匹配，且估计的世界坐标也包含噪声。此步骤仅用于获取初始标定文件，该文件将在后续过程中进行修正。

如上所述，图像中每辆车的位置可以用一个像素点表示。在求解单应性矩阵后，可以根据公式(4)和(5)计算对应于世界坐标的像素坐标。对其求逆得到公式(6)和(7)，我们可以计算对应于像素坐标的世界坐标。



#### C. 多目标跟踪与轨迹噪声估计
对相机和雷达检测到的物体执行卡尔曼滤波和全局最近邻（GNN）数据关联算法（由于原始雷达数据中的ID不连续且容易切换，我们将雷达数据输入到我们编写的多目标跟踪算法中以重新分配ID）以进行多目标跟踪。多目标跟踪算法的流程图如图5所示。

在算法中，时刻t捕获的数据与时刻t-1每个轨迹的预测值相关联。在GNN关联算法中，我们使用的阈值是一个矩形，由状态变量（位置）的均值加减N倍标准差设定。使用Kuhn-Munkres算法进行数据匹配，其中距离度量设定为物体之间的欧几里得距离。

对于新成功关联的数据，执行卡尔曼滤波以更新状态变量并预测其在时刻t+1的状态。更新后的数据记录在轨迹表（Trajectory Table）中。另一方面，关联失败的数据被放入缓存表（Cache Table），每次都执行DBSCAN聚类。当一个类别中的数据点数量达到阈值时，它们被视为一条新轨迹，分配一个新的ID（新ID = max(IDs) + 1）并移至轨迹表。其余数据留在缓存表中，直到下一次聚类。在一定程度上，缓存表的设置减少了由虚警或额外检测引起的一些异常轨迹。

关联失败的预测值被标记为缺失并记录在轨迹表中，缺失计数增加，预测过程继续。由于没有新的观测值来更新状态变量，它们的协方差将逐渐增加。当缺失的预测值重新与新数据关联时，缺失状态和缺失计数被重置。

算法中还设计了遗忘机制。当缓存表中的某个类别在很长一段时间内没有聚类出足够的数据时，它将从缓存表中移除。同样，当轨迹表中物体的缺失数据数量达到阈值或物体超出传感器的检测区域时，该物体的轨迹将被终止，其预测也会停止。

对于卡尔曼滤波，我们将状态变量设置为X轴和Y轴上的位置 $P_x, P_y$ 以及X和Y方向上的速度 $V_x, V_y$。此外，我们将测量噪声和运动模型噪声输入滤波器。然而，这些参数通常起初是未知的，需要根据实测数据进行估计。为了迭代计算测量噪声和运动模型的估计噪声，我们使用设置初始值进行跟踪、选择跟踪效果较好的轨迹、通过拟合轨迹估计噪声、更新滤波器配置并重新跟踪的过程。

曲线拟合和残差计算的方法用于估计轨迹的噪声。对于每条轨迹，我们在X轴和Y轴上分解其运动以确定时间-距离关系。然后，我们使用K次多项式函数拟合曲线。假设车辆在每个轴上的运动是变加速的线性运动。观测值与拟合值之间残差的方差被计算为传感器的测量噪声。

运动模型如公式(10)所示。将每一时刻状态变量的观测值输入到运动模型中，以获得下一时刻模型的预测值。同样的方法应用于模型的预测值作为预测噪声。需要注意的是，轨迹噪声包括系统噪声、固有噪声和环境噪声等。由于系统噪声的单向特性，我们无法通过曲线拟合轨迹点的观测值来估计传感器的系统噪声。我们将轨迹噪声定义为轨迹点观测值与拟合值之间的残差，主要目的是估计传感器的固有噪声，这需要作为一个重要的输入参数馈入卡尔曼滤波器。



#### D. 目标匹配
在对相机和雷达数据进行多目标跟踪后，为每辆车分配了一个ID，并获得了其连续轨迹。然后，我们将相机检测到的车辆ID与雷达跟踪数据进行匹配。换句话说，我们要确定雷达跟踪数据中的哪辆车对应于视频跟踪数据中具有特定ID的车辆。

我们还提出了一种在不同车道设置多条虚拟检测线的方法，以完成视频和雷达跟踪数据的目标匹配。使用这种方法，我们可以计算相机和雷达在时间偏差和空间Y方向偏差的粗略值。

以一条车道为例，我们在视频和雷达中选择同一车道的数据，并在Y轴上的相同值处为两者设置一条虚拟检测线。图6表明，两条虚拟检测线实际上并不在同一位置，因为相机和雷达在坐标系上尚未在空间上统一。当车辆通过检测线时，记录视频和雷达各自的时间戳。两个时间戳之间会有一个时间差，记为 $\Delta t_i$，这是由相机和雷达在时间和空间上的异步耦合引起的。

如公式(11)所示，$\Delta t_i$ 由视频和雷达之间的时间偏差 $\Delta T$、第 $i$ 辆车通过时由空间Y方向偏差 $\Delta Y$ 引起的时间 $\Delta Y/\bar{v}_i$ 以及误差项 $e_i$ 组成。其中假设当数据量充足时，$e_i$ 服从均值为0的正态分布。

如图7所示，记录所有通过检测线的车辆的时间戳。每辆车的信息可以用一个长度为 $N+1$ 的一维向量表示：$[t_{i-N}-t_i, ..., t_{i-1}-t_i, t_{i+1}-t_i, ..., t_{i+N}-t_i]$，其中包括车辆通过检测线的时间戳以及该车辆与前后N辆车的车头时距。当目标车辆前后没有车辆时，该值用0填充。使用欧几里得距离（公式13）作为每辆车之间的信息距离度量，并计算距离矩阵。

使用Kuhn-Munkres算法来实现视频和雷达跟踪数据之间的目标匹配。

目标函数设计如公式(14)所示，用于计算 $\Delta t_i$ 和 $\Delta \bar{t}$ 之间残差的平方和。固定视频检测线的位置，不断调整雷达检测线的位置，使目标函数值最小化。也就是说，我们使 $\Delta Y$ 接近0。当目标函数值达到最小值时，$\Delta T$ 也可以近似得到。这两个参数可以用作后续时空同步优化模型的初始值。上述针对单车道上一条检测线实测数据的计算过程可以扩展到其他车道或具有不同位置的检测线。



#### E. 时空同步优化模型
相机的初步标定使我们能够将像素坐标转换为世界坐标，这些坐标与雷达坐标系处于同一平面。然而，此时世界坐标系和雷达坐标系仍未完全匹配。在本节中，建立了一个12参数 $(\Delta T, \Delta X, \Delta Y, \theta, K_x, K_y, (e_{x\_i}, e_{y\_i})_{i=1}^3)$ 的时空同步优化模型来完成匹配。

图8表明世界坐标系和雷达坐标系之间存在三种偏差。首先，两个坐标系的原点不在同一位置，它们之间有一定的距离，这可以在雷达坐标系中分解得到 $\Delta X$ 和 $\Delta Y$。其次，两个坐标系之间存在平面角度偏差，我们将其设为 $\theta$。如果在相机预标定期间设置的世界坐标系Y轴与雷达坐标系Y轴方向大致相同，则该角度通常较小。最后，两个坐标系在X轴和Y轴上存在尺度偏差。由于世界坐标点是根据相机标定期间相关规范的先验信息估计的，它们不一定与雷达坐标系匹配。因此，我们在两个方向上设置了尺度偏差系数 $K_x$ 和 $K_y$。在雷达坐标系中，X方向1米的长度代表世界坐标系中的 $1*K_x$ 米，Y轴也是如此。

此外，如上所述，我们只选择了四个对应点，这在选择中包含误差。因此，除了原点外，其他三个点包含的误差也被考虑在内。公式(16)用于修正标定文件中的世界坐标。

其中 $K_x$ 和 $K_y$ 表示尺度因子；$\theta$ 表示平面角度；$(x_{w,i}, y_{w,i})$ 表示第 $i$ 个点的世界坐标，$(e_{x,i}, e_{y,i})$ 表示选择误差；$(x_{wr,i}, y_{wr,i})$ 表示从世界坐标系修正到雷达坐标系的第 $i$ 个点的坐标。

我们使用加上 $\Delta T$ 来修正相机时间：$T_{camera} = T_{camera} + \Delta T$。选择视频和雷达跟踪数据中的同一辆车，截取共同的检测时间，并计算每帧车辆坐标的欧几里得距离。为了避免异常值对计算的影响，使用检测时间内距离的中位数作为视频检测坐标与雷达检测坐标之间的距离。最后，取所有车辆距离的平均值作为目标函数(17)的目标。不断调整每个参数的值，直到目标函数的值达到最小。



---

### IV. 案例研究 (Case Study)

#### A. 场景描述与数据预处理
东海大桥（中国上海）配备了200多个相机和MMW雷达，以支持洋山港自动驾驶卡车的商业运营。图9展示了相机和MMW雷达安装在同一路侧杆上，因此它们具有共同的检测区域。车辆在三车道道路上从远到近驶向传感器。我们使用246秒的实测数据评估了该方法的有效性和可行性。

相机以25 fps的帧率保存视频。为了确保车辆识别的准确性，我们使用了在COCO数据集上具有较高平均精度均值（mAP）的DetectoRS检测算法来处理视频并获取每辆车的边界框。对于本研究的场景，我们选择了每个边界框的左下角像素坐标来表示物体的位置。此外，我们对输出结果设置了一系列约束以消除异常或不必要的检测结果，包括置信度约束 ($confidence > 0.6$)、位置约束 ($0m < y < 300m$)、车辆宽度约束 ($1m < width < 4.5m$) 和边界框的宽高比约束 ($0.5 < ratio < 1.5$)。

雷达以20 Hz的频率输出测量数据。在数据预处理期间，我们删除了出现频率较低的ID ($Num < 11$)。对数据重采样执行线性插值。



#### B. 相机预标定
图10展示了我们从视频中捕获的一帧，并选择了四个车道线角点来定位世界坐标。这里，我们假设车道线长度为6米，垂直车道距离为9米，车道宽度为4米。以右下角为世界坐标系原点，估计其他三个点的坐标。同时，也获取了这四个角点的像素坐标以计算单应性矩阵。

表I包含了计算出的单应性矩阵，图11展示了物体坐标从像素坐标系到世界坐标系的转换。雷达检测到的物体坐标也包含在图11b中，说明世界坐标与雷达坐标在同一二维平面上。然而，由于估计的世界坐标与雷达坐标不一致，它们并没有完全匹配。

表II包含了视频和雷达检测到的一辆车的部分结构化数据，包括物体在世界坐标系和雷达坐标系中的坐标，这表明两个传感器在同一时间对同一物体的检测结果存在偏差。



#### C. 多目标跟踪与轨迹噪声估计
对视频和雷达数据进行了初始跟踪。在跟踪算法中，参数设置如下：GNN阈值：$N=3$；聚类阈值：5；遗忘阈值：125帧或次。卡尔曼滤波器初始参数（测量噪声与模型噪声相同）：X方向定位标准差：0.5米，Y方向定位标准差：5米；X方向速度标准差：1 m/s，Y方向速度标准差：5 m/s。

初始跟踪后，选择跟踪效果较好的车辆轨迹进行噪声估计。对每条轨迹进行运动分解，得到如图12a所示的时间-位移图像，使用K次多项式函数进行拟合，K设为3。图12b展示了检测到的轨迹点和拟合点。轨迹的噪声定义为轨迹点观测值与拟合值之间的残差，并分别计算了视频和雷达测量的X和Y方向的定位和速度噪声（图13）。同样的方法也应用于模型的预测值作为预测噪声。再次强调，估计的噪声不包括传感器的系统噪声。比较X和Y方向的噪声差异时，我们更关注两个方向上固有传感器噪声的差异，而不是系统噪声。

表III包含了计算出的噪声的标准差。如表III所示，视频测量在X方向定位上的噪声低于雷达测量，而雷达测量在Y方向定位上的噪声较低。对于速度测量，视频和雷达测量在X方向的噪声相对接近，但雷达在Y方向的速度测量噪声远小于视频。我们分析了这种差异的原因，总结如下：道路在横向上较窄，在纵向上很长。对于视频测量，横向道路具有较高的标定精度，因此视频测量在该方向上的物体定位噪声较小。然而，雷达的测量机制在径向方向上更准确。因此，道路横向方向的测量包含更多噪声。此外，雷达根据多普勒效应测量速度，其结果比基于位置和时间微分的视频测速方法准确得多。



#### D. 目标匹配
虚拟检测线设置在雷达坐标系和世界坐标系Y方向值相同的位置。然而，两条虚拟检测线实际上位于不同的位置，因为两个坐标系在空间上不匹配。本文以雷达坐标系为基准，因此我们不断在世界坐标系中移动虚拟检测线的位置以匹配两个传感器的检测数据。

我们在三条车道的每一条上设置了101条虚拟检测线（从10米到110米每隔1米设置一条线）。在处理视频和雷达跟踪物体匹配时，车辆信息设置为车辆通过检测线的时间戳以及该车辆与前后三辆车的车头时距（N设为3）。根据公式(13)计算距离矩阵，并使用Kuhn-Munkres算法进行进一步匹配。我们不断调整雷达检测线的位置以最小化目标函数(14)的值（约束条件设置为 $\Delta Y \in [0, 100]$），然后求解 $\Delta Y$ 和 $\Delta T$。

图14展示了各车道及所有车道计算结果的统计分布。从图14可以看出，$\Delta Y$ 和 $\Delta T$ 的分布是偏斜的。如果我们取平均值作为最终结果，很容易受到个别异常值的影响。另一方面，中位数可以很好地弥补偏态分布中均值的不足。当样本中存在少量异常值时，通常使用中位数来反映样本的主要情况统计量。其算法也反映了这一特点，即特定值的变化，尤其是边界上的变化，不一定会改变该统计量的值。因此，当分布偏斜时，使用中位数更为实用。

表IV记录了计算结果的中位数。根据计算结果，$\Delta Y$ 约为58米，$\Delta T$ 约为30帧。也就是说，世界坐标系中0米（Y方向）的位置对应于雷达坐标系中58米（Y方向）的位置，雷达时间第0帧的时刻对应于视频时间第-30帧的时刻。

图15比较了某条车道（前3000帧）匹配前（图15a）和匹配后（图15b）的时间戳。代入计算出的 $\Delta Y$ 和 $\Delta T$ 后，车辆在时间戳上已经重叠。



#### E. 时空同步优化模型
根据公式(16)和(17)，建立时空同步优化模型并使用约束非线性最小化求解器求解。所有参数在确定解之前都进行了归一化。为了避免因单一解中的初始值而陷入局部最优解，我们随机生成了100组初始值，同时求解，并比较其最终目标函数值以选择较好的一个作为结果（表V）。

如图16所示，我们选择了一条具有明显特征的轨迹（一辆车执行了换道）和同步前、时间同步后以及时空同步后的目标匹配轨迹点。图16中的虚线连接同一帧中的轨迹点。图16a突出了视频和雷达存在显著的时间不匹配，图16b展示了时间同步后的目标匹配。图16c在空间上得到了进一步修正。

为了说明时间同步的有效性，我们计算了时间同步前后车辆X方向时间-速度曲线的残差。没有计算Y方向时间-速度曲线的原因是车辆在Y方向的主要运动状态是匀速移动，没有明显的变化特征。这里，我们以图17中的车辆轨迹为例，该轨迹在X方向具有明显的换道特征。我们分别计算了视频检测和雷达检测在X方向的车辆轨迹速度，绘制了时间-速度曲线并计算了它们的残差。时间同步越准确，残差越小。下图显示了时间同步前后视频和雷达检测轨迹在X方向的时间-速度曲线。需要注意的是，这里的速度计算是基于该车辆轨迹的拟合值，以获得更平滑的速度值。时间同步前的残差平均值为 $0.3836 m/s$，时间同步后降至 $0.0541 m/s$。残差减少了85.90%，这充分反映了时间同步的有效性。

图18描绘了时空同步前后视频和雷达检测到的所有车辆轨迹。图19展示了时空同步前后视频和雷达检测到的轨迹之间偏差（绝对值）的空间分布。图18和图19中的每一条线代表一辆车的轨迹。同步后，两类轨迹之间的空间偏差大大降低。X方向的平均偏差从2.47米减少到0.42米，Y方向从64.06米减少到2.34米。

对于剩余的偏差，主要因素有以下三个误差：1) 雷达检测误差；2) 视频检测误差；3) 优化模型的计算结果与真实值之间的剩余偏差，即雷达坐标系与世界坐标系在时空上的系统偏差。本文提出的方法旨在最小化优化模型计算结果与真实值之间的偏差。

图18和图19表明，在Y方向50-170米范围内的视频轨迹与雷达检测到的轨迹基本匹配，偏差较小。然而，超过170米后，它们在Y轴上的偏差不大，但在X轴上仍有一定距离。从图18可以看出，在Y方向距离超过170米处，雷达和视频相对于视频检测都向左或向右偏斜。我们认为X方向的大偏差可能是由雷达对远处新出现物体的定位发生左右漂移引起的。随着物体逐渐靠近雷达，雷达定位检测趋于稳定。



---

### V. 结论 (Conclusion)

本研究提出了一种用于传感器融合的路侧MMW雷达-相机新型时空同步方法。利用车道线角点对场景中的路侧相机进行预标定，并利用多目标跟踪算法获取过往车辆轨迹。然后，设置多条虚拟检测线以匹配路侧MMW雷达和相机的连续车辆感知数据，并建立了一个12参数优化模型来解决时空同步问题。最后，通过在中国上海东海大桥的实验证明了该方法的有效性和经济性。实验还发现，路侧相机感知数据在X方向具有较小的定位噪声，而MMW雷达更适合Y方向定位和速度测量，这为路侧传感器融合提供了新的思路。