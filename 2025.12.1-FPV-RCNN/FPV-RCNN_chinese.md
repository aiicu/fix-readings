FPV-RCNN：基于关键点的自动驾驶协同车辆检测深度特征融合



### **摘要**

研究通过车辆间共享集体感知消息（CPM）来减少遮挡，从而提高自动驾驶的感知精度和安全性。然而，实现高精度的数据共享和低通信开销是集体感知的一大挑战，特别是当联网自动驾驶车辆之间需要实时通信时。在本文中，我们提出了一种名为 Fusion PV-RCNN（简称 FPV-RCNN）的高效且有效的基于关键点的深度特征融合框架，该框架建立在 3D 物体检测器 PV-RCNN 之上，用于集体感知。我们引入了一个高性能的边界框提案（Bounding Box Proposal）匹配模块和一个关键点选择策略，以压缩 CPM 大小并解决多车辆数据融合问题。此外，我们还提出了一种基于最大一致性原则（Maximum Consensus Principle）的有效定位误差校正模块，以提高数据融合的鲁棒性。在专门用于集体感知的合成数据集 COMAP 上，与鸟瞰图（BEV）关键点特征融合相比，FPV-RCNN 在高评估标准（IoU 0.7）下的检测精度提高了约 9%。此外，其性能与两种没有共享数据丢失的原始数据融合基线相当。而且，我们的方法还将 CPM 大小显著减小到小于 0.3KB，这比以前工作中使用的 BEV 特征图共享小约 50 倍。即使进一步减少 CPM 特征通道（即从 128 减少到 32），检测性能也没有表现出明显的下降。我们方法的代码已在 https://github.com/YuanYunshuang/FPV_RCNN 开源。



---

### I. 引言 (Introduction)

理解周围环境是自动驾驶最重要的任务之一，特别是对于在复杂的现实世界情况中行驶的自动驾驶车辆（AV）而言。这种 AV 通常配备有不同的传感器，如摄像头、激光雷达（LiDAR）和声纳，以感知世界。然而，仅使用安装在单个主车（Ego Vehicle）上的传感器收集的数据来感知环境有许多局限性，例如遮挡、有限的传感器观测范围和噪声。在这方面，基于联网自动驾驶车辆（CAVs）的协同感知可以通过共享网络中多辆 AV 从不同视角收集的感知信息来有效地缓解这些问题。感知到的信息通过集体感知消息（CPMs）在车辆之间共享。通过这种方式，可以放宽每辆车上传感器的精度和可靠性要求，从而也降低了每辆 AV 的价格。

然而，协同感知的挑战在于定义要共享的信息，并通过有限的通信网络带宽融合共享的信息。因此，目标是在协同代理的网络中以最少的数据传输获得最佳的感知性能。对于要求实时通信的自动驾驶协同车辆检测而言，精确的数据共享和低通信开销仍然是一个瓶颈。理论上，共享原始数据能提供最佳性能，因为没有信息丢失。但这很容易导致通信网络因繁重的数据负载而拥塞。相比之下，共享完全处理过的数据（例如，检测到的物体）需要较少的通信资源。然而，基于物体的融合对代理的定位噪声非常敏感。匹配来自不同代理的检测物体可能非常困难，尤其是那些被远距离传感器不准确检测到的物体。作为一种折衷方案，由深度神经网络从原始数据中提取的深度特征可以减少需要共享的数据量，同时保持相对较高的数据融合性能。

以前的工作通过压缩鸟瞰图（BEV）深度特征图来实现这一点，然而这些特征图非常稀疏，可以进一步压缩以避免冗余。此外，由于分辨率低，融合此类特征图甚至可能无法预测准确的边界框。为此，本文提出了一种更鲁棒的深度特征共享和融合框架，通过将已建立的框架 PV-RCNN 扩展到集体感知场景。我们的框架使用 PointNet 和点集抽象（Point Set Abstraction）聚合来自不同点云的选定高精度 3D 关键点的多尺度感受野信息，然后共享并融合这些信息以生成更准确的检测。与 BEV 关键点融合相比，在通信开销降低的情况下，我们的 3D 关键点融合仍然实现了更高的检测精度。在合成集体感知数据集 COMAP 上的测试示例如图 1 所示。

我们的主要贡献总结如下：
1. 我们提出了一种用于协同车辆检测的 3D 关键点特征融合方案，以解决基于 BEV 特征融合方案的边界框定位精度低的问题。
2. 我们引入了一个关键点选择模块来减少共享深度特征的冗余，从而降低通信开销。
3. 我们提出了一种高效且鲁棒的定位校正模块和一个边界框匹配模块，可以为后期的深度特征融合生成高质量的边界框提案。
4. 我们提出的方法不仅大大优于使用 BEV 特征融合进行集体感知的最先进方法，而且还大幅减小了 CPM 数据大小。

---



### II. 相关工作 (Related Work)

通常，协同感知可以通过车辆到基础设施（V2I）和车辆到车辆（V2V）通信来实现。V2I 通信提供了在主车和基础设施之间交换感知信息的机会。这有助于主车超越自身感知系统的限制。Yang 等人的一个成功应用是所谓的智能路口，其中用于物体检测和跟踪的信息通过路口静态摄像头的 BEV 观测共享给主车。这些摄像头易于部署，但其感知仅限于特定路口的交通场景。相比之下，V2V 通信不受限于特定位置。在一个 CAV 网络中，每辆车都可以看作是一个带有多个传感器的节点，感知数据可以在任何地点的车辆之间共享。我们的工作专注于用于物体检测的 V2V 通信。

在 V2V 通信中，提出了不同的方法来在 CAV 网络中传输数据。在本文中，我们将数据融合策略分类为（1）原始数据共享，（2）完全处理的数据（如检测到的物体），以及（3）半处理数据。Marvasti 等人的研究表明，原始数据共享为物体检测提供了丰富的信息。然而，它消耗大量带宽，对于需要实时通信的自动驾驶来说是不可行的。与原始数据共享相反，建议仅共享检测到的物体以实现更高效的通信。然而，Wang 等人的工作表明，这种完全处理数据的后期融合表现不如早期原始数据融合或半处理数据融合。

为了在不牺牲性能的情况下减少通信资源消耗，人们进一步探索了共享半处理数据。在极端情况下，所有独立传感器均误检的物体可以在这种数据融合后被检测出来。例如，Chen 等人扩展了他们之前的工作，通过融合体素特征和使用深度神经网络（DNN）学习到的深度特征进行协同感知，而不是融合检测到的物体。一方面，在真实世界数据集 KITTI 和 T&J 上的特定交通场景（例如停车场）中显示出了显著的性能提升。另一方面，这些数据集并非专门用于集体感知，而是用于单一自我中心视角的感知。这是因为集体感知需要多辆车同时共享一定程度的视场（FOV）。但是获取这样的真实世界数据集不仅需要昂贵的设备，还需要无数小时的人工标注来获得地面真值（Ground Truth）信息。因此，许多最近的工作诉诸于合成数据以进行更全面的实证研究。数据生成器和模拟工具，例如 CARLA 和 SUMO，不仅可以被操作以生成各种交通状况下用于协同感知的大量逼真数据，还可以提供准确的地面真值信息。在相关研究中，对由 CARLA 生成的模拟点云数据集上的不同数据融合策略的比较表明，原始数据融合和深度特征融合都大大优于基于物体的融合，尤其是在引入车辆定位误差时。此外，也证实，在模拟数据集 LiDARsim 上，共享压缩的深度特征图在满足通信带宽要求的同时实现了高精度的物体检测。

尽管深度特征融合取得了初步成功，但共享的特征图由于其稀疏性仍然包含过多的冗余。这些深度特征高度抽象，难以被神经网络选择、压缩并最终融合。例如，尝试共享中间特征图用于车辆检测。发现这种策略在提供高精度边界框预测方面不够鲁棒，因为共享的特征图分辨率低，即从原始数据进行了 8 倍下采样。此外，上述所有以前的深度特征融合框架都是在没有定位误差校正的情况下与基于物体的融合进行性能评估，并且没有提供基于物体融合方法的实施细节。然而，本地物体检测和基于物体的融合的不同实现会对最终结果产生很大影响。此外，在大多数情况下，只要主车和协作车辆之间存在两个检测到的车辆匹配，仅凭检测到的车辆几何形状就可以毫不费力地恢复定位误差。因此，分析本文提出的带有定位误差校正的最终融合结果也很重要。

总之，我们研究仅共享选定的关键点特征而不是共享深度特征，旨在进一步减小特征大小的同时保持物体检测的性能。此外，我们还引入了定位误差和误差校正，以保证公平比较所有融合方法的性能。

---



### III. 方法 (Method)

#### A. 问题阐述

我们以自我中心的方式阐述集体感知问题。在主车 $C_0$ 的通信范围 $R_{C}$ 内，$N_v$ 个协作 CAVs $\{C_1, C_2, ... C_{N_v}\}$ 以及主车 CAV 在时刻 $t$ 生成了点云集 $PC=\{PC_0, PC_1, ..., PC_{N_v}\}$。基于 $PC_i$ 检测到的 $N_i$ 个车辆的边界框（BBoxes）被称为提案（proposals），记为 $B_i=\{(b_j, s_j) | j=1, ..., N_i\}$。$B_i$ 中的每个实例都是一对包含一个检测到的车辆 $b_j=(x, y, z, w, l, h, r)$ 及其对应的检测置信度 $s_j$ 的数据。在这个符号中，$xyz$ 表示 BBox 中心，$wlh$ 表示尺寸，$r \in [-\pi, \pi]$ 表示方向。在我们提出的框架中，协作 CAV $C_i (1 \le i \le N_v)$ 生成并向主车 CAV $C_0$ 共享 $CPM_i$，其中包含 $B_i$、选定并聚合的深度特征信息 $F_i$ 以及用于定位误差校正的 $K_i$ 个关键点的坐标。然后，主车 $C_0$ 将接收到的 CPMs 信息与本地信息融合，并生成 BBoxes 的最终优化预测。



#### B. 3D 关键点深度特征融合

提出的融合框架建立在 3D 物体检测器 PV-RCNN 之上，因此我们在本文余下部分将其称为 Fusion PV-RCNN 或简称 FPV-RCNN。图 2 展示了两个 CAV 的融合示例。将此框架扩展到任意数量的 CAV 也是直接的。如图所示，两个 CAV 的数据流分别着色为蓝色和黄色。我们首先分别从点云中提取深度特征（图 2a），然后选择并编码最重要的特征进行共享（图 2b）。最后，共享的特征被融合以用于最终检测（图 2c）。



**a) 特征提取：** 为了提取点云的 3D 特征，我们采用了基于体素的稀疏 CNN 主干网络，因为它具有高效率和高准确性。该网络在图 3 左下角展示。原始点云首先被体素化，然后传递给一组 3D 稀疏卷积块。原始体素特征被编码并进行 8 倍下采样为 3D 深度特征。来自最后一个稀疏卷积层的特征随后被压缩并投影为 BEV 特征。



**b) 特征选择与编码：** 主车检测模块采用 CIA-SSD 的检测头，因为它结构简单，并且可以比 PV-RCNN 中的提案生成模块生成更好的提案。此外，CIA-SSD 使用 IoU 校准检测分数，这对于我们在算法 1 中使用分数进行合并的匹配至关重要。该模块生成提案 $B_i$，随后用于选择特征点。只有位于提案内部的特征点会被选中，并进一步编码和压缩为 CPM 格式以减小 CPM 大小。

特征选择的细节如图 3 所示。最远点采样（FPS）用于采样预定义数量 $N_{kpts}$ 的均匀分布的稀疏关键点（步骤 1 到 2）。基于步骤 2 中选定的关键点，采用来自且具有相同参数的体素集抽象（VSA）模块来聚合每个选定关键点的深度特征。该模块使用 PointNet 为每个关键点聚合不同分辨率和抽象级别的相邻体素特征。聚合后的关键点特征随后被分成两条路径。在第一条路径上，这些点通过仅选择位于提案 $B_i$ 内部的关键点（步骤 3 到 4）进行进一步下采样，以生成 CPMs。在第二条路径上，它们被分类并选择用于定位误差校正。对于点云 $PC_i$，我们将 $CPM_i$ 组成如下：CAV $C_i$ 的传感器位姿、提案 $B_i$、用于融合的 $F_i$ 以及用于定位误差校正的 $K_i$ 个关键点的坐标和特征，如图 2 虚线框所示。



**c) 融合与检测：** 在融合步骤中，主车将所有接收到的提案框和关键点转换到同一局部坐标系。转换后的提案随后使用算法 1 进行聚类和合并。如果集合 B 中两个提案的 IoU 高于预定义的阈值（例如 0.3），它们将被聚类到同一子集 $C_k$ 中（步骤 1-5）。在每个 $C_k$ 中，我们首先将每个 BBox $b_i$ 的方向 $r_i$ 对齐到该聚类中所有 BBox 的主导方向，以防止因 BBox 方向冲突而导致的错误方向合并（步骤 8-13）。最后，我们通过用检测置信度 $s_i$ 对 BBox 参数进行加权，将每个聚类中的 BBox 合并为一个单一提案（步骤 14-16）。合并每个聚类中的 BBox 后，我们最终得到 K 个合并后的提案，收集在集合 M 中。

如图 2c 所示，合并后的提案 M（黑框）通过聚合该提案周围的信息（即来自不同 CPM（蓝色和橙色）的相邻关键点（深色点））进行优化。这种聚合是通过 VSA-based RoI-grid pooling 模块实现的。它将提案框划分为规则网格，并为每个网格中心总结相邻的关键点信息。聚合后的网格特征随后被拉伸为向量，并输入全连接层以生成最终的协同检测结果，其中包含正负提案的二元分类和提案框优化回归。与原始 PV-RCNN 不同，我们将全连接层中的批归一化（BN）替换为 Dropout。这是因为每帧多个点云的计算开销使得我们在训练期间只能将批量大小设置为 1，这不满足 BN 的条件。



#### C. CPM 压缩

我们遵循相关研究使用 Draco 压缩编码后的 CPM 特征，以便在比较共享原始特征图和关键点特征的 CPM 大小时也考虑压缩因素。对于这两种特征格式，我们首先将特征图的 2D 点或 3D 关键点写入 PLY 文件格式，然后使用 Draco 压缩该文件。



#### D. 定位误差校正

由于我们的 3D 融合模型依赖于高精度的 3D 关键点，定位误差将急剧降低 FPV-RCNN 的性能。为了避免这种情况，在 BBox 匹配（算法 1）之前引入了定位误差校正模块。首先，我们在如图 3 所述的选定关键点的深度特征之上添加语义分类头。然后，关键点被分类为墙、栅栏、杆、车辆和其他类别。基于语义类别，我们通过 FPS 下采样选择 $K_p$ 个杆点和 $K_{fw}$ 个墙和栅栏点。除了 $C_i, B_i$ 和 $F_i$ 之外，仅共享选定的 $K_i = K_p + K_{fw}$ 个点的 x 和 y 坐标以校正定位误差。这在图 2 的对话框中被描述为 CPM 的第 3 项内容。基于选定的杆、栅栏、墙的关键点和车辆中心，我们使用粗略搜索分辨率的最大一致性算法找到对应的车辆中心和杆点，然后使用这些对应关系计算精确的误差估计。我们不使用墙和栅栏点进行最终误差计算，因为在它们上面的匹配会导致不准确的结果。

---



### IV. 实验 (Experiments)

#### A. 数据集

为了评估所提出方法的性能，我们使用一个名为 COMAP 的合成协同感知数据集，该数据集由 CARLA 和 SUMO 模拟生成。许多现有的真实世界数据集，例如 KITTI、nuScenes 和 Waymo，更适合自我感知，而集体感知需要多个 CAV 同时以足够的 FOV 重叠观察同一场景。相反，包含各种具有准确地面真值信息的逼真协同车辆场景的合成数据集易于获取，且不需要进一步的人工数据标注工作。此外，缺乏基准数据集导致难以比较不同融合方法的性能。因此，在本文中，我们遵循许多其他工作使用此类合成数据集进行实证研究。

COMAP 中总共有 7788 帧样本——4155 帧用于训练，3633 帧用于测试。每一帧包含来自主车的点云、主车 40 米通信范围内的协作车辆的点云，以及每个 CAV 对应的 GT BBoxes。GT BBoxes 根据 57.6 米的检测范围进行选择，以保证紧急制动的最小安全距离。为了通信效率，最多只加载四辆协作车辆的点云。为了便于特征融合步骤，所有点云的方向都对齐到世界坐标系。此外，z 坐标（高度）也进行了对齐，以避免因安装在不同高度车辆上的激光雷达导致的物体检测性能大幅下降。对齐后，所有点云在 x-y 平面上按检测范围过滤，并在高度范围 $[-0.1, 3.9]m$ 内过滤。在训练期间，没有观测到反射点的被遮挡 GT BBoxes 被移除。最后，预处理后的点云在输入框架中的 DNN 之前被体素化为 0.1 米的大小（见图 2）。



#### B. 比较模型和基线

**a) BEV 关键点深度特征融合：** 由于第 II 节中提到的融合深度特征的工作都共享 BEV 特征，我们也建立了一个 BEV 特征融合的比较模型。然而，与以前的工作不同，我们只选择提案 $B_i$ 内部的特征进行共享，以确保 BEV 和 3D 特征融合在类似的 CPM 大小量级下进行公平比较。我们将此框架标记为 BEV。BEV 特征融合的流程与图 2a-c 中描绘的流程兼容。与 FPV-RCNN 不同的模块细节如图 4 所示。由特征提取生成的 BEV 特征被传递到空间语义特征聚合（SSFA）模块，该模块可以提取更鲁棒的特征以生成准确的预测。此特征图由两个卷积层进一步编码和压缩，然后由提案 $B_i$ 进行选择。除了选定的 BEV 关键点外，这种情况下的 CPM 还包含传感器位姿，但不包含提案，因为单阶段检测器不需要它们。在融合步骤中，共享的特征图首先通过几个转置卷积层上采样到更高的分辨率，然后通过加权特征图的求和进行合并。权重是自动适应的，因为它们是由卷积层学习的。合并后的特征图随后由三个卷积层进一步融合并收缩到检测分辨率以进行最终检测。



**b) 基线：** 我们将原始数据融合策略作为基线。该策略避免了共享过程中的任何数据丢失，因此更有可能表现最佳。即，我们采用两个对应的原始数据融合网络作为基线——一个用于 BEV 关键点融合（记为 $B_{bev}$），另一个用于 3D 关键点融合（记为 $B_{fpvrcnn}$）。$B_{bev}$ 采用 CIA-SSD 作为基础物体检测器。其融合框架采用自相关研究，并且部分取自 FPV-RCNN 框架（仅包含特征提取和主车检测模块）。对于 $B_{fpvrcnn}$，我们在 $B_{bev}$ 中添加了 VSA 和 RCNN（RoI-grid pooling 和检测头）模块以优化提案，使其尽可能与 FPV-RCNN 相似。



#### C. 实验设置

**a) 训练设置：** 训练目标是相对于预定义的锚点（anchors）生成的。对于 $B_{bev}$、BEV、$B_{fpvrcnn}$ 的主车检测以及 FPV-RCNN，我们在 8 倍下采样特征图的每个位置生成对应于旋转 0 和 $\pi/2$ 的两个锚点。这些锚点的长、宽、高分别为 [4.41, 1.98, 1.64] 米。如果锚点与 GT BBox 的 IoU 超过 0.6，则定义为正样本；如果低于 0.45，则为负样本；否则在分类中被忽略。对于 $B_{fpvrcnn}$ 和 FPV-RCNN 的协同检测，我们生成相对于主车检测合并提案（见算法 1）的目标。但这里使用 0.3 的单一 IoU 阈值来区分正样本 $(\ge 0.3)$ 和负样本 $(< 0.3)$。对于主车检测，我们分别监督所有传入点云 PC 的预测结果。然而，对于协同感知，我们仅从主车点云的角度监督检测结果。

采用与原始工作相同的 SSD 头损失函数和参数进行物体分类。但正负样本的权重不同，即 50 对 1，以防止网络将所有样本归类为负样本。对于 RCNN 头，二元交叉熵损失用于分类，平滑 $L_1$ 损失用于回归。它们在所有样本上进行了归一化。由于 CAV 之间也相互共享自身的位姿，我们在将主车和所有选定的协作车辆的 GT BBoxes 输入非极大值抑制（NMS）之前，也将它们添加到检测中。分类分数和 NMS IoU 的阈值分别设置为 0.3 和 0.01，并在测试阶段保持不变。

我们在单个 Nvidia 1080Ti GPU 上运行所有实验，以模拟 AV 中受限的计算资源。$B_{bev}$ 从头开始训练 50 个 epoch，批量大小为 8 帧。训练好的权重用于初始化 $B_{fpvrcnn}$、BEV 和 FPV-RCNN 中的特征提取和主车检测模块的权重。然后这三个网络再微调 10 个 epoch，$B_{fpvrcnn}$ 的批量大小为 4，其余两个为 1。应用 Adam 优化器（系数为 0.95 和 0.999）通过随机梯度下降来优化损失。其学习率和衰减均设置为 $1e^{-4}$。我们在代码库中提供了详细设置以复现我们的模型。



**b) 测试设置：** 测试了不同数量的协作车辆以分析协同感知的性能。这是通过在每次测试运行中固定协作车辆数量 $N_v$ 来完成的。具体来说，$N_v$ 在 0、2 到 4 之间变化。在每次运行中，仅选择至少拥有 $N_v$ 个协作点云的帧作为评估测试集。如果协作点云数量超过 $N_v$，我们从中随机选择 $N_v$ 个，以模拟 CAV 的随机几何分布。

此外，针对关键点特征融合分析了不同的 CPM 特征通道。我们将 CPM 特征通道设置为 128，以便在 CPM 压缩过程中无信息丢失的条件下，与 BEV 和 3D 关键点融合进行比较。为了进一步研究在 FPV-RCNN 框架中减小 CPM 大小的可能性，我们通过设置不同的 FPS $N_{kpts}$（2048 和 1024）和不同的 CPM 特征编码通道 $N_{ch}$（128、64 和 32）进行了一系列实验。

最大一致性算法对噪声幅度非常稳定——不同的噪声分布只会导致最大一致性算法搜索范围的变化。因此，我们仅对每辆车的绝对定位误差使用一个固定的正态分布，以研究位姿误差对融合框架的影响。这与相关研究不同，后者在主车和协作车辆之间的相对位姿中引入误差，并将平移误差从 0 变化到 0.4 米，旋转误差从 0 变化到 $4^{\circ}$。在我们的实验中，我们仅使用他们工作中最大的误差设置作为主车和协作车辆的全局定位误差：x 和 y 方向为 $N(0, 0.4^2)m$，车辆方向为 $N(0, 4^2)^{\circ}$。这将导致更大的相对误差。根据误差标准差，最大一致性的搜索范围经验性地设置为 x 和 y 轴 $[-1, 1]m$，方向 $[-6, 6]^{\circ}$。平移和方向的搜索分辨率分别设置为 1 米和 $1^{\circ}$。



**c) 评估指标：** 所有结果均通过精确率-召回率曲线下面积定义的平均精度（AP）进行评估。使用 IoU 标准（0.3, 0.5, 0.7）来统计正检测，以评估检测性能。

---



### V. 结果与评估 (Result and Evaluation)

#### A. 与基线的比较

表 I 显示了基线（灰色单元格）和融合模型的 AP 分数。在有协作车辆 $(N_v > 0)$ 的情况下，与 $B_{bev}$ 和 $B_{fpvrcnn}$ 融合基线（灰色单元格中的粗体字）相比，BEV 融合在低 IoU 阈值（0.3）下的性能下降幅度很小，是可以接受的。值得注意的是，FPV-RCNN 的性能甚至超过了 $B_{fpvrcnn}$，在不同的 IoU 下都有小的 AP 增益。例如，当只有两辆协作车辆时，BEV 在 $IoU=0.3$ 时的 AP 相比其基线下降了 0.72%，而 3D 融合甚至增加了 0.43%。然而，随着 IoU 阈值增加到 0.5，BEV 和 $B_{bev}$ 之间的差距略有增加。在 $IoU=0.7$ 时，它们之间的差距甚至增加到 8.34%。相比之下，FPV-RCNN 的性能略好于其基线 $B_{fpvrcnn}$，且它们的性能差距很小并保持一致。这意味着额外的 RCNN 头有助于提高低 IoU 阈值下 BBox 的定位精度，但对 BBox 的召回率帮助不大。这是因为 RoI-grid pooling 可以更好地聚合从点云中学到的 3D 关键点特征，以实现高精度的 BBox 预测。换句话说，与 BEV 相比，我们的模型更适合于协同物体检测的特征融合，能够提供高精度和可靠的 BBox 预测。

尽管如此，当没有协作车辆时，我们的 FPV-RCNN 表现比其他两个基线差得多。这是因为这些自依赖的检测结果仅由特征提取和主车检测模块生成。这两个模块的权重是在整个 BEV 和 3D 融合框架的训练过程中进行微调的。这一观察结果表明，BEV 倾向于学习对单个点云检测任务更有帮助的特征。因此，在这种配置下它会过拟合，其性能优于更通用的 $B_{bev}$（例如，在 $IoU=0.7$ 时为 61.59% 对 57.98%）。相比之下，FPV-RCNN 专注于学习对后期融合有用的特征，因此受到非协同检测的原始预训练权重的反向影响。需要注意的是，这个问题可以通过在实际应用中根据需求加载不同的预训练权重来规避。



#### B. 不同 CPM 大小下的 FPV-RCNN 性能

表 II 显示了具有不同 CPM 编码参数的 FPV-RCNN 结果。$N_{kpts}$ 代表 FPS 的关键点数量，$N_{ch}$ 代表编码 CPM 特征的通道数。此外，结果是用两种不同数量的协作车辆 $(N_v=\{2, 4\})$ 进行评估的。总体而言，更好的性能主要与更大的 $N_{kpts}$ 相关，所有最佳 AP 分数（粗体蓝色字体）都出现在 $N_{kpts}=2048$ 时。但对于特定的 IoU 和 $N_v$，不同 $N_{ch}$ 的性能变化范围小于 1%。有趣的是，在大多数情况下，最佳 AP 甚至出现在最小的 $N_{ch}$ 处（粗体字体）。



#### C. 关于定位噪声的消融研究

为了展示我们在算法 1 中提出的匹配模块的有效性，我们将此模块的结果与 V2Vnet 中使用的 NMS 物体融合进行了比较。如表 III 中粗体字体所示，我们的匹配模块在大多数情况下优于 NMS 融合。特别是当存在定位噪声时，使用算法 1 的匹配更加稳定。此外，我们还研究了 FPV-RCNN 第二阶段 3D 特征融合的性能增益。表现最好的结果用蓝色标记，这清楚地表明 FPV-RCNN 的融合模块可以优化结果。此外，通过观察检测结果，我们发现大多数误报检测在第二阶段 3D 特征融合中被 RCNN 移除了。然而，这种效果很难影响 AP 结果。因此，我们在完整的 FPV-RCNN 和算法 1 的结果之间没有观察到大的 AP 提升。此外，如表 III 所示，正如预期的那样，带有定位噪声的 3D 模型表现比无噪声版本差。但在较低 IoU 阈值下的性能仅略有下降。然而，如图 6 所示，在有定位噪声的情况下，我们的 3D 模型仍然比 BEV 融合模型表现更好。由于原始数据融合基线没有关键点选择和分类模块，难以校正定位误差，因此它们的结果未在图 6 中绘制。

此外，我们比较了在所有 CPM 和 CAV 数量上平均的压缩深度特征的 CPM 大小。图 5 给出了 BEV 和 3D（FPV-RCNN，用不同的 $N_{kpts}$ 标记）关键点特征共享之间的定量比较。可以看出，压缩关键点特征的平均 CPM 大小减小到 0.3KB 左右，这比压缩整个特征图生成的 CPM（约 14KB）小约 50 倍。在相同特征通道数 $(N_{ch}=128)$ 下，3D 关键点融合传输的数据比 BEV 关键点融合少，但性能却有大幅提升（90.88% 对 82.21%，见表 I）。此外，我们的框架生成的 CPM 大小与在低交通密度场景中评估的基于物体的标准化 CPM 处于同一数量级。

上述观察结果表明，我们提出的框架对 CPM 特征编码大小的变化相对稳定。因此，如果通信网络未被完全占用且无线网络可以处理更大的 CPM，最好增加 $N_{kpts}$ 而不是增加特征编码通道。另一方面，作为一个巨大的优势，如果通信网络已经负载很重，CPM 可以被压缩到尽可能小，而性能仅略有下降。

尽管如此，当前模型也有几个局限性。首先，我们仅在合成数据上进行了实证研究。在未来的工作中，我们将首先把实验扩展到真实世界数据，以进一步分析所提出的 FPV-RCNN 模型的功效。其次，通信延迟仅由 CPM 大小反映。这需要通过分析真实世界场景中的 CPM 通信和传输来进一步研究。

---



### VI. 结论 (Conclusion)

在本文中，我们提出了一种名为 FPV-RCNN 的高效框架，用于基于点云的自动驾驶协同车辆检测。该框架以 PV-RCNN 为基础，通过添加关键点选择模块、带有定位误差校正的边界框提案匹配模块以及关键点融合模块，将其作为协同感知场景的物体检测基础网络。与在模拟数据集 COMAP 上的 2D BEV 特征融合相比，表明我们的方法大幅提高了协同车辆检测的性能。与以往共享完整 BEV 特征图的工作相比，我们的方法显著降低了实时通信中 CAV 网络的数据传输负载，并且由于噪声校正模块的存在，对定位噪声也更具鲁棒性。在未来的工作中，我们计划在真实的协同驾驶场景中评估我们的方法。