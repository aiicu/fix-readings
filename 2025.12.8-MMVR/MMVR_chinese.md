

基于多模态虚实融合 Transformer 的协同感知

**作者：** Hui Zhang, Guiyang Luo, Yuanzhouhan Cao, Yi Jin, Yidong Li
**单位：** 北京交通大学计算机与信息技术学院；北京邮电大学网络与交换技术国家重点实验室
**会议：** 2022 IEEE 第13届并行架构、算法与编程国际研讨会 (PAAP)

---



### 摘要

汽车智能化和网联化已成为汽车产业未来发展的必然趋势。现有的智能网联汽车（ICV）依赖单车智能来执行基本的感知任务，在处理诸如小目标和远距离目标等复杂交通场景的精确识别和定位问题时，能力仍然较弱。为了解决这一问题，我们提出了一种用于协同感知的多模态虚实融合 Transformer。具体而言，为了获取 RGB 图像和激光雷达点云的互补信息，我们提出了多模态虚实融合（MVRF）方法，该方法生成虚拟点并补偿稀疏位置上的点信息缺失。此外，构建了异构图注意力网络（HGAN）来捕捉智能体间的交互并自适应地融合多智能体的特征。HGAN 包含一系列编码器层，每一层都有一个异构智能体间注意力模块和一个多尺度自注意力模块，旨在根据不同智能体类型学习不同的关系，并同时捕捉全局和局部的空间注意力。广泛的实验表明，与最先进的方法相比，所提出的方法获得了更优越的性能。

**关键词：** 协同感知，智能网联汽车，多模态融合

---



### I. 引言

智能网联汽车（ICV）是指搭载先进的车载传感器、控制器、执行器等装置，并融合现代通信与网络技术，实现车与车、路、人等之间的智能信息交换与共享。凭借复杂的环境感知、智能决策、协同控制等功能，它可以实现安全、高效、舒适、节能的驾驶，最终实现替代人类操作的新一代汽车。ICV 集成了感知、决策和控制。具体来说，它通过分析车载传感器的实时信号来感知道路环境，并决定安全高效的行驶路线，从而控制车辆的转向和速度以到达预定位置。因此，环境感知是 ICV 的基本功能，也是实现行为决策和车辆控制的前提。

现有的 ICV 依赖本地的多源冗余传感器来实现对驾驶环境的感知（单车智能）。例如，华为的高级自动驾驶汽车配备了各种类型的传感器，包括激光雷达、毫米波雷达、超声波雷达和摄像头。然而，由于车载传感器安装位置、检测距离和视角的限制，在繁忙路口、恶劣天气、小目标和远距离目标等复杂交通场景下，仍然难以解决精确感知、识别和定位的问题。依靠单车传感器融合很难解决上述问题，这严重制约和影响了 ICV 的安全性和鲁棒性。例如，2016 年 5 月，一辆特斯拉 Model S 在佛罗里达州的一个路口与一辆左转的半挂卡车相撞，导致一人死亡。这次事故的主要原因是单车摄像头的感知能力受到了复杂多变的环境因素的影响。特别是在城市道路场景中，单车感知的范围和精度容易受到天气、目标距离等因素的影响。仅依靠单车传感器的堆叠无法克服复杂交通场景下感知能力不足的障碍。

协同感知提供了一种扩展车辆感知范围和提高感知精度的替代方法。最近的协同感知侧重于如何利用来自邻近车辆和基础设施的视觉线索来提升整体感知性能。一些方法利用原始数据融合来获得整体视图。它们可以从根本上克服小目标和被遮挡目标的问题，但需要巨大的传输带宽。一系列工作通过后期融合进行协同。虽然它们节省带宽，但每个智能体的感知输出可能充满噪声且不完整，导致不可逆的信息丢失和感知错误。中间融合是处理感知精度和通信带宽之间权衡的更有效方式。F-Cooper 以相同的权重融合共享特征。V2VNet 通过空间感知图神经网络聚合特征。DiscoNet 提出了一个教师-学生框架，通过将编码特征限制为早期融合的特征来增强训练。学生模型可以在早期融合监督的指导下学习更好的表示。然而，它在远距离小目标上仍然表现不佳。如图 1 所示，点云非常稀疏的远距离物体无法被精确检测。在稀疏点云的情况下，存在一些误报和漏报。

为了解决这个问题，在本文中，我们引入了一种用于协同感知的多模态虚实融合 Transformer。首先，我们选择其中一辆 ICV 作为主车（ego vehicle），在连接的智能体（车辆或基础设施）之间构建异构图。所有其他连接的智能体（不包括主车）将其激光雷达点云映射到主车的坐标格式中。同时，我们提出了多模态虚实融合（MVRF）方法来增强稀疏点云位置上的信息。然后，采用基于锚框的 PointPillar 来从多个智能体的点云生成视觉特征。为了有效地建模智能体间的交互并自适应地聚合它们的特征，提出了异构图注意力网络，它由 3 个编码器层组成，每一层包含一个异构智能体间注意力（Hiaa）模块、一个多尺度自注意力（Mssa）模块和一个前馈网络（Ffn）。最后，聚合的特征被进一步用于分类和边界框回归。

---



### II. 方法论

在本文中，我们将 V2X 感知视为异构多智能体协同感知。我们的目标是利用一种基于多模态信息的专门设计的 Transformer，它可以有效地实现动态异构关系图的多智能体协同特征融合，如图 2 所示。具体而言，为了利用 RGB 图像和激光雷达点云的互补信息，我们提出了 MVRF 方法，基于 RGB 图像生成虚拟点并执行虚实融合，以便共同利用它们来减少稀疏点位置上的漏报和误报。此外，构建了 HGAN 模块以捕捉长期全局依赖关系和局部上下文信息。



#### A. 多模态虚实融合 (Multi-Model Virtual-Real Fusion)

MVRF 依赖于 2D 检测、3D 检测以及从 2D 到 3D 空间的变换。对于 2D 检测，我们利用 CenterNet 检测器，它将每个对象检测为关键点三元组。具体来说，我们用 $X$ 表示 RGB 图像。检测器以 $X$ 为输入，预测边界框 $B_{x} \in \mathbb{R}^{4}$ 和类别分数 $S_{x} \in \mathbb{R}^{C}$，其中 $C$ 是类别数量。此外，我们在用于掩码估计的建议生成网络之上扩展了一个级联 RoI head，它为每个对象生成像素级分割 $M_{x,b} \in [0,1]^{W \times H}$。

对于 3D 检测，设 $P=\{(u_{i},v_{i},o_{i},r_{i})\}$ 为点云，其中 $(u_{i},v_{i},o_{i})$ 是 3D 坐标，$r_{i}$ 是反射率因子。3D 检测器旨在从点云 $P$ 生成 3D 边界框 $B$。3D 边界框 $B=\{(u_{j},v_{j},o_{j},l_{j},w_{j},h_{j},\theta_{j})\}$ 由 3D 中心坐标 $(u_{j},v_{j},o_{j})$、物体尺寸 $(l_{j},w_{j},h_{j})$ 和沿 z 轴的偏航旋转 $\theta_{j}$ 组成。在本文中，我们将点云 $P$ 处理为鸟瞰图 pillars，并使用 2D 卷积骨干网提取特征。来自多个智能体骨干网的特征被收集并在下文中用提出的 HGAN 进行处理。最后，它们被 PointPillars 中的检测头用于生成 3D 边界框 $B$。

基于 2D 检测结果，我们预测密集的虚拟点 $V=\{(u_{k},v_{k},o_{k},r_{k})\}$ 并将其映射到 3D 激光雷达空间，如图 3 所示。为了实现这一点，我们首先将激光雷达点云映射到 RGB 图像坐标中。我们分别用 $t$ 和 $t^{\prime}$ 表示激光雷达传感器和 RGB 图像的捕获时间。设 $T_{lidar2car}$ 为从激光雷达传感器到车辆参考系的映射。$T_{t^{\prime}2t}$ 表示车辆从 $t^{\prime}$ 到 $t$ 的映射。$T_{car2rgb}$ 表示从车辆参考系到 RGB 传感器的映射。因此，从激光雷达到 RGB 传感器的映射表示为：

$$
T_{lidar2rgb}=T_{car2rgb}T_{t^{\prime}2t}T_{lidar2car} \tag{1}
$$

然后，我们获得具有对应深度 $d_{i}$ 的图像坐标 $n_{i}$，并将落入检测掩码 $M_{x,b}$ 的单个检测 $b$ 的映射点收集为一个簇 $U_{b}=\{(n_{i},d_{i})|n_{i} \in M_{x,b} \forall i\}$。该簇仅收集落入检测掩码 $M_{x,b}$ 的映射点 $n_{i}$。接下来，使用随机采样方法在 2D 图像空间中生成虚拟点 $V \in M_{x,b}$。对于每个虚拟点 $V_{k}$，我们利用其在簇 $U_{b}$ 中的最近邻来预测深度：$d=\arg \min_{d_{i}}||n_{i}-V_{k}||$。$r_{k}$ 由 2D 检测器中的对象性分数生成。给定 2D 坐标和预测的深度，我们可以将虚拟点映射回 3D 激光雷达空间。虚拟点和原始真实点被融合以进一步执行协同检测。



#### B. 异构图注意力网络 (Heterogeneous Graph Attention Network)

从车辆和基础设施获得的传感器信息可能具有不同的属性。基础设施处的激光雷达通常安装在较高的位置，可以感知更广范围内的物体。车辆处的激光雷达可以在一定范围内感知周围环境。为了处理这种异构性，我们构建了一个基于 Transformer 的 HGAN 来捕捉长期全局依赖关系和局部上下文信息，如图 4 所示。有向图表示为 $G=(P,E)$，其中 $P$ 是由基础设施 $I$ 和车辆 $S$ 组成的节点集，$E$ 包含四种类型的边，即 $E=\{I-I, I-S, S-I, S-S\}$。对于智能体 $a$，其节点类型表示为 $P_{a} \in \{I,S\}$。基于 Transformer 的 HGAN 由 3 个编码器层组成，每一层包含一个 Hiaa 模块、一个 Mssa 模块和一个 Ffn 模块。

Hiaa 模块旨在自适应地融合来自不同智能体的信息，表示为：

$$
H_{a} = \text{Linear}_{P_a} (\sum_{b \in N(a)} \text{MultiHead}(a, b) \times \text{FeatAggre}(a, b)) \tag{2}
$$

它包含一个线性网络 $P_{a}$、一个异构多头注意力 MultiHead 和一个特征聚合网络 FeatAggre。$N(a)$ 是距离主智能体小于固定阈值的邻居智能体集合，$P_{a}$ 是节点类型。具体来说，$\text{Linear}_{P_{a}}$ 包含一组以相关节点类型 $P_{a}$ 为条件的线性层。$\text{MultiHead}(a, b)$ 生成节点对之间的注意力权重，表示为：

$$
\text{MultiHead}(a, b) = [\text{head}_{multi}^{1}(a, b), \text{head}_{multi}^{2}(a, b), \dots, \text{head}_{multi}^{h}(a, b)] \tag{3}
$$

$$
\text{head}_{multi}^{j}(a, b) = \text{softmax}((W_{H}^{Q}H_{a} \times W_{E(a,b)}^{multi,j} \times (W_{P_{a}}^{K}H_{a})^{T})) \tag{4}
$$

其中 $h$ 是头的总数，$j$ 是当前头编号。$W_{P_{b}}^{K}$、$W_{P_{a}}^{Q}$ 和 $W_{E(a,b)}$ 在不同头中具有不同的参数。$\text{FeatAggre}(a, b)$ 用于聚合不同智能体的特征，表示为：

$$
\text{FeatAggre}(a, b) = [\text{head}_{aggre}^{1}(a, b), \text{head}_{aggre}^{2}(a, b), \dots, \text{head}_{aggre}^{h}(a, b)] \tag{5}
$$

$$
\text{head}_{aggre}^{j}(a, b) = \text{linear}_{P_{b}}^{j}(H_{b}) \times W_{E(a,b)}^{aggre,j} \tag{6}
$$

其中 $\text{linear}_{P_{b}}^{j}(H_{b})$ 表示由节点类型 $P_{b}$ 和当前头编号 $j$ 索引的线性层。

Mssa 模块用于捕捉全局和局部空间特征交互。它采用各种窗口大小来捕捉不同的注意力范围，这可以极大地提升 3D 检测性能。假设其中一个窗口大小为 $Z_{j} \times Z_{j}$，输入特征为 $H_{n} \in \mathbb{R}^{H \times W \times C}$，Mssa 的输出可以公式化为：

$$
\text{head}^{j} = [h^{1}, h^{2}, \dots, h^{HW/Z_{j}^{2}}] \tag{7}
$$

$$
\overline{head}_{m}^{i} = \text{softmax}(\frac{h^{i}W_{m}^{Q} \times (h^{i}W_{m}^{K})^{T}}{\sqrt{d}} + L)h^{i}W_{m}^{V} \tag{8}
$$

$$
O_{m} = [\overline{head}_{m}^{1}, \overline{head}_{m}^{2}, \dots, \overline{head}_{m}^{HW/Z_{j}^{2}}] \tag{9}
$$

$$
O^{j} = [O_{1}, O_{2}, \dots, O_{h_{j}}] \tag{10}
$$

其中 $\text{head}$ 是分支 $j$ 的分割特征集，其窗口大小为 $Z_{j} \times Z_{j}$。$h^{i}$ 是 $\text{head}$ 中的一个元素，其中 $i \in \{1, 2, \dots, HW/Z_{j}^{2}\}$。$W_{m}^{Q}$、$W_{m}^{K}$ 和 $W_{m}^{V}$ 代表查询、键和值映射矩阵。$L$ 是相对位置编码。$O_{m}$ 是分支 $j$ 的第 $m$ 个头的输出，其中 $m=\{1, 2, \dots, h_{j}\}$。$h_{j}$ 是注意力头数。$h_{j}$ 个头的输出被连接以生成当前分支的最终输出 $O^{j}$。为了捕捉不同的注意力范围，我们利用不同的窗口大小，每个窗口大小对应一个分支。我们通过一个分割注意力（Split Attention）模块整合来自所有分支的输出 $O^{j}$，以自适应地融合多尺度局部和全局上下文信息。

---



### III. 实验

#### A. 实验设置

我们选择了公开可用的 V2X 感知数据集 V2XSet。它是由 CARLA 和 OpenCDA 生成的。CARLA 提供真实的交通场景建模和传感器模拟，OpenCDA 用于同时控制一系列车辆和车辆网络通信协议。该数据集收集了覆盖 CARLA 中 5 种不同道路类型和 8 个城镇的 73 个交通场景。每个场景最多 25 秒。此外，可以相互通信的智能体数量在 2 到 5 之间。每个智能体配备 32 线激光雷达，数据范围为 120 米。基础设施传感器部署在路口、路段中间和入口匝道，高度为 14 英尺。数据集总共包含 6694 个训练样本，1920 个验证样本和 2833 个测试样本。在训练中，主车将在车辆中随机选择，而在测试中，所有比较方法都设定固定的主车。体素大小沿 X、Y 和 Z 轴设置为 [0.4m, 0.4m, 4m]。检测范围沿 X、Y 和 Z 轴设置为 [-140.8, 140.8]m，[-38.4, 38.4]m，[-3, 1]m。Transformer 网络包含 3 个编码器层，Mssa 模块中有 3 种类型的窗口大小，即 $Z_{j} \in \{4, 8, 16\}$。应用 Adam 优化器以端到端的方式训练所提出的方法。初始学习率设置为 $10^{-3}$，每 10 个 epoch 衰减 0.1 倍。



#### B. 实验结果

在本文中，我们将 NoFusion 方法作为基线，该方法仅利用主车的点云而不进行协同感知。我们还与 LateFusion 和 EarlyFusion 方法进行了比较。LateFusion 合并多个智能体的识别结果，并利用非极大值抑制生成最终检测结果。EarlyFusion 收集多个智能体的原始传感器数据，以实现周围环境的全局感知。中间融合聚合来自每个智能体的编码特征，并利用聚合后的特征实现协同检测。我们与五种最先进的中间融合方法进行了比较：F-Cooper，OPV2V，V2VNet，DiscoNet 和 V2X-ViT。

表 I 展示了在 V2XSet 数据集上与先前最先进方法的性能比较。在完美设置下，具有协同检测的方法比基线 NoFusion 取得了显著的性能提升。我们提出的方法在 AP@0.5 和 AP@0.7 上分别比它高出 28.3% 和 31.3%。与 EarlyFusion 相比，我们在 AP@0.5 上增加了 7.0% 的 AP，在 AP@0.7 上增加了 0.5% 的 AP。F-Cooper 利用特征级融合，并借助 maxout 策略突出重要特征。其性能在 AP@0.5 上下降了 4.9%，在 AP@0.7 上下降了 3.5%。OPV2V 实现了一种单头自注意力融合机制来聚合多智能体间的特征。我们提出的方法大幅超越了它，即在 AP@0.5 上高出 8.2%，在 AP@0.7 上高出 5.1%。V2VNet 利用空间感知图神经网络来解析和聚合不同智能体的特征。与其相比，我们的方法在 AP@0.5 上增加了 4.4% AP，在 AP@0.7 上增加了 3.8% AP。DiscoNet 提出了一种蒸馏协作图，结合知识蒸馏和矩阵值边权重，以自适应地建模智能体间注意力并增强协同检测性能。我们提出的方法在 AP@0.5 和 AP@0.7 上分别比它高出 4.5% 和 2.0%。V2X-ViT 提出了多智能体自注意力和多尺度窗口自注意力模块，以捕捉智能体间交互和智能体内空间关系。它在 AP@0.5 上达到 88.2% AP，在 AP@0.7 上达到 71.2%，其性能在 AP@0.5 和 AP@0.7 上分别下降了 0.7% 和 0.3%。

在噪声设置下，LateFusion 的准确率急剧下降至 AP@0.5 的 54.9% 和 AP@0.7 的 30.7%，甚至比 NoFusion 基线更差。EarlyFusion 的性能在 AP@0.7 上达到 38.4%，这仍然比 NoFusion 基线差。相反，我们的方法在 AP@0.5 上达到 84.3%，在 AP@0.7 上达到 61.9%，比基线有显著提升，即在 AP@0.5 上高出 23.7%，在 AP@0.7 上高出 21.7%。对于中间融合，我们的方法在 AP@0.5 和 AP@0.7 上都显著优于 F-Cooper 和 OPV2V。具体来说，F-Cooper 在 AP@0.7 上达到 46.9%，OPV2V 达到 48.7%，而我们的准确率为 61.9% AP。我们将结果与直接采用空间感知图网络融合特征的 V2VNet 进行了进一步比较。我们比它提升了 5.2%（AP@0.5）和 12.6%（AP@0.7）。此外，与 DiscoNet 相比，我们的方法在 AP@0.5 上实现了 4.5% 的提升，在 AP@0.7 上实现了 7.8% 的提升。

图 5 展示了一些定性检测结果。可以观察到，所提出的方法可以很好地定位物体，即使是远距离物体。例如，在第一行的第三张图像中，虽然最左侧的点云非常稀疏，但使用所提出的方法可以精确检测到物体。同样的情况也可以在第二行的最后两张图像中看到。即使车辆距离主车很远，也能被准确检测到。这些证明了我们提出的方法的有效性。



**表 I：V2XSet 数据集上与先前方法的比较。我们展示了 IoU=0.5 和 0.7 时在完美（Perfect）和噪声（Noisy）设置下的平均精度 (AP)。所有数字均为百分比 (%)。**

| 模型         | Perfect AP@0.5 | Perfect AP@0.7 | Noisy AP@0.5 | Noisy AP@0.7 |
| :----------- | :------------- | :------------- | :----------- | :----------- |
| NoFusion     | 60.6           | 40.2           | 60.6         | 40.2         |
| LateFusion   | 72.7           | 62.0           | 54.9         | 30.7         |
| Early Fusion | 81.9           | 71.0           | 72.0         | 38.4         |
| F-Cooper     | 84.0           | 68.0           | 71.5         | 46.9         |
| OPV2V        | 66.4           | 80.7           | 70.9         | 48.7         |
| V2VNet       | 84.5           | 67.7           | 79.1         | 49.3         |
| DiscoNet     | 84.4           | 69.5           | 79.8         | 54.1         |
| V2X-ViT      | 88.2           | 71.2           | 83.6         | 61.4         |
| **Ours**     | **88.9**       | **71.5**       | **84.3**     | **61.9**     |

---



### IV. 结论

在本文中，我们提出了一种用于协同感知的多模态虚实融合 Transformer。采用多模态虚实融合方法在稀疏位置生成虚拟点，以获取 RGB 图像和激光雷达点云的互补信息。我们提出了异构图注意力网络来捕捉智能体间的交互并自适应地聚合多智能体的特征。聚合后的特征被进一步用于分类和边界框回归。与最先进的方法相比，广泛的实验验证了我们方法的重要性和优越性。