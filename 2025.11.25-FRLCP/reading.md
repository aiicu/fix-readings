论文总结：基于动作分支和联邦强化学习的车辆协作感知

**原文标题：** Vehicular Cooperative Perception Through Action Branching and Federated Reinforcement Learning
**来源：** IEEE TRANSACTIONS ON COMMUNICATIONS, VOL. 70, NO. 2, FEBRUARY 2022
**作者：** Mohamed K. Abdel-Aziz 等

---



### 1. 研究背景与动机

* **协作感知的必要性：** 单车传感器的感知范围受限于视距和盲区，协作感知通过车对车（V2V）通信共享信息，可扩展感知范围。
* **核心挑战：** 在有限的通信资源下，交换原始传感数据（如海量激光雷达点云）是不可行的。
* **关键问题：** 为了实现高效的协作感知，车辆必须解决三个基本问题：共享哪些数据？以何种分辨率共享？与哪些车辆共享？



### 2. 系统模型与主要贡献

本文提出了一个利用**深度强化学习（DRL）**的框架，旨在最大化车辆对接收到的传感信息的满意度。



#### 2.1 传感数据压缩：区域四叉树 (Region Quadtree)

* 为了减少数据量，利用四叉树结构对空间数据进行压缩。
* 感应范围被递归分解为不同分辨率的块，每个块的状态为：占用、未占用或未知。
* 车辆仅需传输特定的四叉树块，而非整个点云。



#### 2.2 问题分解与 DRL 建模

为了解决复杂的联合优化问题，作者将其分解为两个层级的 RL 问题：
1.  **路侧单元（RSU）层级：** 负责**车辆关联**（决定谁与谁配对）和**资源块（RB）分配**。
2.  **车辆层级：** 负责**内容选择**，即决定传输哪些四叉树块给关联车辆。



#### 2.3 关键技术创新

* **动作分支（Action Branching, BDQ）：**
    * **问题：** 传统的 DQN 在面对巨大的离散动作空间（如多种车辆配对和 RB 分配组合）时，动作数量呈指数增长，难以训练。
    * **解决方案：** 采用分支竞争 Q 网络（BDQ）架构，将动作维度分布在不同的网络分支上，实现了网络输出随动作空间线性增长，从而有效解决了维度灾难。
* **联邦强化学习 (Federated RL)：**
    * **目的：** 上面的训练是RSU独立和车辆独立的，引入RL为了加速训练并利用所有车辆的经验。
    * **机制：** 车辆在本地训练模型，并定期将参数上传给 RSU；RSU 聚合（平均）参数后广播回车辆，更新全局模型。具体见 Algorithm 2



### 3. 仿真结果

基于实际交通数据（SUMO 仿真）的实验表明：
1.  **BDQ 的有效性：** 在大动作空间（如高分辨率四叉树 $L=3$）下，传统 DQN 无法完成训练，而 BDQ 表现出强大的鲁棒性和良好的性能。
2.  **联邦学习的优势：** 联邦 RL 方法相比非联邦方法，在相同的训练时间内能获得更高的奖励，证明了其能显著加速训练过程。
3.  **性能提升：** 经过训练的智能体在车辆满意度方面始终优于未经训练的随机智能体，且能以更低的传输速率实现相同的满意度。



### 4. 结论

本文通过引入四叉树压缩、BDQ 神经网络架构和联邦学习机制，有效地解决了车辆协作感知中资源受限和动作空间巨大的难题，实现了高效的车辆关联、资源分配和内容选择。