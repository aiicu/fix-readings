基于动作分支和联邦强化学习的车辆协作感知

**来源：** IEEE TRANSACTIONS ON COMMUNICATIONS, VOL. 70, NO. 2, FEBRUARY 2022
**作者：** Mohamed K. Abdel-Aziz, Cristina Perfecto, Sumudu Samarakoon, Mehdi Bennis, Walid Saad



### 摘要

协作感知在将车辆的感知范围扩展到视距（LoS）之外方面起着至关重要的作用。然而，在有限的通信资源下交换原始传感数据是不可行的。为了实现高效的协作感知，车辆需要解决以下基本问题：需要共享哪些传感数据？以何种分辨率共享？以及与哪些车辆共享？为了回答这些问题，本文提出了一个新的框架，利用基于四叉树的点云压缩机制，允许基于强化学习（RL）的车辆关联、资源块（RB）分配和协作感知消息（CPM）的内容选择。此外，为了加快车辆间的训练过程，引入了联邦强化学习方法。仿真结果表明，RL 智能体能够有效地学习车辆关联、RB 分配和消息内容选择，同时最大化车辆对接收到的传感信息的满意度。结果还表明，联邦 RL 改善了训练过程，与非联邦方法相比，可以在相同的时间内实现更好的策略。

**关键词：** 协作感知，四叉树分解，联邦强化学习，车对车（V2V）通信，关联，资源块（RB）分配。

---

### I. 引言 (Introduction)

近年来，车辆正迅速配备越来越多的各种传感器（例如，雷达、激光雷达和摄像头），其质量各不相同 [3]。这些传感器支持广泛的应用，以辅助和增强驾驶体验，从简单的前方碰撞和变道警告，到更高级的完全自动驾驶应用，例如 Waymo（谷歌的自动驾驶汽车）。这些车辆和其他未来自动驾驶车辆上的内置传感器在自主导航和路径规划中起着至关重要的作用。然而，这些传感信息的可靠性容易受到天气条件、由于高密度交通或建筑物导致的许多盲区以及传感器的制造、部署和操作缺陷的影响，所有这些都可能危及这些备受期待的应用的成功。

为了克服这个问题，可以利用车对车（V2V）通信的最新进展（特别是未来无线系统所设想的）。V2V 通信被视为智能交通系统（ITS）的有前景的促进因素 [4]。它可以简化车辆之间传感信息的交换，以增强对其传感范围之外的周围环境的感知；这一过程被称为协作感知 [5]-[7]。协作感知的优势在 [8] 和 [9] 中得到了验证，证明了其安全效益，并表明它极大地提高了感知性能。受其潜力的激励，几个标准化机构目前正致力于正式定义协作感知消息（CPM）、其内容和生成速率 [5], [10], [11]。

此外，越来越多的文献探讨了协作感知在各种场景中的使用 [12]-[17]。在 [12] 中，作者研究了 CPM 中应包含哪些信息以提高车辆的感知可靠性。文献 [13] 和 [16] 从传感器融合的角度研究了协作感知。在 [13] 中，提出了一种混合车辆感知系统，融合了本地车载传感器数据和全局接收到的传感器数据。同时在 [16] 中，提出了两种新颖的协作 3D 物体检测方案，称为后期融合和早期融合，具体取决于融合是发生在物体检测阶段之后还是之前。此外，在 [14] 中，作者研究了感知在车队控制和通信设计中的作用。文献 [15] 的作者对原始数据级别的协作感知进行了研究，以增强自动驾驶系统的检测能力；其中融合了每辆车从连接车辆的不同位置和角度收集的传感数据。最后，作者在 [17] 中讨论了基于机器学习的协作感知方法的挑战和机遇。虽然很有趣，但这些工作都没有深入分析无线连接的影响。

通过无线网络进行的协作感知不能依赖于交换原始传感数据或点云，因为通信资源可用性有限 [5]。例如，典型的使用 64 个激光二极管的商用激光雷达每秒产生 280 万个数据点，水平和垂直视场分别为 $360^{\circ}$ 和 $26.8^{\circ}$，覆盖范围超过 70 米。即使共享这一小部分信息也需要巨大的数据速率，这就是为什么在 [18] 和 [19] 中研究了毫米波（mmWave）通信的使用，以利用其高数据速率，从而处理海量的原始传感数据传输。

为了放宽数据速率要求，从而避免使用毫米波范围的通信，应对这些原始传感数据进行有效压缩，以节省存储和可用的通信资源。一种对这种空间原始传感数据有用的可能技术被称为区域四叉树（region quadtree）[20]。区域四叉树是一种树状数据结构，用于有效地存储二维空间上的数据。四叉树递归地将二维空间分解为四个相等的子区域（块），直到块内的所有位置具有相同的状态，或者直到达到预定义的最大分辨率（树深度）。

只有少数先前的工作（例如 [21] 和 [22]）在车载网络领域中使用了四叉树概念。在 [21] 中，作者介绍了一种用于自动驾驶的通信系统，车辆可以在其中查询和访问其他人捕获的传感信息。他们使用八叉树（四叉树的 3D 版本）来建模世界，以便车辆能够轻松地查找和查询道路区域。文献 [22] 中的作者使用四叉树分解来寻找在给定地理区域内将消息中继到特定车辆的最小成本。在我们的工作中，利用四叉树概念来建模协作感知场景中的传感信息。通过这样做，一个四叉树块代表三种状态之一：占用、未占用或未知，因此，车辆可以传输覆盖特定区域的特定四叉树块，而不是传输相应的巨大点云。尽管如此，根据带宽可用性调整传输的四叉树块的数量和分辨率是一个具有挑战性的问题。

此外，正如 [5] 所建议的，简单地向所有相邻车辆广播这些传感信息（四叉树块）会对可用的通信资源造成巨大负担，特别是当车载网络拥塞时。以前的工作通过两种方式解决了这个问题：通过过滤 CPM 中的对象数量来调整网络负载（如 [23]），或者通过调整 CPM 的生成规则（如 [12] 和 [24]）。然而，所有这些工作仍然是广播传感信息。因此，为了减轻广播的负面影响，迫切需要一种原则性的方法来选择哪些车辆应接收相关信息、以何种分辨率以及通过哪些资源块（RB），但这也很复杂。

深度强化学习（DRL）已被证明在车辆和无线通信领域的类似复杂情况中很有用 [24]-[28]。据我们所知，只有 [24] 处于协作感知场景中，其中 DRL 在车辆智能体中的主要目标是通过决定传输 CPM 或丢弃它来减轻网络负载，而无法更改其内容。虽然很有趣，但需要正确选择车辆之间交换的 CPM 内容，以便在符合可用通信资源的同时，最大化车辆对接收到的传感信息的满意度。

#### A. 贡献

本文的主要贡献是一个新颖的框架，用于解决联合关联车辆、分配 RB 和选择车辆之间交换的 CPM 内容的问题，目标是最大化所有车辆对接收到的传感信息的平均满意度。

使用传统数学工具解决此类问题既复杂又难以处理。因此，我们要诉诸于机器学习技术，特别是 DRL [29]。具体而言，我们将主问题分为两个子问题：第一个问题侧重于关联车辆和分配 RB，并在路侧单元（RSU）级别解决；而另一个子问题侧重于选择 CPM 的内容，并在车辆级别解决。这两个问题都被公式化为 DRL 问题，其中 RSU 的目标是学习产生更高平均车辆满意度的关联和 RB 分配，而每辆车的目标是学习哪些传感信息是有用的并应传输给其关联车辆。此外，为了增强训练过程，我们建议使用联邦 RL [30]-[32]。具体而言，在每个时间帧，RSU 覆盖范围下的每辆车都会与 RSU 共享其最新的模型参数，然后 RSU 对所有接收到的模型参数进行平均，并将结果广播回其覆盖范围下的车辆。仿真结果表明，可以在 RSU 和车辆级别学习到实现更高车辆满意度的策略。此外，结果还表明，联邦 RL 改善了训练过程，与非联邦方法相比，可以在相同的时间内实现更好的策略。最后，结果表明，经过训练的智能体在实现的车辆满意度方面始终优于未经训练的随机智能体。

简而言之，这项工作的主要贡献可以总结如下：

* 我们在考虑无线通信带宽影响的同时，对车辆关联、RB 分配和 CPM 内容选择的联合问题进行了数学公式化。
* 我们提出了针对车辆关联和 RB 分配的 RL 问题公式化，以及针对 CPM 内容选择的 RL 问题。
* 此外，为了克服 RL 问题公式中固有的大型动作空间，我们应用了 [33] 中提出的竞争（dueling）和分支（branching）概念。
* 我们提出了一种联邦 RL 方法来增强所有车辆的训练过程。
* 我们基于实际交通数据进行仿真，以证明所提方法的有效性。

本文的其余部分组织如下。在第二节中，描述了系统模型的不同部分，包括传感、无线通信和四叉树模型。全网问题在第三节中公式化。随后在第四节简要介绍了 RL 及其如何在我们的协作感知场景中利用。在第五节中，介绍了巨大的动作空间问题及其克服方法。联邦 RL 方法在第六节中描述。最后，在第七节中展示了仿真结果，并在第八节中得出了结论。

---

### II. 系统模型 (System Model)

考虑由单个 RSU 覆盖和服务的道路交叉口，如图 1 所示。我们考虑一个时隙系统，时隙索引为 $t$，时隙持续时间为 $\tau$。设 $\mathcal{N}_t$ 为在时隙 $t$ 由 RSU 服务的 $N$ 辆车的集合，其中 $N$ 是 RSU 可以同时服务的最大车辆数。我们将每辆车 $n \in \mathcal{N}_t$ 在时隙 $t$ 的位置表示为 $l_n(t)$，并假设每辆车都配备了一个具有固定圆形半径 $r$ 的传感器。从车辆的角度来看，传感器关于任何位置的输出可以具有三种状态之一：占用 ($s_+$)、未占用 ($s_-$) 或未知 ($s_0$)。

占用状态 $s_+$ 对应于车辆感应范围内感应到障碍物的位置。未占用状态 $s_-$ 对应于车辆感应范围内感应到无障碍物的位置。最后，未知状态 $s_0$ 对应于车辆传感器无法感应的位置，或者是由于遮挡，或者是由于它们在车辆的感应范围之外。

假设车辆 $n$ 的传感器在时隙 $t$ 关于位置 $x$ 的输出是 $s_n(x,t) \in \{s_+, s_-, s_0\}$。此外，这里任何位置的“地面真值”状态要么是占用 $s_+$，要么是未占用 $s_-$，由 $G(x,t) \in \{s_+, s_-\}$ 给出。由于传感器并非完全可靠，因此 $s_n(x,t)$ 将以固定的概率 $\lambda_n$ 是正确的，即 $s_n(x,t)=G(x,t)$，其中 $\lambda_n \in (0.5, 1]$ 代表传感器的可靠性。因此，给定其传感器输出，位置 $x$ 相对于车辆 $n$ 的占用概率为：

$$
p_{n}(x,t)=Pr(G(x,t)=s_{+}|s_{n}(x,t))
$$

$$
=\begin{cases}\lambda_{n}& \text{如果 } s_{n}(x,t)=s_{+},\\ 1-\lambda_{n}& \text{如果 } s_{n}(x,t)=s_{-},\\ 1/2& \text{如果 } s_{n}(x,t)=s_{0},\end{cases} \quad (1)
$$

请注意，某些位置被障碍物遮挡，或处于感应范围之外，这导致 $s_n(x,t)=s_0$。为了模拟这种未知状态的高不确定性，将其映射为 $p_n(x,t) = \frac{1}{2}$，这代表概率分布中的最高不确定性。

设 $q_n(x,t)$ 为时隙 $t$ 开始时位置 $x$ 的感应信息的价值（或质量）。$q_n(x,t)$ 取决于占用概率 $p_n(x,t)$ 和信息年龄 (AoI) $\Delta_n(x,t)$ [34], [35]，其由下式给出：

$$
q_n(x,t) = |2p_n(x,t) - 1| e^{-\mu \Delta_n(x,t)}, \quad \Delta_n(x,t) = \tau t - \Gamma_n(x), \quad (2)
$$

其中参数 $\mu \in (0,1)$，$\Gamma_n(x)$ 定义了位置 $x$ 上次被车辆 $n$ 感应到的时刻。这里，我们选择 AoI 作为指标来强调新鲜传感信息的重要性。请注意，价值函数 $q_n(x)$ 随着其 AoI 增加（过时信息）或位置 $x$ 的占用概率接近 $1/2$（不确定信息）而减小。

此外，每辆车 $n$ 都有兴趣扩展其感应范围。车辆的速度越高，其感兴趣区域 (RoI) 就应该越大。因此，每辆车都对其运动方向前方 $t_{int}$ 秒感兴趣。为了简单起见，车辆 $n$ 的 RoI 由直径为 $v_n t_{int}$ 的圆形区域捕获，其中 $v_n$ 是车辆的速度。在 RoI 内，车辆对获取离其当前位置较近以及离其运动方向较近的位置的传感信息有更高的兴趣。因此，我们正式定义车辆 $n$ 对位置 $x$ 的兴趣 $w_n(x)$ 如下：

$$
w_{n}(x)=\begin{cases}\frac{v_{n}t_{int}\cos~\theta-d}{v_{n}t_{int}\cos~\theta}, & d < v_n t_{int} \cos \theta, \\ 0, & \text{其他}. \end{cases} \quad (3)
$$

其中 $d$ 是位置 $x$ 与车辆位置 $l_n(t)$ 之间的欧几里得距离，$\theta$ 是车辆运动方向与位置 $x$ 之间的角度，如图 1 所示。为了捕捉收集新信息的需求，车辆 $n$ 的兴趣 $w_n(x)$ 需要根据有价值信息的缺乏程度进行加权，即 $1-q_n(x)$。因此，车辆 $n$ 对位置 $x$ 的修正兴趣由下式给出：

$$
i_n(x) = w_n(x)[1-q_n(x)]. \quad (4)
$$

我们假设每辆车在每个时隙最多可以与一辆车关联以交换传感信息。我们定义 $E(t)=[e_{nn'}(t)]$ 为全局关联矩阵，其中如果车辆 $n$ 在时隙 $t$ 与车辆 $n'$ 关联（传输），则 $e_{nn'}(t)=1$，否则 $e_{nn'}(t)=0$。假设关联是双向的，即 $e_{nn'}(t)=e_{n'n}(t)$。此外，我们假设每一对关联车辆可以彼此同时通信，即每辆车配备有两个无线电设备，一个用于发送，另一个用于接收。此外，一组 $K$ 个正交资源块（RB）在车辆之间共享，每个 RB 的带宽为 $w$，其中每个发射无线电仅分配一个 RB。

我们进一步定义 $\eta_{nn'}^k(t) \in \{0,1\}$ 为 RB 分配，对于所有 $k \in \mathcal{K}$ 和 $n, n' \in \mathcal{N}$。这里，如果车辆 $n$ 在时隙 $t$ 通过 RB $k$ 向车辆 $n'$ 传输，则 $\eta_{nn'}^k(t)=1$，否则 $\eta_{nn'}^k(t)=0$。为了避免自干扰，分配给每个关联对的 RB 是正交的，即 $\eta_{nn'}^k(t) \neq \eta_{n'n}^k(t)$，前提是 $e_{nn'}(t)=1$。

设 $h_{nn'}^k(t)$ 为时隙 $t$ 中车辆 $n$ 到车辆 $n'$ 在 RB $k$ 上的瞬时信道增益，包括路径损耗和信道衰落。我们考虑 5.9 GHz 载波频率并采用 [36] 中现实的 V2V 信道模型，其中根据车辆的位置，信道模型分为三种类型：视距、弱视距和非视距。因此，时隙 $t$ 中从车辆 $n$ 到车辆 $n'$ 的数据速率（以每时隙数据包数为单位）表示为：

$$
R_{nn'}(t) = e_{nn'}(t) \cdot \frac{\tau}{M} \sum_{k \in \mathcal{K}} \eta_{nn'}^k(t) \omega \log_2 \left( 1 + \frac{P h_{nn'}^k(t)}{N_0 \omega + I_{nn'}^k(t)} \right) \quad (5)
$$

其中 $M$ 是数据包长度（以比特为单位），$P$ 是每个 RB 的传输功率，$N_0$ 是加性高斯白噪声的功率谱密度。这里，$I_{nn'}^k(t) = \sum_{i,j \in \mathcal{N} \setminus \{n,n'\}} \eta_{i,j}^k(t) P h_{in'}^k(t)$ 表示接收器 $n'$ 在 RB $k$ 上从其他在同一 RB $k$ 上传输的车辆接收到的总干扰。

#### A. 四叉树表示

存储和交换车辆之间的原始传感信息（例如，关于单个位置 $x$ 的信息）需要大量的内存和通信资源，才能使协作感知被认为是有用的。为了缓解这一挑战，每辆车可以使用一种称为区域四叉树的压缩技术，该技术可以有效地存储二维空间上的数据 [20]。在该技术中，每辆车将其感应范围转换为边长为 $2r$ 的正方形块。该块递归地分为 4 个块，直到：

* 达到最大分辨率级别 $L$，或者
* 块内每个位置的状态相同。

不失一般性，我们假设每个块可以使用 $M$ 比特表示。图 2 显示了车辆 $k$ 的感应范围的四叉树表示，其中 $L=5$。车辆 $n$ 的四叉树内的块 $b$ 的状态为：

* **占用**：如果块内的任何位置 $x$ 的状态是占用，
* **未占用**：如果块内的每个位置都是未占用，
* **未知**：否则。

由此可见，每个块 $b$ 的占用概率 $p_n(b)$ 可以以与 (1) 相同的方式定义：

$$
p_{n}(b)=\begin{cases}\lambda_{n}& \text{如果 } s_{n}(b)=s_{+},\\ 1-\lambda_{n}& \text{如果 } s_{n}(b)=s_{-},\\ 1/2& \text{如果 } s_{n}(b)=s_{0},\end{cases} \quad (6)
$$

并且块 $b$ 的传感信息的价值 $q_n(b)$ 以与 (2) 相同的方式定义。

设 $\mathcal{B}_n(t)$ 代表在时隙 $t$ 开始时车辆 $n$ 可用于传输的四叉树块集。假设 $\mathcal{B}_n(t) = \mathcal{B}_n^c \cup \mathcal{B}_n^p$，其中 $\mathcal{B}_n^c$ 是从其自身当前感应范围可用的块集，而 $\mathcal{B}_n^p$ 是从先前时隙可用的块集（可以是旧的自身块或从其他车辆接收的块）。请注意，由于四叉树压缩，$\mathcal{B}_n^c$ 的基数上限为：$|\mathcal{B}_n^c| \le \sum_{l=0}^{L-1} 4^l = \frac{1-4^L}{1-4}$。此外，为了保持交换的传感信息新鲜，对 $\mathcal{B}_n^p$ 的基数应用上限：$|\mathcal{B}_n^p| \le B_{max}^p$，如果 $\mathcal{B}_n^p$ 的基数超过 $B_{max}^p$，则丢弃具有较高 AoI 的块。确定需要共享哪些四叉树块以及与哪些车辆共享并不简单。为了回答这些问题，我们要先对问题进行公式化。

---

### III. 问题公式化 (Problem Formulation)

在我们的模型中，每辆车 $n$ 都有兴趣与另一辆车 $n'$ 关联（配对），其中每对交换四叉树块形式的传感信息，目标是最大化两辆车的联合满意度。车辆 $n$ 对时隙 $t$ 从车辆 $n'$ 接收到的传感信息的满意度可以定义如下：

$$
f_{nn'}(t) = \sum_{b \in \mathcal{B}_{n'}(t)} \sigma_{n'}^b(t) \left( \frac{\sum_{x \in b} i_n(x)}{\Lambda(b)} \cdot q_{n'}(b) \right) \quad (7)
$$

其中，如果车辆 $n'$ 在时隙 $t$ 向车辆 $n$ 传输了块 $b$，则 $\sigma_{n'}^b(t)=1$，否则 $\sigma_{n'}^b(t)=0$，$\Lambda(b)$ 是块 $b$ 覆盖的面积。此外，应该注意的是，车辆 $n$ 对接收分辨率与其 RoI 权重成比例的四叉树块更满意（根据公式 (4)），即对于具有较高 $i_n(x)$ 的区域，块 $b$ 具有更高的分辨率（更小的覆盖面积 $\Lambda(b)$），这通过 $\frac{\sum_{x \in b} i_n(x)}{\Lambda(b)}$ 捕获。此外，车辆 $n$ 对接收具有更有价值的传感信息的四叉树块更满意，这由 $q_{n'}(b)$ 捕获。因此，我们的协作感知全网问题可以正式提出如下：

$$
\max_{\eta(t), E(t), \sigma(t)} \sum_{n, n' \in \mathcal{N}} f_{nn'}(t) \cdot f_{n'n}(t) \quad (8a)
$$

$$
\text{s.t.} \sum_{b \in \mathcal{B}_n(t)} \sigma_n^b(t) \le \sum_{n' \in \mathcal{N}} R_{nn'}(t), \forall n \in \mathcal{N}, \forall t, \quad (8b)
$$

$$
\sum_{n' \in \mathcal{N}} \sum_{k \in \mathcal{K}} \eta_{nn'}^k(t) \le 1, \forall n \in \mathcal{N}, \forall t, \quad (8c)
$$

$$
\sum_{n' \in \mathcal{N}} e_{nn'}(t) \le 1, \forall n \in \mathcal{N}, \forall t, \quad (8d)
$$

$$
e_{nn'}(t) = e_{n'n}(t), \forall n, n' \in \mathcal{N}, \forall t, \quad (8e)
$$

$$
\eta_{nn'}^k(t) \in \{0,1\}, e_{nn'}(t) \in \{0,1\}, \sigma_n^b(t) \in \{0,1\}, \forall t, k \in \mathcal{K}, n, n' \in \mathcal{N} \quad (8f)
$$

其目标是关联车辆 $E(t)$，分配 RB $\eta(t)=[\eta_{nn'}^k(t)]_{n,n' \in \mathcal{N}}^{k \in \mathcal{K}}$，并选择传输消息的内容（每辆车要传输的四叉树块）$\sigma(t)$，以最大化关联车辆对的联合满意度之和。注意 (8a) 是由其香农数据速率对每辆车传输的四叉树块数量的上限，而 (8b) 将分配给每辆车的 RB 数量限制为 1。

注意，RB 分配 $\eta_{nn'}^k(t)$ 和车辆关联 $E(t)$ 直接决定了数据速率 $R_{nn'}(t)$（如公式 (5) 所示），指定了车辆之间传输的四叉树块的最大数量。此外，所选的四叉树块 $\sigma(t)$ 与 $E(t)$ 一起直接决定了车辆满意度（如公式 (7) 所示）。因此，找到该问题的最优解（RB 分配、车辆关联和消息内容选择）是复杂且不直接的。

从集中式的角度来看，如果 RSU 试图解决这个问题，RSU 需要知道车辆之间的实时无线信道和每辆车感应信息的细节，以便最优地解决 (8)。在 RSU 和车辆之间频繁交换这些快速变化的信息会产生巨大的通信开销，这是不切实际的。从分散式的角度来看，为了最大化 (7)，车辆 $n'$ 需要知道车辆 $n$ 根据 (4) 的确切兴趣，以便最优地选择要传输的四叉树块，这也是不切实际的。因此，为了解决 (8)，我们利用已被证明在处理此类复杂情况中有用的机器学习技术，特别是 DRL [29]。

---

### IV. 基于强化学习的协作感知 (Reinforcement Learning-Based Cooperative Perception)

#### A. 背景

RL 是一种理解目标导向学习和决策的计算方法 [37]。RL 是关于通过交互学习如何行动以实现目标。学习者（或决策者）被称为智能体，它与环境（包括智能体之外的所有事物）进行交互。因此，任何目标导向的学习问题都可以简化为智能体与其环境之间交换的三个信号：一个代表智能体所做选择的信号（动作），一个代表做出选择的基础的信号（状态），以及一个定义智能体目标的信号（奖励）。在典型的 RL 问题中，智能体的目标是最大化其收到的奖励总量，这意味着不仅要最大化即时奖励，还要最大化长期的累积奖励。

RL 问题通常使用马尔可夫决策过程 (MDP) [37] 形式化，特征为 $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R})$。即，在时间步 $t$，具有状态 $s \in \mathcal{S}$ 的智能体使用策略 $\pi(a|s)$ 执行动作 $a \in \mathcal{A}$，并接收奖励 $r_t = \mathcal{R}(s,a) \in \mathbb{R}$，且以概率 $p(s'|s,a) = \mathcal{T}(s,a,s')$ 转移到状态 $s' \in \mathcal{S}$。我们定义 $R_t = \sum_{t'=t}^H \gamma^{t'-t} r_t$ 为时间范围 $H$ 上的折扣回报，折扣因子 $\gamma \in [0, 1)$，并定义 $Q^{\pi}(s,a) = \mathbb{E}_{\pi}[R_t|s_t=s, a_t=a]$ 为状态 $s$ 和动作 $a$ 的动作价值（Q 值）。此外，设 $\pi^*$ 为最大化 Q 值函数的最优策略，$Q^{\pi^*}(s,a) = \max_{\pi} Q^{\pi}(s,a)$。RL 的最终目标是通过让智能体与环境交互来学习最优策略 $\pi^*$。

在用于解决 RL 问题的各种技术中，在这项工作中，我们将提倡使用 Q 学习和深度 Q 网络 (DQN)。

**1) Q 学习和 DQN：** Q 学习迭代地估计最优 Q 值函数，$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$，其中 $\alpha \in [0,1)$ 是学习率，$[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$ 是时间差分 (TD) 误差。只要进行充分的状态/动作空间探索，表格（无近似）情况下的收敛到 $Q^{\pi^*}$ 是有保证的；因此，表格学习不适合具有大状态空间的问题。实际的 TD 方法使用函数逼近器（如神经网络）来逼近 Q 值函数，即利用深度 Q 网络 (DQN) 进行 Q 值逼近的深度 Q 学习 [29]。

当使用非线性函数逼近器（如神经网络）来表示 Q 值函数时，RL 可能会不稳定甚至发散 [38]。为了克服这个问题，DQN 依赖于两个关键概念：经验回放和向定期更新的目标值调整 Q 值的迭代更新。近似 Q 值函数使用深度神经网络 $Q(s,a;\phi)$ 参数化，其中 $\phi$ 是 Q 网络的参数（权重）。为了使用经验回放，智能体在每个时间步 $t$ 的经验 $\mu_t = (s_t, a_t, r_t, s_{t+1})$ 被存储在数据集 $\mathcal{D}_t = \{\mu_1, ..., \mu_t\}$ 中。在学习期间，Q 学习更新应用于从存储样本池中均匀随机抽取的经验样本（小批量）$(s,a,r,s') \sim U(\mathcal{D})$。Q 学习更新使用以下损失函数：

$$
L(\phi) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} [(r + \gamma \max_{a'} Q(s',a';\phi^-) - Q(s,a;\phi))^2]
$$

其中 $\phi^-$ 是用于计算目标的网络参数。目标网络参数 $\phi^-$ 仅每 $C$ 步使用 Q 网络参数更新一次，并在各次更新之间保持固定 [29]。

#### B. 协作感知场景

为了解决 (8)，时间线被分为两个尺度：称为时间帧的粗尺度和称为时隙的细尺度。在每个时间帧开始时，RSU 将车辆关联成对并为这些对分配 RB。关联和 RB 分配在整个帧期间保持固定，该帧由 $X$ 个时隙组成。在每个时隙 $t$ 开始时，每辆车选择要传输给其关联车辆的四叉树块。通过利用 RL，我们可以制定两个不同但相互关联的 RL 问题：车辆 RL 和 RSU RL。

**1) 车辆 RL：** 在此 RL 问题中，对于给定的关联 $nn'$ 和 RB 分配，每辆车 $n$ 充当 RL 智能体，希望学习将哪些四叉树块传输给其关联车辆 $n'$，以最大化车辆 $n'$ 的满意度。因此，RL 环境的全局状态定义为 $\langle \mathcal{B}_n(t), \mathcal{I}_{n'}(t), v_n, v_{n'}, l_n(t), l_{n'}(t) \rangle$，其中 $\mathcal{I}_{n'}(t)$ 是车辆 $n'$ 在时隙 $t$ 根据 (4) 的 RoI 权重集。然而，车辆 $n$ 无法观察到此全局状态，相反，车辆 $n$ 的局部观测是 $\langle \mathcal{B}_n(t), v_n, v_{n'}, l_n(t), l_{n'}(t) \rangle$。在每个时隙 $t$，通过利用此局部观测，车辆 $n$ 采取动作 $\sigma_n(t)$，选择要传输给其关联车辆 $n'$ 的四叉树块，并相应地从车辆 $n'$ 接收等于 $f_{n'n}(t)$ 的反馈（奖励）。简而言之，每辆车 $n$ 处的 RL 问题要素描述如下：

* **全局状态：** $\langle \mathcal{B}_n(t), \mathcal{I}_{n'}(t), v_n, v_{n'}, l_n(t), l_{n'}(t) \rangle$。
* **局部观测：** $\langle \mathcal{B}_n(t), v_n, v_{n'}, l_n(t), l_{n'}(t) \rangle$。
* **动作：** $\sigma_n(t)$。
* **奖励：** $f_{n'n}(t)$。

**2) RSU RL：** RSU 充当 RL 智能体，该 RL 环境的状态由 RSU 服务的所有车辆的位置和速度给出，$\langle v_n, l_n \forall n \in \mathcal{N} \rangle$。基于每个时间帧开始时的此状态，RSU 采取车辆关联 $E(t)$ 和 RB 分配 $\eta(t)$ 的动作。然后，一旦时间帧结束，每辆车将报告其在整个帧期间的平均满意度，并且 RL 奖励计算为这些反馈的平均值。简而言之，RSU 处的 RL 问题要素可以总结如下：

* **状态：** $\langle v_n, l_n \forall n \in \mathcal{N} \rangle$。
* **动作：** $E(t)$ 和 $\eta(t)$。
* **奖励：** $\frac{1}{N} \sum_{n \in \mathcal{N}} (\frac{1}{X} \sum_{t=1}^X f_{n'n}(t))$。

为了解决这两个 RL 问题，可以使用 DQN 算法 [29]。然而，尽管其在具有高维状态空间的领域（如我们的领域）中取得了成功，但其应用于高维离散动作空间仍然很困难，因为在 DQN 中，在决定采取哪个动作之前，必须估计每个可能动作的 Q 值。此外，需要显式表示的动作数量随着动作维度的增加呈指数增长 [33]。

此时，我们注意到我们的两个 RL 问题遭受高维动作空间的困扰。具体而言，在 RSU RL 问题中，RSU 需要选择 $E(t)$ 和 $\eta(t)$。关联矩阵 $E(t)$ 的大小为 $N \times N$，由于我们的一对一关联假设，关联问题的可能动作数量将为 $\prod_{n=1}^{\lfloor N/2 \rfloor} (2n-1)$。此外，RB 分配矩阵 $\eta(t)$ 的大小为 $N \times K$，因此，假设每辆车仅分配 1 个 RB，可能动作的总数为 $K^N$。同样，在车辆 RL 问题中，每辆车需要选择 $\sigma_n(t)$，其维度为 $|\mathcal{B}_n|_{max} \times 1$，产生的可能动作总数等于 $2^{|\mathcal{B}_n|_{max}}$。

这种大量的动作会严重影响可用的离散动作强化学习算法（如 DQN）的学习行为，因为大动作空间难以有效探索，因此神经网络的成功训练变得难以处理 [39]。

---

### V. 克服大动作空间问题 (Overcoming the Large Action Space Problem)

最近，文献 [33] 的作者介绍了一种称为分支竞争 Q 网络（Branching Dueling Q-Network, BDQ）的新智能体。由此产生的神经网络架构允许将动作维度的表示分布在各个网络分支上，同时保持一个共享模块，该模块编码输入状态的潜在表示并有助于协调分支。此架构如图 3 所示。值得注意的是，与传统 DQN 网络架构中经历的组合增长相反，该神经网络架构表现出网络输出随动作空间增加而线性增长。

在这里，我们在我们的 RL 问题中采用 [33] 中的这些 BDQ 智能体。因此，RSU 智能体（神经网络）将具有 $N$ 个分支，构建如下：

* $\lfloor N/2 \rfloor$ 个分支对应于关联动作，每个分支具有 $j_i = N - 2i + 1$ 个子动作，其中 $i$ 是分支 ID。例如，让我们考虑一个 $N=6$ 的简化场景，则可以形成 $\lfloor N/2 \rfloor = 3$ 个车辆对：代表第一辆车的第一个分支将有 $N - 2 \cdot (1) + 1 = 5$ 个候选车辆与之配对，而对于第二个分支，候选减少到 3 个，依此类推。如果 $N$ 为奇数，这会导致在每个 $\lfloor N/2 \rfloor$ 分支上选择的子动作的任何组合都对应唯一的车辆关联。
* $\lfloor N/2 \rfloor$ 个分支对应于 RB 分配，每个分支具有 $\binom{K}{2}$ 个子动作，已知每个关联对分配 2 个正交 RB（每辆车一个）。

使用 BDQ 智能体的结果是，为了选择关联动作 $E(t)$，需要估计 $\sum_{n=1}^{\lfloor N/2 \rfloor} (2n-1)$ 个动作的 Q 值，而不是非分支网络架构中的 $\prod_{n=1}^{\lfloor N/2 \rfloor} (2n-1)$。类似地，选择 RB 分配 $\eta(t)$ 需要估计 $\frac{N}{2} \times \binom{K}{2}$ 个动作的 Q 值，而不是传统 DQN 架构中涉及的 $\binom{K}{2}^{N/2}$ 个值。同样，通过在我们的车辆 RL 问题中利用 BDQ 智能体，对于消息内容选择 $\sigma_n(t)$，仅需要估计 $2 \times |\mathcal{B}_n|_{max}$ 个动作的 Q 值，而不是 $2^{|\mathcal{B}_n|_{max}}$ 个动作。

#### A. 在协作感知场景中训练 BDQ 智能体

为了训练 RSU 和车辆智能体，选择 DQN 作为算法基础。因此，在每个 RSU 剧集开始时，选择车辆任意轨迹的随机起点，导致 RSU 观察到不确定的状态 $\langle v_n, l_n \forall n \in \mathcal{N} \rangle$。这里，此状态是 RSU 处可用的 BDQ 智能体（神经网络）的输入。然后，以概率 $\epsilon$，此 BDQ 智能体随机选择关联 $E(t)$ 和 RB 分配 $\eta(t)$ 动作，并以概率 $1-\epsilon$，它将选择具有最大 Q 值的动作（由神经网络的输出确定）。

对于具有 $|\mathcal{A}_i|=j_i$ 个离散子动作的任何动作维度 $i \in \{1, ..., J\}$，状态 $s \in \mathcal{S}$ 和子动作 $a_i \in \mathcal{A}_i$ 处的每个单独分支的 Q 值根据 [33] 用公共状态值 $V(s)$ 和相应的状态相关子动作优势 $A_i(s,a_i)$ 表示为：
$$
Q_i(s, a_i) = V(s) + \left( A_i(s, a_i) - \frac{1}{j_i} \sum_{a_i' \in \mathcal{A}_i} A_i(s, a_i') \right) \quad (9)
$$

确定动作后，RSU 将关联和 RB 分配决策转发给相应的车辆。此关联和 RB 分配决策将在接下来的 $X$ 个时隙中保持有效。一旦 RSU 决策传达给车辆，每辆车 $n$ 就可以计算其局部观测 $\langle \mathcal{B}_n(t), v_n, v_{n'}, l_n(t), l_{n'}(t) \rangle$。请注意，此局部观测构成在车辆 $n$ 运行的 BDQ 智能体的输入。此外，每辆车也采用 $\epsilon$-贪婪策略，因此将以概率 $\epsilon$ 选择随机的传感块进行传输，并以概率 $1-\epsilon$ 选择最大化 Q 值的传感块。

然后，产生的传感块将被安排通过分配的 RB 传输到其关联车辆。注意，根据 (5) 中的数据速率 $R_{nn'}(t)$，关联车辆可能仅接收这些块的随机子集。然后，它将根据 (7) 计算其对接收到的块的满意度 $f_{n'n}(t)$，并将此值作为奖励反馈给车辆 $n$。车辆 $n$ 接收奖励，观察下一个局部观测并将此经验 $\mu_t^n = (s_t, a_t, r_t, s_{t+1})$ 存储在数据集 $\mathcal{D}_t^n = \{\mu_1^n, ..., \mu_t^n\}$ 中。在 $X$ 个时隙后，每辆车将其在整个帧期间接收到的平均奖励反馈给 RSU，RSU 将计算所有接收到的反馈的平均值，并使用该结果作为其关联和 RB 分配动作的奖励。RSU 将其自身经验 $\mu_m^{RSU} = (s_m, a_m, r_m, s_{m+1})$ 存储在数据集 $\mathcal{D}_m^{RSU} = \{\mu_1^{RSU}, ..., \mu_m^{RSU}\}$ 中，其中 $m$ 是帧索引。每 $Z$ 帧开始一个新的 RSU 剧集。

一旦智能体收集了足够数量的经验，就开始训练其自己的神经网络。首先，从存储样本池中均匀随机抽取经验样本（小批量），$(s,a,r,s') \sim U(\mathcal{D})$。使用这些样本，BDQ 智能体的分支神经网络架构内的损失函数计算如下 [33]：

$$
L(\phi) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} \left[ \frac{1}{J} \sum_i (y_i - Q_i(s, a_i))^2 \right] \quad (10)
$$

其中 $i$ 是分支 ID，$J$ 是分支总数，$a$ 表示联合动作元组 $(a_1, ..., a_i, ..., a_J)$。此外，$y_i = r + \gamma \frac{1}{J} \sum_i Q_i^-(s', \arg\max_{a_i' \in \mathcal{A}_i} Q_i(s', a_i'))$ 在 (10) 中表示时间差分目标。最后，对 $L(\phi)$ 关于网络参数执行梯度下降步骤。BDQ 智能体的训练过程总结在算法 1 中。

---

### VI. 联邦强化学习 (Federated RL)

我们现在观察到，到目前为止，每辆车 $n$ 仅利用其自身经验独立训练其 BDQ 智能体。因此，为了拥有一个在不同情况下表现良好的弹性智能体，训练过程应运行足够长的时间以使车辆获得广泛的经验。或者，车辆可以定期相互共享其训练后的模型，以增强训练过程并在更短的时间内获得更好的模型。为此，我们研究了联邦 RL [30] 的作用，其中不同的智能体（车辆）在中央实体（RSU）的协调下协同训练全局模型，同时保持训练数据（经验）分散 [40], [41]。

我们不是在监督学习任务中应用联邦学习（FL），而是在这项工作中研究将 FL 用于我们的协作感知车辆 RL 问题中的强化学习。具体而言，在每个时间帧 $m$ 结束时，RSU 服务下的每辆车 $n$ 根据其本地经验更新（训练）其本地模型（神经网络权重）$\phi_m^n$，通过按照 (10) 对 $L(\phi_m^n)$ 执行梯度下降步骤。接下来，每辆车与 RSU 共享此更新后的模型，RSU 通过聚合所有接收到的模型来计算全局模型，如下所示：

$$
\phi_m^* = \frac{1}{N} \sum_{n \in \mathcal{N}} \phi_m^n
$$

其中 $\phi_m^*$ 是 RSU 在时间帧 $m$ 计算的全局模型。计算全局模型后，RSU 将 $\phi_m^*$ 广播回其服务下的车辆，其中每辆车用 $\phi_m^*$ 替换其本地模型。算法 2 总结了我们的协作感知场景中的整个 FRL 过程。

---

### VII. 仿真结果与分析 (Simulation Results and Analysis)

基于实际交通数据进行仿真，以证明所提方法的有效性。考虑一个交通信号灯调节的交叉口场景。使用城市交通仿真 (SUMO) 框架 [42] 生成了几个随机车辆移动轨迹的交叉口场景。每个场景跨越 30000 个时隙，总共由 30 辆进出单个 RSU 覆盖区域的车辆组成。车辆具有不同的尺寸以模拟各种汽车、公共汽车和卡车。除非另有说明，仿真参数列于表 I 中。

此外，接下来讨论用于训练 RSU 和车辆智能体的超参数。对于所有智能体，训练总是从前 1000 个仿真步之后开始；随后，对于每个仿真时间步，将运行一个训练步。使用学习率为 $10^{-4}$ 的 Adam 优化器。使用 64 的小批量大小和折扣因子 $\gamma=0.99$ 进行训练。此外，目标网络每 1000 个时间步更新一次。对于所有神经网络，所有隐藏层使用整流非线性 (ReLU) 激活函数，输出层使用线性激活函数。每个神经网络由共享网络模块中的两个隐藏层（分别具有 512 和 256 个单元）和每个分支的一个隐藏层（具有 128 个单元）组成。最后，每个智能体的重播记忆缓冲区大小设置为 $10^6$。

首先，我们验证 BDQ 智能体是否能够在不经历任何显著性能下降的情况下处理巨大的动作空间问题（与经典 DQN 智能体相比）。为此，我们通过增加最大四叉树分辨率 $L$ 来改变车辆 RL 问题的动作空间大小。注意，当 $L=2$ 时，可用的最大块数为 $\frac{1-4^L}{1-4}=5$，导致总动作数为 $2^5=32$，而当 $L=3$ 时，可用的最大块数为 21，导致总动作数为 $2^{21} \approx 2 \times 10^6$，假设每辆车 $n$ 仅传输其 $\mathcal{B}_n^c$ 内的块。图 4 显示了在每种 $L$ 情况下 BDQ 和 DQN 智能体的学习曲线。当 $L=2$（小动作空间）时，BDQ 和 DQN 智能体的学习曲线具有可比性，并且它们以相同的速率学习。然而，当 $L$ 增加到 3（大动作空间）时，DQN 智能体的训练过程无法完成，因为计算成本太高。这是由于 DQN 网络需要显式表示大量的动作，因此每次迭代必须训练极多的网络参数。然而，BDQ 智能体表现良好，并显示出对巨大动作空间的鲁棒性，这证明了其适合克服其他形式的 RL 所面临的可扩展性问题。

接下来，在图 5 中，我们研究了非联邦场景中 RSU 智能体针对不同 $N$ 值的训练进度，其中 $N$ 是 RSU 可以服务的最大车辆数。图 5 演示了 RSU 奖励如何随着训练剧集数量逐渐增加，即 RSU 和车辆在训练期间学习了更好的关联、RB 分配和消息内容选择。然而，可以注意到，随着服务车辆数量 $N$ 的增加，RSU 奖励的增加率降低，因此需要更多的剧集才能达到相同的性能。后者的原因是 RSU 智能体状态空间的膨胀，这需要探索更多的剧集。此外，每训练 100 个剧集进行一次评估，使用贪婪策略进行 10 个剧集。图 5 显示了训练期间评估过程的进度，并验证了智能体在训练期间学习了更好的策略。

在图 6 中，我们比较了联邦和非联邦场景以及不同 $N$ 值的训练过程演变。从该图中，我们观察到，对于相同的训练周期，如果与非联邦场景相比，联邦场景实现了更好的奖励，因此，在所有车辆上实现了更好的策略。这一结果证实了 FL 算法在增强和促进 RL 训练过程中是有用的。

一旦获得了训练后的 RSU 和车辆智能体，这些智能体就被部署在一个新生成的运行 20000 个时隙的车辆移动轨迹场景中。图 7 显示了在两种场景下（使用训练后的智能体与非训练后的随机选择动作的智能体），所有车辆和不同 $N$ 值的车辆奖励的互补累积分布函数 (CCDF)。我们可以通过简单的检查看到，训练后的智能体实现的车辆奖励分布优于非训练的情况。对于 $N=4$ 和 $N=6$ 时，此结果均成立。此外，图 8 显示了平均实现的车辆奖励与平均传输速率的关系。请注意，对于给定的传输速率范围，经过训练的智能体在 $N=4$ 和 $N=6$ 时都能比未经训练的智能体获得更好的车辆奖励，例如，当 $N=4$ 和 $N=6$ 时，经过训练的智能体可以在给定的传输速率范围内分别平均多获得约 60% 和 40% 的奖励。此外，与未经训练的智能体相比，经过训练的智能体可以用更低的传输速率实现相同的车辆奖励。总之，利用 RL，RSU 和车辆智能体学会了如何采取更好的关联、RB 分配和消息内容选择动作，从而最大化车辆对接收到的传感信息的满意度。

最后，为了评估学习到的协作感知策略的质量，我们考虑一个仅有 $N=2$ 辆车的网络，最大四叉树分辨率级别为 $L=2$。我们将我们训练后的智能体与一个 Oracle（预言机）进行比较，Oracle 确切知道每辆车的 RoI 权重，并选择最大化两辆车车辆满意度的四叉树块。图 9 绘制了两种情况下所有车辆实现的车辆奖励的 CCDF。可以观察到，训练后的智能体与 Oracle 之间的性能差距很小，这证明了所提方法在学习传输哪些四叉树块以增强协作感知方面的有效性。

---

### VIII. 结论 (Conclusion)

在本文中，我们研究了关联车辆、分配 RB 和选择 CPM 内容的问题，以便在考虑无线通信影响的同时最大化车辆对接收到的传感信息的满意度。为了解决这个问题，我们诉诸于 DRL 技术，其中建模了两个 RL 问题。为了克服我们 RL 问题公式中固有的大动作空间，我们应用了竞争和分支概念。此外，我们提出了一种联邦 RL 方法来增强和加速车辆的训练过程。仿真结果表明，可以在 RSU 和车辆侧学习到实现更高车辆满意度的策略，从而导致更高的车辆满意度。

